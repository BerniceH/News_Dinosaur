{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬蟲 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from bs4) (4.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (2.19.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests) (2018.8.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib import request\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/conda/lib/python3.6/site-packages (3.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install redis\n",
    "import redis\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHash(object):\n",
    "    def __init__(self, cap, seed):\n",
    "        self.cap = cap\n",
    "        self.seed = seed\n",
    "\n",
    "    def hash(self, value):\n",
    "        ret = 0\n",
    "        for i in range(len(value)):\n",
    "            ret += self.seed * ret + ord(value[i])\n",
    "        return (self.cap - 1) & ret\n",
    "\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, host='redis', port=6379, db=0, blockNum=1, key='bloomfilter'):\n",
    "        \"\"\"\n",
    "        :param host: the host of Redis\n",
    "        :param port: the port of Redis\n",
    "        :param db: witch db in Redis\n",
    "        :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it.\n",
    "        :param key: the key's name in Redis\n",
    "        \"\"\"\n",
    "        self.server = redis.Redis(host=host, port=port, db=db)\n",
    "        self.bit_size = 1 << 31  # Redis的String类型最大容量为512M，现使用256M\n",
    "        self.seeds = [5, 7, 11, 13, 31, 37, 61]\n",
    "        self.key = key\n",
    "        self.blockNum = blockNum\n",
    "        self.hashfunc = []\n",
    "        for seed in self.seeds:\n",
    "            self.hashfunc.append(SimpleHash(self.bit_size, seed))\n",
    "\n",
    "    def isContains(self, str_input):\n",
    "        if not str_input:\n",
    "            return False\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        ret = True\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            ret = ret & self.server.getbit(name, loc)\n",
    "        return ret\n",
    "\n",
    "    def insert(self, str_input):\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            self.server.setbit(name, loc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BloomFilter方法:處理資料去重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = BloomFilter()\n",
    "# url = \"https://tw.news.appledaily.com/politics/realtime/20181125/1473059/\"\n",
    "# url = url.encode('utf-8')\n",
    "# if bf.isContains(url):   # 判断字符串是否存在\n",
    "#     print('url is exists!')\n",
    "# else:\n",
    "#     print('url is not exists!')\n",
    "#     bf.insert(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloomfilter'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BloomFilter的key\n",
    "bf.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = redis.ConnectionPool(host='redis', port=6379, db=0)\n",
    "r = redis.StrictRedis(connection_pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'bloomfilter0']\n"
     ]
    }
   ],
   "source": [
    "#搜尋redis裡面有哪些keys\n",
    "keys = r.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#刪除redis裡key為XXX\n",
    "r.delete(\"bloomfilter0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "傳送log 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-logstash in /opt/conda/lib/python3.6/site-packages (0.4.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-logstash\n",
    "import logging\n",
    "import logstash\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"logstash\"\n",
    "crawler_logger = logging.getLogger('crawler_logger')\n",
    "crawler_logger.setLevel(logging.INFO)\n",
    "\n",
    "# TCP\n",
    "crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Producer 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka in /opt/conda/lib/python3.6/site-packages (1.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka\n",
    "from kafka import KafkaProducer\n",
    "import sys\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "連線Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    # Kafka集群在那裡?\n",
    "    bootstrap_servers = [\"kafka1:29092\"],\n",
    "    # 指定msgKey的序列化器, 若Key為None, 無法序列化, 透過producer直接給值\n",
    "    key_serializer = str.encode,\n",
    "    # 指定msgValue的序列化器\n",
    "    value_serializer = lambda m: json.dumps(m).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVBS 爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvbs_crawler2kafka():\n",
    "    ###TVBS爬蟲開始###\n",
    "    url = \"https://news.tvbs.com.tw/politics/\"\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    news_ul = soup.find_all(\"ul\", id = \"block_pc\")[0]\n",
    "    news_a = news_ul.find_all(\"a\")\n",
    "\n",
    "    news_no_string_list = []\n",
    "    for a in news_a:\n",
    "        news_no_string = a[\"data-news-id\"]\n",
    "        news_no_string_list.append(news_no_string)\n",
    "\n",
    "    news_no_list = []\n",
    "    for n in news_no_string_list:\n",
    "        news_no_int = n.strip('\\'')\n",
    "        news_no_list.append(news_no_int)\n",
    "\n",
    "    #json_data = []\n",
    "    for news_no in news_no_list:\n",
    "\n",
    "        print(\"新聞來源:\", \"TVBS新聞台\")\n",
    "\n",
    "        news_url = \"https://news.tvbs.com.tw/politics/\" + news_no\n",
    "        print(\"新聞網址:\", news_url)\n",
    "\n",
    "        \n",
    "        #判斷網址是否爬過\n",
    "        bf = BloomFilter()\n",
    "        # Unicode-objects must be encoded before hashing\n",
    "        news_url_u = news_url.encode('utf-8')\n",
    "        if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "            print('url is exists!')\n",
    "            continue\n",
    "        else:\n",
    "            print('url is not exists!')\n",
    "            bf.insert(news_url_u)\n",
    "        \n",
    "\n",
    "        req = requests.get(news_url)\n",
    "        # requests如果找不到指定編碼，會猜測網頁編碼，有時會形成亂碼，故給指定編碼utf-8\n",
    "        req.encoding = (\"utf-8\")\n",
    "        soup = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"h1\", class_ = \"margin_b20\").text\n",
    "        title = re.sub(r\"\\u3000\", \" \", title)\n",
    "        title = re.sub(r\"\\xa0\", \" \", title)\n",
    "        print(\"新聞標題:\", title)\n",
    "\n",
    "        date = soup.find(\"div\", class_ = \"icon_time time leftBox2\").text\n",
    "        print(\"發佈時間:\", date)\n",
    "\n",
    "        author = []\n",
    "        author_text = soup.find(\"h4\", class_ = \"font_color5 leftBox1\").text\n",
    "        author_text = re.sub(r\"攝影.*報導\", \"\", author_text)\n",
    "        author.append(author_text)\n",
    "        print(\"作者:\", author)\n",
    "\n",
    "        content = soup.find(\"div\", class_ = \"h7 margin_b20\").text\n",
    "        content = content.replace(\"\\t\", \"\").replace(\"   \",\"\").replace(\"\\n\", \"\").replace(\"（中央社）\",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，\",\"\").replace(\"快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"TVBS最新大數據分析\",\"\").replace(\"看完整內容快點我加入TVBS新聞LINE好友！\",\"\")\n",
    "        print(\"內文:\", content)\n",
    "\n",
    "        keyword = soup.find(\"div\", class_ = \"adWords\").text\n",
    "        keyword = keyword.replace(\"\\t\", \"\")\n",
    "        keyword = keyword.replace(\",\",\"\")\n",
    "        keyword = keyword.replace(\"\\n\", \"\").replace(\"編輯  \",\"\").replace(\" 報導\",\"\")\n",
    "        keyword = keyword.replace(\"記者\",\"\").replace(\"      \",\",\")\n",
    "        keyword = keyword.split(\",\")\n",
    "        print(\"關鍵字\", keyword)\n",
    "\n",
    "        img = soup.find(\"div\", class_ = \"margin_b20\")\n",
    "        img_url = img.find('img')['src']\n",
    "        print(\"圖片網址:\", img_url)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        j = {\"source\": \"TVBS新聞台\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "             \"kw\": keyword, \"img_url\": img_url}\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.info('python-crawler-logstash:TVBS crawler success!!')\n",
    "        crawler_logger.handlers.clear()\n",
    "        #json_data.append(j)\n",
    "    ###TVBS爬蟲結束###\n",
    "    \n",
    "\n",
    "   \n",
    "        # 步驟2.指定想要發佈訊息的topic名稱\n",
    "        topic_name = \"test\"\n",
    "\n",
    "        try:\n",
    "            print(\"Start sending messages ...\")\n",
    "            # 步驟3.產生要發佈到Kafka的訊息\n",
    "            # - 參數  # 1: topicName\n",
    "            # - 參數  # 2: msgKey\n",
    "            # - 參數  # 3: msgValue\n",
    "\n",
    "            producer.send(topic = topic_name, key = \"tvbs\", value = j)\n",
    "            print(\"Message sending completed!\")\n",
    "        except Exception as e:\n",
    "            # 錯誤處理\n",
    "            e_type, e_value, e_traceback = sys.exc_info()\n",
    "            print(\"type ==> %s\" % (e_type))\n",
    "            print(\"value ==> %s\" % (e_value))\n",
    "            print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "            print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "            print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "            # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.error('python-crawler-logstash :TVBS crawler error message!!')\n",
    "            crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETN爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setn_crawler2kafka():\n",
    "    ###SETN爬蟲開始###\n",
    "    page = 1\n",
    "    #json_data = []\n",
    "    while True:\n",
    "        page_url = \"https://www.setn.com/ViewAll.aspx?PageGroupID=6&p=\" + str (page)\n",
    "        print (\"Processing:\", page_url)\n",
    "\n",
    "        response = urlopen (page_url)\n",
    "        bs = BeautifulSoup (response)\n",
    "        titles = bs.find_all (\"h3\", \"view-li-title\")\n",
    "\n",
    "        if len (titles) == 0:\n",
    "            break\n",
    "\n",
    "        for t in titles:\n",
    "\n",
    "            print(\"新聞來源:\", \"SETN三立新聞網\")\n",
    "\n",
    "            news_url = \"https://www.setn.com\" + t.find (\"a\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            req = requests.get (news_url)\n",
    "            bs = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "            title = bs.find('h1', class_ = \"news-title-3\").text\n",
    "            title = re.sub(r\"\\u3000\", \" \", title)\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = bs.find(\"time\", class_ = \"page-date\").text.strip()\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "            author = []\n",
    "            a_divs = bs.find(\"div\", id=\"Content1\")\n",
    "            first_p = a_divs.find(\"p\")\n",
    "            author_text = first_p.text\n",
    "            #找第一段有沒有含\"／\"，如果沒有給予空字串\n",
    "            if author_text.find(\"／\") == -1:\n",
    "                author_text = \"\"\n",
    "            #如果作者是空字串，給予空list\n",
    "            if author_text == \"\":\n",
    "                author = []\n",
    "            else:\n",
    "                author.append(author_text)\n",
    "            print(\"作者:\", author)\n",
    "\n",
    "            c_divs = bs.find('div', itemprop='articleBody')\n",
    "            content_all = c_divs.find_all (\"p\")\n",
    "            content = \"\"\n",
    "            for c in content_all:\n",
    "                if c.attrs == {}:\n",
    "                    content = content + c.text\n",
    "                    #print(c.text)\n",
    "\n",
    "                    c_divs = bs.find('div', itemprop='articleBody')\n",
    "            content_all = c_divs.find_all (\"p\")\n",
    "            content = \"\"\n",
    "            for c in content_all:\n",
    "                if c.attrs == {}:\n",
    "                    content = content + c.text\n",
    "                    #print(c.text)\n",
    "            print(\"內文:\", content)\n",
    "\n",
    "            keyword = []\n",
    "            k_divs = bs.find(\"div\", class_=\"keyword page-keyword-area\")\n",
    "            k_strong = k_divs.find_all(\"strong\")\n",
    "            for k in k_strong:\n",
    "                keyword_text = k.text\n",
    "                keyword.append(keyword_text)\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "            img_p = bs.find(\"p\", style = \"text-align: center;\")\n",
    "            img_url = img_p.find(\"img\")[\"src\"]\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"SETN三立新聞網\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep (1)\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:SETN crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###SETN爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"setn\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :SETN crawler error message!!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        time.sleep (2)\n",
    "        page = page + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AppleDaily爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appledaily_crawler2kafka():\n",
    "    ###AppleDaily爬蟲開始###\n",
    "    for page in range(1, 12):\n",
    "        time.sleep(2)\n",
    "        href = \"https://tw.news.appledaily.com/politics/realtime/\" + str(page)\n",
    "        res = requests.get(href)\n",
    "        html = BeautifulSoup(res.text)\n",
    "\n",
    "        all_news_1 =  html.find_all(\"ul\", class_= \"rtddd slvl\")\n",
    "        for all_news in all_news_1:\n",
    "            news = all_news.find_all(\"a\")\n",
    "            print(\"第\", page, \"頁\")\n",
    "            for n in news:\n",
    "\n",
    "                print(\"新聞來源:\", \"蘋果日報\")\n",
    "\n",
    "                #my_news = {}\n",
    "                news_url =\"https://tw.news.appledaily.com/\" + str(n[\"href\"])\n",
    "                print(\"新聞網址:\", news_url)\n",
    "                \n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "                news_per = requests.get(news_url)\n",
    "                bs = BeautifulSoup(news_per.text)\n",
    "\n",
    "                title = bs.find(\"h1\").text\n",
    "                title = re.sub(r\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)           \n",
    "\n",
    "                date = bs.find(\"div\", class_=\"ndArticle_creat\").text.replace(\"出版時間：\", \"\")\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                if not bs.find(\"div\", class_=\"ndgKeyword\") == None:\n",
    "                    key_word = bs.find(\"div\", class_=\"ndgKeyword\").find_all(\"a\")\n",
    "                    key_list = []\n",
    "                    for k in key_word:\n",
    "                        key_list.append(k.text)\n",
    "                else:\n",
    "                    key_list = []       \n",
    "\n",
    "                content_dir = bs.find(\"div\", class_=\"ndArticle_margin\")\n",
    "                for a in content_dir.find_all(\"a\"):\n",
    "                    if not a == None:\n",
    "                        a.extract()\n",
    "                for s in content_dir.find_all(\"span\"):\n",
    "                    if not s == None:\n",
    "                        s.extract()\n",
    "                for i in content_dir.find_all(\"iframe\"):\n",
    "                    if not i == None:\n",
    "                        i.extract()\n",
    "                for d in content_dir.find_all(\"div\"):\n",
    "                    if not d == None:\n",
    "                        d.extract()\n",
    "                for st in content_dir.find_all(\"style\"):\n",
    "                    if not st == None:\n",
    "                        st.extract()\n",
    "                content = content_dir.text.strip()\n",
    "                content = content.replace(\"）\", \")\").replace(\"（\", \"(\").replace(\"／\", \"/\").replace(\"╱\", \"/\")       \n",
    "                pat = re.compile(r\"\\(((.{0,15})/.{0,8}[\\u5831][\\u5c0e])\\)\")\n",
    "                pat_2 = re.compile(r\"【((.{0,15})/.{0,8}[\\u5831][\\u5c0e])】\")\n",
    "                ans_news = pat.search(content)\n",
    "                ans_news_2 = pat_2.search(content)\n",
    "                if ans_news == None:\n",
    "                    author = []\n",
    "                elif not ans_news_2 == None:\n",
    "                    author = ans_news[2].split(\"、\")\n",
    "                else:\n",
    "                    #print(ans_news[2])\n",
    "                    author = ans_news[2].split(\"、\")\n",
    "                    content = content.split(ans_news[0])[0]\n",
    "                print(\"作者:\", author)   \n",
    "                print(\"內文:\", content)  \n",
    "                print(\"關鍵字:\", key_list)\n",
    "\n",
    "                img_box = bs.find(\"div\", class_=\"ndAritcle_headPic\")\n",
    "                if not img_box == None:\n",
    "                    img_url = img_box.find(\"img\")['src']\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "                j = {\"source\": \"蘋果日報\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                     \"content\": content, \"kw\": key_list, \"img_url\": img_url}\n",
    "                time.sleep(random.randint(1, 2))\n",
    "                \n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash:AppleDaily crawler success!!')\n",
    "                crawler_logger.handlers.clear()\n",
    "                #json_data.append(j)\n",
    "    ###AppleDaily爬蟲結束###\n",
    "\n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"test\"\n",
    "\n",
    "                try:\n",
    "                    print(\"Start sending messages ...\")\n",
    "                    # 步驟3.產生要發佈到Kafka的訊息\n",
    "                    # - 參數  # 1: topicName\n",
    "                    # - 參數  # 2: msgKey\n",
    "                    # - 參數  # 3: msgValue\n",
    "\n",
    "                    producer.send(topic = topic_name, key = \"appledaily\", value = j)\n",
    "                    print(\"Message sending completed!\")\n",
    "                except Exception as e:\n",
    "                    # 錯誤處理\n",
    "                    e_type, e_value, e_traceback = sys.exc_info()\n",
    "                    print(\"type ==> %s\" % (e_type))\n",
    "                    print(\"value ==> %s\" % (e_value))\n",
    "                    print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                    print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                    print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                    # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                    crawler_logger.error('python-crawler-logstash :AppleDaily crawler error message!!')\n",
    "                    crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOWnews爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nownews_crawler2kafka():\n",
    "    ###NOWnews爬蟲開始###\n",
    "    for page in range(1,6):\n",
    "        href = 'https://www.nownews.com/cat/politics/page/'+str(page)\n",
    "\n",
    "        response = requests.get(href)\n",
    "        soup=BeautifulSoup(response.text)\n",
    "        all_news = soup.find_all(\"div\",class_=\"td_block_inner tdb-block-inner td-fix-index\")\n",
    "\n",
    "        news = all_news[1].find_all(\"h3\",class_=\"entry-title\")\n",
    "        for n in news:\n",
    "            try:\n",
    "                print(\"新聞來源:\", \"NOWnews\")\n",
    "\n",
    "                news_url = n.find(\"a\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "                \n",
    "                \n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "                    \n",
    "\n",
    "                response = requests.get(news_url)\n",
    "                html = BeautifulSoup(response.text)\n",
    "                title = html.find(\"h1\",class_=\"entry-title\").text\n",
    "                title = re.sub(\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = html.find(\"time\",class_ = \"entry-date\").text\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                author = []\n",
    "                author_text = html.find(\"div\",class_=\"td-post-author-name\").text\n",
    "                author_text = author_text.split(\"／\")[0]\n",
    "                author_text = author_text.split(\"/\")[0].replace(\"記者\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                author.append(author_text)\n",
    "                print(\"作者:\", author)\n",
    "\n",
    "                te = html.find(\"div\", class_=\"td-post-content\").contents\n",
    "                #print(te)\n",
    "                #print(te[0].name == None)\n",
    "                content = \"\"\n",
    "                for a in te:\n",
    "                    if a.name == \"p\":\n",
    "                        content = content + a.text\n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                #處理keyword\n",
    "                if not html.find(\"ul\",class_=\"td-tags\") == None:\n",
    "                    kw = html.find(\"ul\",class_=\"td-tags\").find_all(\"a\")\n",
    "                    keyword = []\n",
    "                    for k in kw:\n",
    "                        keyword.append(k.text)\n",
    "                else:\n",
    "                    keyword = []\n",
    "                print(\"關鍵字:\", keyword)\n",
    "\n",
    "                img_url = html.find(\"div\",class_=\"td-post-featured-image\")\n",
    "                img_url = img_url.find(\"img\")[\"src\"]\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "            except:\n",
    "                print(\"錯誤:\", title, news_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"NOWnews\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(random.randint(0,2))\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:NOWnews crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###NOWnews爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"nownews\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :NOWnews crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORM爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storm_crawler2kafka():\n",
    "    ###STORM爬蟲開始###\n",
    "    json_data = []    \n",
    "    for page in range(1,5):\n",
    "        href = 'https://www.storm.mg/category/118/'+str(page)\n",
    "\n",
    "        response = urlopen(href)\n",
    "        html = BeautifulSoup(response)\n",
    "        #找到特定範圍在選取網址列\n",
    "        newf = html.find(\"div\",class_ = \"middle_category_cards\")\n",
    "        #選取所有單篇網址列\n",
    "        news = newf.find_all(\"div\",class_ = \"category_card\")\n",
    "        #迴圈抓取整頁新聞連結\n",
    "        for r in news:\n",
    "\n",
    "            print(\"新聞來源:\", \"風傳媒 THE STORM MEDIA\")\n",
    "\n",
    "            news_url = r.find(\"a\",class_ = \"card_link\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            response = urlopen(news_url)\n",
    "            html = BeautifulSoup(response)\n",
    "\n",
    "            title = html.find(\"h1\", id = \"article_title\").text\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = html.find(\"span\", class_=\"info_time\").text\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "            author = []\n",
    "            author_text = html.find(\"span\", class_=\"info_author\").text.replace(\"新新聞\",\"\")\n",
    "            author.append(author_text)\n",
    "            print(\"作者:\", author)\n",
    "\n",
    "            content = html.find(\"div\", id=\"CMS_wrapper\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "            print(\"內文:\", content)\n",
    "\n",
    "            keyword = html.find(\"div\", id=\"tags_list_wrapepr\").text.split()\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "            img_url = html.find(\"img\", id=\"feature_img\")[\"src\"]\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            time.sleep (1)\n",
    "\n",
    "            j = {\"source\": \"風傳媒 THE STORM MEDIA\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                 \"content\": content, \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(random.randint(0,2))\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:STORM crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###STORM爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"storm\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :STORM crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETtoday爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ettoday_crawler2kafka():\n",
    "    ###ETtoday爬蟲開始###\n",
    "    #json_data = []\n",
    "    for page in range(1, 1007):\n",
    "        url = \"https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1\" + \"&idx=1&page=\" + str(\n",
    "            page)\n",
    "        print(\"第\", page, \"頁\",\" \",\"Processing: \", url)\n",
    "        try:\n",
    "            response = urlopen(url)\n",
    "        except HTTPError:\n",
    "            print(\"大概是結束了\")\n",
    "            break\n",
    "        html = BeautifulSoup(response)\n",
    "\n",
    "        all_ar = html.find(\"div\", class_=\"result_archive\")\n",
    "\n",
    "        val = all_ar.find_all(\"div\", class_=\"archive clearfix\")\n",
    "\n",
    "        for u in val:\n",
    "\n",
    "            print(\"新聞來源:\", \"ETtoday\")\n",
    "\n",
    "            box2 = u.find(\"div\", class_=\"box_2\")\n",
    "            news_url = box2.find(\"a\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            a_response = urlopen(news_url)\n",
    "            a_html = BeautifulSoup(a_response, \"html.parser\")\n",
    "\n",
    "            title = a_html.find(\"h1\", class_=\"title\").text\n",
    "            title = re.sub(\"\\u3000\", \" \", title)\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = a_html.find(\"time\", class_=\"date\").text.strip()\n",
    "            date = re.sub(\"年\", \"/\", date)\n",
    "            date = re.sub(\"月\", \"/\", date)\n",
    "            date = re.sub(\"日\", \"\", date)\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "\n",
    "            a_story = a_html.find(\"div\", class_=\"story\")                \n",
    "            content_p = a_story.find_all(\"p\")\n",
    "            author_c = \"\"\n",
    "            for c_p in content_p:\n",
    "                #如果c_p裡面的第一個格子是None\n",
    "                if c_p.contents[0].name == None:\n",
    "                    author_c = author_c + c_p.text\n",
    "                elif c_p.contents[0].name == \"span\":\n",
    "                    author_c = author_c + c_p.text\n",
    "            author = []\n",
    "            if re.search(r\".*／.?.?報導\", author_c) == None:\n",
    "                author = []\n",
    "            else:\n",
    "                #因為作者與內文會放在同一個格子，所以將內文中會抓到的作者部分在內文處理時刪除\n",
    "                content_del = re.search(r\".*／.?.?報導\", author_c).group()\n",
    "                author_text = re.search(r\".*／.?.?報導\", author_c).group() \n",
    "            author.append(author_text)\n",
    "            print(\"作者:\", author)              \n",
    "\n",
    "            content = \"\"\n",
    "            for c in content_p:\n",
    "                if c.contents[0].name == None:\n",
    "                    content = content + c.text\n",
    "                    content = content.replace(\"\\u3000\", \" \")      \n",
    "                    content = content.replace(content_del, \"\")            \n",
    "            print(\"內文:\", content)\n",
    "\n",
    "\n",
    "            keyword = []\n",
    "            a_web = a_html.find(\"article\")\n",
    "            a_kw = a_web.find(\"p\", class_=\"tag\")\n",
    "            kw = a_kw.find_all(\"a\", target=\"_blank\")\n",
    "            for k in kw:\n",
    "                kw_n = k.text\n",
    "                keyword.append(kw_n)\n",
    "            if keyword == [\"\"]:\n",
    "                keyword = []\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "\n",
    "            if not a_story.find(\"img\")['src'] == None:\n",
    "                img_url = a_story.find(\"img\")['src']\n",
    "                img_url = \"https:\" + img_url\n",
    "            else:\n",
    "                img_url = None\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"ETtoday\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:ETtoday crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###ETtoday爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"ettoday\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :ETtoday crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 頁\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487411/\n",
      "新聞標題: 韓國瑜賽馬場創意惹議 觀光局：希望明年1月赴港考察\n",
      "發佈時間: 2018/12/21 21:17\n",
      "作者: ['周昭平']\n",
      "內文: (新增：準觀光局長潘恆旭說明)準高雄市長韓國瑜昨與中華民國工商建研會座談時，提到未來想把已停工的中油五輕廠，引進賽馬、發展相關產業鏈。此驚人發想立即引發爭議。準高雄市府新聞局長王淺秋今上午出面說明強調，昨天韓國瑜提賽馬只是創意發想，場地也不一定會在中油廠區，需進一步透過專業評估與合乎中央法規，才可能成為確定執行計畫。王淺秋說，昨韓國瑜會在會中提及賽馬，主要是因高雄目前財務現實狀況，要想辦法創造稅收、在價值供應鏈上創造最多效益，賽馬場只是評估考量之一，不過因實際上世界多國都有賽馬場，創造經濟供應鏈驚人，且有機會做公益，所以才會成為考量發展產業之一，雖說目前為止都只是創意發想階段，但觀光局已表示，希望明年一月可以安排到香港考察，同時針對法規問題了解，若評估可行就會來推動。此外針對韓國瑜昨在會中提到有台灣人曾任香港馬會會長，遭網友質疑經查香港馬會根本沒有台灣人，酸韓國瑜遭詐騙。對此，準高市府觀光局局長潘恆旭說，韓所提台灣人會長應是前香港馬會行政總裁黃至剛，他曾到台灣成大念書，後獲美國博士學位後回到台灣工作多年，1996年獲聘香港馬會行政總裁，2007年退休。但黃至剛是否為台灣人，這部分他並不清楚。潘恆旭說，黃至剛與韓國瑜之前有透過相熟的朋友聊到賽馬的經驗，但至於是否實際碰面他並不知情，此外黃至剛的職稱應是行政總監，韓國瑜講會長是口誤。潘恆旭說，因團隊還未就任，韓國瑜對此僅在構想階段，曾提過，但內部也還未有縝密的沙盤推演與輿情的配套，拋出想法是想要振興高雄經濟，畢竟賽馬可創造兩萬工作機會與很高產值，強調其原始概念是如此，不是說非推賽馬不可。至於明年一月去香港實地考察是否成行，會等到就任後，團隊評估覺得可試試看，確實可當成一個可評估的案子，就會成行。另外界質疑，中油五輕廠區多為汙染整治控制場址，該處若興建馬場時光是整治就要花17年。王淺秋回應，賽馬場並「不一定要在高雄煉油廠，很多其他場地可以考量！」強調引進不只是為賽馬這件事，而是運動型態供應鏈可以帶起的產業發展與經濟效益。《蘋果》追問昨行政院會已宣示中油五輕要推動轉型為「循環經濟園區」，韓國瑜昨提廠區要蓋賽馬場是否與中央政策牴觸？王淺秋回應，「是否場地在那裏(中油煉油廠)，其實都還沒有具體評估考量過。」她反問媒體，「您(昨天)在現場嗎？您在會場上嗎？」認為報導所指韓看中中油五輕蓋賽馬場說法，是經過與會者轉述，並非完全是韓國瑜所講的話。王淺秋最後強調，引進賽馬不是一個已經具體確定要執行的政策，須等就職後相關首長會商跟專業評估，同時考量相關中央地方法規，確實可行才會成為確定要執行的計畫。\n",
      "關鍵字: ['韓國瑜', '中油五輕', '賽馬', '王淺秋']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487802/\n",
      "新聞標題: ​金門檢調選後持續查賄 2當選人被提當選無效之訴\n",
      "發佈時間: 2018/12/21 21:11\n",
      "作者: ['蔡宇凌']\n",
      "內文: 九合一選後金門地檢署持續查賄，金城鎮代當選人張含麗、金寧鄉代當選人莊振源2人，涉嫌透過樁腳以每票3000元買票，今日向福建金門地方法院對2人提起當選無效之訴。 檢察官認金城鎮鎮民代表當選人張含麗及金寧鄉鄉民代表當選人莊振源，均有透過樁腳，涉有公職人員選舉罷免法第99條第1項之投票行賄罪嫌，本於公益代表人之身分擔任原告，向法院對2人一併提起當選無效之訴。 縣選委會已在11月30日公布當選人名單，而依據選罷法的規定，若當選人有涉賄情事者，選委會、檢察官或同一選舉區的候選人得以當選人為被告，自公告當選之日起30日內，向法院提起當選無效之訴。 金門地檢署檢察長毛有增表示，選舉雖然已經結束，但相關選舉不法案件並不因此而終結，選舉不法及虛設戶籍妨害投票案件仍由檢察官持續偵辦中，司法單位一定會堅守工作崗位，持續捍衛選舉之純正性與公平性。\n",
      "關鍵字: ['金門', '查賄', '當選人', '當選無效之訴']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_7e4d00a08a608b5395d89ea36dc6c7d3.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487792/\n",
      "新聞標題: ​賴揆視察金門大橋工程進度 指示如質如時完工\n",
      "發佈時間: 2018/12/21 21:05\n",
      "作者: ['蔡宇凌']\n",
      "內文: 金門大橋工程預定進度32.18%，目前進度為31.89%，行政院長賴清德今天到金門大橋工地視察施工情形，指示工作團隊如質如時完工，盼在110年能給小金門鄉親，一條更安全的道路。 賴清德指出金門大橋是台灣極有難度的公共工程，其橋長5.4公里，有4.77公里在海上，最深到23公尺，且氣象、海象條件困難，還有要打到23公尺深的地盤是花崗石岩，是非常艱鉅工程，這類工程不僅在台灣少見，在國際上也少見。做為曾在金門當兵的人，有機會能參與工程，感到很高興及光榮。 賴清德強調要將這條金門大橋建造起來，除了給小金門鄉親一條更安全的道路，也能讓大、小金門串聯，使整體的經濟效益得到更大發揮，不管是醫療、水電或各種功能，都能一併完成。 賴清德表示，臺灣再生能源剛起步，特別是風力發電，行政院期待風力發電產業的發展不僅滿足臺灣電力需求，也希望風力發電產業組成「國家隊」推動至國外，金門大橋也是風力發電產業未來發展的試金石，意義重大。\n",
      "關鍵字: ['賴清德', '金門大橋', '如期完工']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_0248dfc90c18b6012ed74a2a647c4699.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487633/\n",
      "新聞標題: 集郵韓粉最愛就職禮 個人化郵票竟唱起「夜襲」\n",
      "發佈時間: 2018/12/21 20:23\n",
      "作者: ['周昭平']\n",
      "內文: (更新：新增影片)「韓流」也吹進國營事業單位！高雄郵局選前看好國民黨高雄市長候選人長韓國瑜吹起韓流效應，勝選後立即連繫韓國瑜競辦取得他本人Q版商標授權，趕製個人化郵票套組，將在下周12月25日就職當天在典禮會場設置臨時郵局販售，今郵票套組首度亮相，郵摺設計除有Q版韓國瑜圖像，還將他經典競選口號印製在會旋轉的摩天輪車廂上，連造勢場上合唱的「夜襲」，都可在郵摺上按壓聽取，設計極具巧思。發行韓國瑜個人化郵票套組的高雄郵局，今由局長邱鴻恩與企劃行銷科科長許健雄帶著成品，到國民黨高雄市黨部，與準新聞局長王淺秋共同發表郵票套組設計。高雄郵局表示，之前高雄市長陳菊就職時也曾發行個人化郵票，當時發行兩萬套，當時是市府統一購買當作贈品送給貴賓，而韓國瑜的郵票套組則是看好韓流效應，設計風格活潑，一套要賣350元，但受限時間趕印不及，只印製1萬1千套，若時間允許，印量至少3萬起跳。仔細看這套韓國瑜就職紀念郵票，郵摺封面用了韓國瑜選前唱過的「愛你1萬年」，還寫上「一個城市有了夢想，才會偉大」，裡面有高雄愛河和85大樓等景點，一戰成名的愛情摩天輪的車廂上則有他「人進的來，東西賣得出去、高雄發大財」等施政理念口號及造勢晚會的香腸瀑布、韓冰及韓粉如貪吃蛇騎機車掃街拜票等造勢現場等，藉插畫以詼諧幽默方式表現。高雄郵局表示，因應民眾與集郵人士需求，預計就職典禮當天在會場設臨時郵局並發行這套個人化郵票套組，預計發行1萬1千套，原則上每人每次限購三套，一套350元，其中1千套將作為義賣，所得會依照韓國瑜希望捐給財團法人中華基督教衛理公會，做為守護偏鄉學童與弱勢家庭愛心善款。\n",
      "關鍵字: ['韓國瑜', '就職紀念郵票', '愛情摩天輪', '夜襲']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487745/\n",
      "新聞標題: 中華民國護照享德國自動通關 法蘭克福、慕尼黑兩機場適用\n",
      "發佈時間: 2018/12/21 20:18\n",
      "作者: ['陳培煌']\n",
      "內文: 外交部今天表示，台灣今年夏天起納入德國法蘭克福及慕尼黑機場自動通關試辦計畫，持中華民國有效護照者可享有自動化通關出境的待遇，持中華民國有效護照者在機場指定的查驗機，可享有自動化通關出境的待遇，對德方將台灣納入試辦對象表示歡迎。外交部指出，提升民眾旅外尊嚴與便利，一向是外交部重點工作，日後仍將持續推動包括德國在內等國家重要國際機場給予通關便捷待遇。\n",
      "關鍵字: ['德國', '法蘭克福', '慕尼黑', '自動通關', '中華民國', '護照']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_d5b2a296cb0b22e6054be116848296d1.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487775/\n",
      "新聞標題: 爭取民進黨主席 卓榮泰：扭轉資進黨印象、拚經濟絕非兩難\n",
      "發佈時間: 2018/12/21 20:14\n",
      "作者: ['鄭鴻達']\n",
      "內文: 九合一選舉後，民進黨大敗，總統蔡英文也請辭黨主席，行政院秘書長卓榮泰登記參選黨主席。卓榮泰在臉書張貼「勞資正向循環，創造財富要同步分配」文章表示，促進世代對話是黨主席的責任，接下來的難題，是拚經濟果實如何讓人民有感？他強調，扭轉人民的「資進黨」印象、給人民「拚經濟」的感覺絕非選擇上的兩難，而執政黨能做的，就是促進勞資雙方的正向、有機的循環。卓榮泰說，大家經常在媒體上，看見勞工指責慣老闆壓榨，老闆抱怨員工計較，這皆非正向互動跟循環，當經常接受這樣觀點跟資訊，就很難促進勞資關係，沒有互信、就沒有合作，沒有合作、就很難有感受，而一個執政黨能做的，就是促進勞資雙方的正向、有機的循環。他表示，行政院長賴清德已針對企業投資障礙「五缺」進行處理，因此獲得六大工商團體強烈建議留任的肯定，而打開經濟的活水，行政院已經跨出第一步，這點也反映在數據上，但只有這樣，顯然不夠。卓榮泰強調，政府努力為企業打開投資的大門，企業有競爭機會賺錢，也要同步給予勞工好處。他提醒，黨要扮演提醒與溝通的角色，提醒政府兼顧「創造」與「分配」，讓業主跟勞工不會成為互相搶錢的仇人，而是一起賺錢的夥伴。卓榮泰表示，「好員工是公司的資產」，員工有收穫就會更努力，工作有效率，企業才會有收益，這是下一個時代的企業家，都應該抱持的信念；照顧員工也不只反映在薪資上，也該從休息時間、進修、升遷、職災保障等方面，給予有彈性的規劃，來因應不同產業的需求。\n",
      "關鍵字: ['卓榮泰', '民進黨', '拚經濟', '資進黨']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_4b1816cfa50d9316285df719b5b6ee55.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487700/\n",
      "新聞標題: 打擊假訊息草案已送立法院 立法院下會期才處理\n",
      "發佈時間: 2018/12/21 19:50\n",
      "作者: ['曾盈瑜']\n",
      "內文: 為了打擊假訊息，政府展開相關修法，行政院政務委員羅秉成召集「防制假訊息危害專案小組」，盤點13項修法，上周行政院會已通過修正《災害防救法》、《糧食管理法》、《農產品市場交易法》、《傳染病防治法》、《食品安全衛生管理法》、《核子事故緊急應變法》、《廣播電視法》等7項，主要是對散播相關假訊息加重其刑期或罰鍰，送立法院審查。 此外，專案小組也討論到《社會秩序維護法》修正草案，規定若散布假訊息不實之事使公眾產生畏懼或恐慌者，可處3萬元以上、30萬元以下罰鍰，但後來因考量言論自由，未送行政院會通過，還需進一步院際討論協調。 錯假訊息透過網路傳播快速，因此日前政院版也提出「數位通訊傳播法」草案，該案5月已在立法院交通委員會初審通過，「數位通訊傳播法」草案規定，服務提供者，像是臉書、Google、Yahoo等，在知悉行為或資訊違法後，要立即移除或使他人無法接取，才能免責；權利人也能通知服務提供者，使用者發布的訊息涉有侵權行為，要求下架或移除資料。 不過，因朝野立委對於草案內容的「侵權」定義、要求下架的主張是否影響言論自由等尚有疑慮，因此目前草案還須協商，尚未完成二、三讀。 由於立法院本會期12月底即將結束，民進黨團總召柯建銘說，「13+N」個配套修法，行政院正在陸續送出，但都須留待下會期處理。\n",
      "關鍵字: ['行政院', '柯建銘', '立法院']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_db7a4718a67cd0fb81e7cffad2b0d4b3.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487516/\n",
      "新聞標題: 【最即時專題6】嗆謝長廷「鱷魚的眼淚」！羅智強：逼死蘇啟誠的就是你和民進黨\n",
      "發佈時間: 2018/12/21 19:38\n",
      "作者: ['即時新聞中心']\n",
      "內文: 前大阪辦事處長蘇啟誠之死，隨著遺孀曝光蘇尋短原因，外界現在又將矛頭重新指回駐日代表謝長廷身上。而準市議員羅智強，也在臉書砲轟謝長廷，「就是逼死蘇啟誠」元凶。謝長廷稍晚透過臉書回應，表示將對羅智強提告，而羅智強也再度更新發文，表示「謝長廷要告我，歡迎」、「讓社會再次公評，我説的是不是合理評論，有沒有道理？」 羅智強今日在臉書寫道：「我曾説過，這世界上有二種鱷魚。第一種鱷魚，吃了人不掉淚！另一種，吃了人還假裝掉淚。而大阪代表處蘇啟誠處長的輕生，讓我們看到民進黨政壇，充斥著這第二種鱷魚。」 羅智強說「一群『會掉淚的鱷魚』，吃人不吐骨頭就算了，吃了人，還大喊善哉慈悲、切莫殺生。可鄙可惡，令人憤怒！」更喊話「逼死蘇啟誠的，不是輿論，不是假新聞，就是民進黨政府、就是謝長廷！」\n",
      "關鍵字: None\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_c3c058fc8807f37afc9a3d68c5760c58.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487576/\n",
      "新聞標題: 楊秋興爆料「韓國瑜有壓力」 12/6已婉拒出任副市長\n",
      "發佈時間: 2018/12/21 19:17\n",
      "作者: ['王勇超', '張世瑜']\n",
      "內文: (新增：動新聞)高雄市準市長韓國瑜市政小內閣已拍板公布，與副市長一職擦身而過的前高雄縣長楊秋興，今天從廈門剛回國，又風塵僕僕趕到左營，參加韓粉們為慶祝韓國瑜25日就職，免費放送在地美食的活動。面對媒體追問他與副市長一職的轉折，楊秋興低調回應，「過去不要再談，韓市長有一些壓力」，所以他最後決定不要讓市長困擾。楊秋興表示，昨天他在廈門參加台商協會26周年大會，他去旅遊，應要求去參加大會。針對他傳簡訊給國民黨主席吳敦義，婉拒高捷、高銀董事長職務，楊秋興表示，吳敦義對他是否要入高市府小內閣很關心，當天是韓國瑜市長邀請他當高雄捷運公司董事長或者是高雄銀行董事長，他當場就婉拒市長，並表示做兩岸關係也很重要，所以他主動說：「兩岸小組我可以參與！」楊秋興強調，這是無給職，「決定了，就讓吳副總統了解！」 對於為何要婉拒加入韓國瑜小內閣？楊秋興證實，韓國瑜確實有找他幫忙，但後來自己了解到韓市長有壓力，再想想自己是去幫忙做事情的，不是去當官的，所以12/6就向韓市長旁邊的人轉達不入小內閣，雖自己沒入市府，但願意韓市長施政諮詢對象。 媒體追問，如果是副市長的職務，會不會接受？楊秋興再度強調：「現在我已經功成身退，就這樣就好！」有媒體問：「外傳王金平要擋你的路？」楊秋興說：「這個我就不予置評！」並表示韓市長有很大的壓力，他不要給韓市長困擾。 楊秋興說，他進入韓國瑜團隊的初衷，「就是去幫忙的，不是求官啦！」他跟台商的關係熟，跟大陸也有互信，所以他能義務來幫忙。媒體再追問楊秋興：「壓力是否來自國民黨高層？」楊秋興說：「這個我也不好說！」他可以體會，還是有些壓力，但不要給韓市長困擾。 楊秋興澄清，當出傳說KMT大老硬塞27個名單到韓國瑜小內閣，「後來指向我楊秋興，但其實不是！」當初韓市長11月24日與他聊天，希望他能推薦一些小內閣名單，「我確實找了7、8位左右！」名單是韓市長在用晚餐、大約晚上8、9點，他在旁邊幫忙整理一下，在出去澳門之前，稍微幫他整理一下。 楊秋興說，他拿兩張A4紙，「是擬建議名單，有十幾個，但我大概建議六、七個而已！」另外是韓市長內心已有的名單，再來是現任要留用的名單，「我把它寫下來！」扣掉四個一條鞭的單位，「其他27個的筆跡是我的沒錯，我交給韓市長，我有攝影！」楊秋興說，這兩張A4紙，「被韓市長身邊的人洩漏給媒體」，他覺得很不好，「你旁邊有小紅衛兵，我覺得很不好！」這也是他之後會重做思考的原因。 楊秋興表示，他與韓國瑜的關係沒有生變，聯繫管道仍暢通，但現在未入市政府，將來可以做他諮詢的對象，「市長現在很忙，我也不主動打電話給他！」他將向韓國瑜要邀請卡，參加25日的就職典禮。對於韓國瑜小內閣人事的看法，楊秋興說，局處很多，「要給市長一些時間」，希望他們能有好的表現。至於自己未來規劃？楊秋興則笑說：「就四處遊山玩水。」針對韓國瑜拋出高雄蓋賽馬場的話題，楊秋興似乎有不同看法，他表示，若從動物的角度，還要再斟酌。當然韓市長將想法提出來，有意見本來就可以再討論，但他個人比較接受釋昭慧法師的意見。關懷生命協會創會理事長釋昭慧昨曾表示，堅決反對韓國瑜擬發展的賽馬產業鏈，「這是人類為了利益，不惜讓動物受苦的殘忍行為」，且直指韓國瑜若硬要發展賽馬產業，會成為違法市長。倒是旗津賭場一事，楊秋興與韓國瑜似乎看法相同，楊說，他一直認為，旗津應該興建國際休閒度假村，然後可以小規模開放博奕，這在世界各國都是很普遍的狀況。\n",
      "關鍵字: ['楊秋興', '副市長', '旗津', '韓國瑜']\n",
      "圖片網址: None\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1486688/\n",
      "新聞標題: 慶祝韓國瑜就職 韓粉今起美食接力看這裡\n",
      "發佈時間: 2018/12/21 18:52\n",
      "作者: ['吳慧芬', '王勇超']\n",
      "內文: (新增：過音影片)韓國瑜將於下周二上任，從本周五起到25日就職日，天天都有熱心的「韓粉」免費招待美食，本周五有2千顆肉粽、2千碗廣東煲湯，周六有千杯杏仁茶，23日有5千碗滷肉飯、5千顆客家野薑花粽與888瓶冷泡茶，24日有1千份雞排、700份魚排，25日還有200杯綠豆冰沙可以喝，支持韓國瑜共拼經濟。選戰時大力支持韓國瑜的婦女界人士潘金英，今天下午在左營區自由路黃昏市場，準備了2千份糯米雞、2千分廣東煲湯，要免費發送給民眾，消息傳出吸引許多民眾前來排隊，下午兩點不到，就排了200多人，其中排第一號的劉先生(45歲)在胸口掛著自己用鐵絲製作的韓國瑜吊飾，他表示，他今天早上九點就來排隊，要慶祝韓市長當選「貨出得去，人進得來，高雄發大財！」英文老師謝小姐表示，她是韓國瑜的支持者，之前很多次「貪食蛇」(指韓粉慶祝韓國瑜當選，發放美食的排隊隊伍)都因為沒有時間排隊而沒參加，今天剛好有時間，就來參加，「這種活動帶動高雄的經濟！」在現場發送咖啡的業者表示，今天很高興，藉由慶祝韓國瑜當選市長的機會，特別現場請大家喝咖啡，「就像韓市長說的，東西要品質好，我們就推銷出去！」蔡姓民眾喝了咖啡，直呼：「香醇好喝！」韓國瑜選前喊出拼經濟口號，當選後「韓總美食」均受惠，韓國瑜光顧過的小吃店幾乎業績都提升，還有韓粉特製「韓總美食版清單」，讓外縣市遊客，得以按圖索驥從苓雅區滷肉飯，吃到旗山冰店，遍嚐高雄美食，帶動小吃經濟。 另有大方民眾，慶祝韓國瑜下周二上任，從本周五起天天招待美食。這包括一名住在台北韓粉郭先生，本要在25日韓國瑜當選日，於愛河畔發放1萬碗滷肉飯，受限於場地，改在23日中午於國民黨高市黨部前發送5000碗魯肉飯、5000顆客家野薑花粽，另高雄在地財記居冷泡茶，當日也將一併贈送888瓶茶飲。\n",
      "關鍵字: ['韓國瑜', '韓總美食', '就職典禮', '美食接力']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487718/\n",
      "新聞標題: 【最即時專題10】蘇啟誠輕生關鍵電話 台日協秘書長張淑玲：我沒打\n",
      "發佈時間: 2018/12/21 18:50\n",
      "作者: ['陳培煌']\n",
      "內文: 我駐日本大阪辦事處前處長蘇啟誠自殺過世滿百日，蘇啟誠遺孀昨發聲明指出，蘇啟誠輕生原因是「不想受到羞辱」。《蘋果》昨獨家報導，蘇啟誠在生前曾接到外交部一通電話告知懲處內容，而《聯晚》與《中時》報導，打這通電話的人是台灣日本關係協會秘書長張淑玲。張淑玲晚間出面澄清強調：「我沒有打過他(蘇啟誠)生前20分鐘電話，部長(外交部長吳釗燮)既未授權、我也沒有打那通電話，當然不會有繪聲繪影那通電話相關內容。」至於是否有其他人打過電話給蘇啟誠？張淑玲表示：「我目前沒有這樣掌握」，蘇輕生前一天20幾分鐘電話是完全沒有聽說。對於駐日代表謝長廷或代表處有打電話給蘇啟誠？張低調說：「這不方便回應！」蘇啟誠過世前兩三天，她都沒有跟蘇有任何聯絡，因有當時在台灣有公務要處理，因此後面兩天都沒有跟蘇聯絡，包括使用通訊軟體。對於蘇啟誠在風災後的情緒，張說：「我個人沒有感覺到他(蘇)有情緒上不穩定，(跟他溝通)都是一般性把事實說清楚。」對於有媒體提到蘇啟誠遺孀拒絕張淑玲參加追思會，張指出，她在蘇輕生後沒有跟家屬有直接碰過面，當時蘇的追思會都是私人朋友參加，這部分她不便談論。她強調，蘇啟誠這樣離開，同為跟蘇共事20幾年的同事都覺得痛，何況是家人。外界關注蘇啟誠與大阪辦事處原預定被懲處情況，張淑玲說，外交部已經再三澄清，當時日本風災發生後，希望可以趕快釐清相關事實，需要一段時間，當時都有對外說明，在還沒有辦法確認責任歸屬錢，不可能有懲處或調職。有媒體報導稱張淑玲有一個兒子在日本，因此積極運作想派任駐大阪處長，張淑玲鄭重澄清：「我沒有家人或親戚在日本」，她沒有表達想要派駐大阪意願，也未有長官向她提過此事，且蘇啟誠7月才赴任大阪處長，「我們在怎樣的狀況都不可能做這樣的處置！」\n",
      "關鍵字: ['日本', '大阪', '蘇啟誠', '張淑玲']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_ff333c8f2e83ba4244f92c1ae1696b5e.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487704/\n",
      "新聞標題: 談韓國瑜推賽馬產業爭議 陳其邁喊加油要他挑對人\n",
      "發佈時間: 2018/12/21 18:41\n",
      "作者: ['周昭平']\n",
      "內文: 高雄市準市長韓國瑜昨拋出中油五輕興建賽馬場議題引發爭議，競爭對手陳其邁今到台中與市長林佳龍參訪光復新村，會後受訪時被問及此問題時回應，韓國瑜還未上任，要大家多給他一點時間，多了解多看看，並意有所指說，「挑對人、挑有能力專業的人」幫助市長，壓力就不會那麼大，他說自己不會如媒體所稱要他「珍重」，反替他加油。陳其邁今回應賽馬爭議時直言，「當市長壓力非常大，韓市長的辛苦與考驗才剛開始！」但他也說，政策若能貼近民意、務實可行，加上找到好的人，在市政工作能全力幫助市長，「挑對人、挑有能力專業的人，壓力就不會那麼大，就能夠全力推動市政。」面對韓國瑜近日身陷用人及政策推動等爭議，他說自己不會如媒體評論要他「珍重」，他要替韓國瑜加油，並開玩笑說，盧秀燕剛就任，「有一個林佳龍障礙在前，壓力一定很大」，也要盧秀燕要多加油。\n",
      "關鍵字: ['陳其邁', '韓國瑜', '人事', '賽馬產業']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_b4230ec7372599977d801a9c1c1739df.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487727/\n",
      "新聞標題: 拉攏無黨無望 新北綠營高敏慧、陳文治選正副議長\n",
      "發佈時間: 2018/12/21 18:40\n",
      "作者: ['突發中心陳威叡']\n",
      "內文: 新北市議員選舉民進黨表現不如預期，從上屆原32席縮減為25席，爭取正、副議長龍頭機會微乎其微，日前原派出現任副議長陳文治拉攏無黨籍議員抗衡藍營未果，今日下午經黨團會議後，決議正副議長由高敏慧與陳文治搭配競選。                                                            國民黨本屆新北議員選舉一舉拿下33席議員席次，若加上無黨結盟，泛藍實質過半，議會龍頭寶座可謂囊中之物，正副議長由蔣根煌、陳鴻源角逐，反觀民進黨本次僅拿下25席，選舉結果不如預期，僅有拉攏無黨籍議員結盟才有機會爭取議長龍頭寶座。 新北市黨團本周二結束黨團會議後，原先推派陳文治協調拉攏無黨籍議員，結果不如預期，於今日黨團會議後，由今日下午登記參選高敏慧與陳文治搭檔爭取正副議長。\n",
      "關鍵字: ['民進黨', '新北市議長', '高敏慧', '陳文治']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_2743cd3d3780607049e07106e061348c.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487422/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cc6eeb1d951c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"22:27\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappledaily_crawler2kafka\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \"\"\"\n\u001b[1;32m    410\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running job %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schedule_next_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e80c0763cd2f>\u001b[0m in \u001b[0;36mappledaily_crawler2kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"新聞網址:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mnews_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_per\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    510\u001b[0m         }\n\u001b[1;32m    511\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!pip install schedule\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "\n",
    "#def job():\n",
    "#    print(\"I'm working...\")\n",
    "#schedule.every(1).minutes.do(job)\n",
    "\n",
    "#schedule.every().day.at(\"16:42\").do(tvbs_crawler2kafka)\n",
    "#schedule.every().day.at(\"17:51\").do(setn_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(appledaily_crawler2kafka) \n",
    "#schedule.every().day.at(\"22:27\").do(nownews_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(storm_crawler2kafka())\n",
    "\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 頁   Processing:  https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1&idx=1&page=1\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339321.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338839.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339228.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339294.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339271.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339214.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339177.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339057.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339086.htm\n",
      "url is exists!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339035.htm\n",
      "url is exists!\n",
      "第 2 頁   Processing:  https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1&idx=1&page=2\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339020.htm\n",
      "url is not exists!\n",
      "新聞標題: 同意和卓榮泰直播 館長：我歸覽趴火笑你不敢來\n",
      "發佈時間: 2018/12/24 12:05\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 民進黨黨主席候選人、行政院秘書長卓榮泰日前曾表示，若成吉思汗健身俱樂部館長陳之漢願邀他一起直播，他很願意聊聊，盼能聽到「同溫層聽不到的聲音」。對此，館長回應，卓榮泰沒跟他聯繫，「我笑你不敢來」，「我龜懶趴火，我很想要問一些問題」，要卓榮泰別只是透過媒體放話。據《鏡週刊》報導，卓榮泰認為，想要聆聽支持者的聲音，「我可以去跟陳其邁喝咖啡，我也可以去跟館長聊一聊啊！我可以唱詹雅雯的〈深情海岸〉，我也可以找鄭進一來唱〈家後〉吧！」對此，館長在直播中表示，卓榮泰沒有與他聯絡，自己很想跟一個要做黨主席的人一起直播，「我第一個就問促轉會」、「我很想問你，你當黨主席的時候，你的立委能不能通過一些有意義的審查案」、「我笑你不敢來！你要來不用新聞叫囂、直接打電話來嘛！」館長指出，若一起直播，大家就想看新任黨主席能不能讓他問一些問題，「我龜懶趴火，我很想要問一些問題」，像是薪水都沒漲，原物料一直漲，年輕人連摩托車都快買不起，同樣的機車在國外賣三分之一的價錢，「請問我們親愛的民進黨做了嗎？你們是上來圖利財團，還是為人民做事的？」\n",
      "關鍵字: ['卓榮泰', '館長']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3785/d3785468.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338982.htm\n",
      "url is not exists!\n",
      "新聞標題: 林佳龍「成為光」與市民勉勵 網喊：我把戶籍遷回去\n",
      "發佈時間: 2018/12/24 11:29\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 台中市長林佳龍在24日這任期最後一天，在臉書釋出《成為光》影片並表示，「當我們一起，為家的美好，感到驕傲。我們沒有停下的理由，永遠相信希望、相信光明、相信台中。」。影片一出，有網友大喊，「我把戶籍遷回去」卸任在即，林佳龍一早在臉書PO《成為光》，娓娓道出擔任台中市長的心路歷程，影片回顧綠川柳川整治、台中火車站新站平台、台中花博、捷運綠線試運轉、台中公園日月湖百年首度大清淤、興建水資源處理中心與外埔綠能生態園區等政績。林佳龍表示，經歷10年努力，他有幸在4年前獲得台中市民的付託，開啟一趟新的旅程，走在光裡，也引著光，照進台中的每個角落。\n",
      "\n",
      "林佳龍指出，當山線不再是遙遠邊緣，當海線不再停滯不前，當屯區充滿生機朝氣，當舊城重拾昔日光榮；當山海屯城，不再隔閡，攜手前進。\n",
      "\n",
      "林佳龍提及，「當我們一起，為家的美好，感到驕傲。我們沒有停下的理由，永遠相信希望、相信光明、相信台中。這一刻，我們成為光。」影片一出，許多網友紛紛留言「我身旁的朋友，都說林佳龍 做的很好，卻為什麼會沒選上？ 怪怪」、「謝謝市長四年的陪伴！期待再見」、「你四年後回來選，我把戶籍遷回去選你」\n",
      "關鍵字: ['林佳龍']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3785/d3785337.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338838.htm\n",
      "url is not exists!\n",
      "新聞標題: 不受「小英海嘯」影響！鄭文燦漂亮連任 沈富雄：民進黨宋楚瑜\n",
      "發佈時間: 2018/12/24 11:03\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 民進黨籍桃園市長鄭文燦以55萬2330票大勝國民黨候選人陳學聖，成功高票連任。資深媒體人陳揮文在節目上提到，鄭文燦不受到總統蔡英文執政影響，仍然能漂亮連任，所以「打鐵還是要自身硬」，不要牽拖母雞。但前民進黨立委沈富雄卻持不同意見，認為鄭文燦是「民進黨宋楚瑜」，所以票數當然多。陳輝文在23日播出的政論節目《新聞深喉嚨》中指出，鄭文燦的得票數跟得票率都往上，「你們這些民進黨的立委，很擔心海嘯來，你要看看鄭文燦，人家怎麼做，他怎麼做的？他得票都往上，他有受到中央的影響嗎？」可能也有影響，也有可能他的票會更高，但是鄭文燦連任的非常漂亮。陳輝文說，還是要認真去經營選區的話，不管總統候選人是蔡英文、賴清德，或是其他人。總統候選人對「小雞」立委有沒有影響？有影響，但是打鐵還要自身硬，自己才是關鍵。「我承認總統候選人對立委會有影響，但是你要想辦法把影響盡量降到最低，把你自己的優勢跟優點突顯出來，不要牽拖別人。」沈富雄則認為，陳揮文的說法似是而非，民進黨下次可能不是50席，也許只剩20席或30席，掉了那麼多，難道這些人都不努力嗎？他們努力得要死，怎麼會不努力？鄭文燦這一次比上一次好，他上一次連勝選的講稿都沒有準備好，上次是僥倖選上，是因為海嘯把他帶上來，所以成績當然不是那麼好，現在給他4年的機會，就變成了「民進黨的宋楚瑜」，所以票當然好。\n",
      "關鍵字: ['鄭文燦', '沈富雄']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3771/d3771098.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338899.htm\n",
      "url is not exists!\n",
      "新聞標題: 卡蛋被離職 林洲民：對遠雄放水是柯文哲政治生命結束的開始\n",
      "發佈時間: 2018/12/24 10:55\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 大巨蛋遲遲無法復工，傳出台北市長柯文哲將撤換都發局長林洲民，改由台中市祕書長黃景茂接任，林洲民則轉任都更中心董事長，但此人事異動讓林洲民不滿，在臉書開砲市府有「趙友友」。據《聯合報》報導，林洲民被要求離職，讓他直指對遠雄放水就是柯文哲政治生命結束的開始。大巨蛋卡蛋3年半，傳出原因出在林洲民。林洲民2015年5月以遠雄未按圖施工，勒令大巨蛋停工，同年12月他在都市設計審議會上要求遠雄撤件，後來遠雄在2017年向北市府重新遞件，但都審會委員及林洲民強力要求，雙方沒達到共識，至今復工無望。柯文哲過去對遠雄態度強硬，但隨著大巨蛋遲遲無法復工，大巨蛋成為揮不去的陰影，對遠雄態度也逐漸放軟，據了解，柯文哲曾在內部會議痛罵副市長林欽榮及林洲民，也怪罪兩人讓自己選票流失。柯文哲25日連任市長當天，將宣布小內閣名單，如今卻傳出都發局長一職將異動，由黃景茂接掌，林洲民則調任到都更中心董事長。面對人事異動的傳聞，林洲民在臉書發文，以「趙友友」暗指與遠雄集團趙藤雄交好的人士，文中指出，「趙友友友友們，請適可而止！我，林洲民，在過去兩個星期，親眼看見，親耳聽到：『把林洲民移開，大巨蛋審查，比較容易通過！』。」此外，據《聯合報》報導，林洲民在和友人私人訊息中爆料，自己被副市長鄧家基、祕書長張哲揚及市長辦公室主任李文宗等人先後約談，要求離職，原因是公宅跳票、執行不力及大巨蛋審查找遠雄麻煩。據了解，鄧家基、張哲揚要求林洲民離職，並詢問是否願意調任都更中心董事長，林洲民多次問「這真的是柯文哲的意思嗎？這是你們的話，還是市長的話？」，並要求和柯文哲面談。林洲民質疑，是不是自己「卡蛋」才被拔官，但當場都否認；他還諷刺蔡壁如、李文宗是十足「趙友友」，連帶影響柯文哲的判斷，他相信對遠雄放水，就是柯文哲政治生命結束的開始。對此，張哲揚表示，當天沒提到大巨蛋三字，大家肯定林洲民在建築設計上的能力，但選舉期間，北市府公宅進度飽受外界抨擊，這是管理問題，至於轉任都更中心董事長，則是希望繼續借重林洲民的專長。李文宗也指出，北市府每周針對大巨蛋都有開會，一向公開透明，自己沒遠雄的私人電話，更沒私下見面，怎會是「趙友友」？讓林洲民轉任都更中心董事長，純粹是選後職務調動，且政務官任用是市長的人事權。\n",
      "關鍵字: ['大巨蛋', '林洲民', '遠雄', '柯文哲']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3784/d3784010.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338858.htm\n",
      "url is not exists!\n",
      "新聞標題: 傳吳釗燮有望回鍋國安會 部長機要趙怡翔將外派副代表\n",
      "發佈時間: 2018/12/24 09:39\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 行政院年後改組，外交部、國安會人事也將調整。據媒體報導，有黨政高層透露，外交部長吳釗燮有望回鍋國安會祕書長，而新任外交部長人選將在駐美代表高碩泰與駐歐盟代表曾厚仁擇一，此外，部長機要趙怡翔可能外派為副代表。據媒體報導，李大維擔任外交部長時，吳釗燮時任國安會祕書長，卻無法插手外交部的細部政策與人事，因此，國安會與外交部常有不同意見；直到今年2月，兩人職務互調，吳釗燮得以掌握對外宣傳力度、對內政策觀念，在人事任命上異動頻繁，拔擢年輕同仁，並勤於至各館處巡視，對外交部大幅掌握。吳釗燮上任以來，邦交國布吉納法索、多明尼加、薩爾瓦多接連與台灣斷交，在野黨多次要求吳釗燮下台以示負責，但吳釗燮仍是民進黨內少數具有國際視野人士，故極受總統蔡英文的器重，年後改組有望回鍋國安會，擔任祕書長，而李大維則結束「階段性任務」。據了解，若吳釗燮調回國安會秘書長，新任外交部長人選將在高碩泰與曾厚仁擇一；過去李大維擔任外交部長時，高碩泰有時會跳過外交部，直接向吳釗燮報告；而曾厚仁在蔡政府上任後，被延攬出任總統府副祕書長，是蔡英文重要幕僚，隨後轉任國安會副祕書長，高、曾兩人與吳釗燮都有工作默契。另一方面，據了解，吳釗燮將安排其機要趙怡翔出任公使或副代表，消息傳出在部內引發抨擊；趙怡翔現年31歲，2年前在蔡英文就職總統大典上，以一口流利英文，獲封「口譯哥」，隨後跟著吳釗燮任國安會、外交部機要，傳出趙怡翔的辦公室已清空，外派人事已在作業，去處可能是美或歐洲等館處。\n",
      "關鍵字: ['吳釗燮', '國安會', '趙怡翔']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3629/d3629336.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338807.htm\n",
      "url is not exists!\n",
      "新聞標題: 孫大千批國民黨老將變不出新把戲 「人民期待過更好的生活」\n",
      "發佈時間: 2018/12/24 08:54\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 前國民黨立委孫大千24日在臉書表示，當國民黨的老將都變不出新把戲的時候，黨為什麼不能夠大公無私，為國舉才？台灣人民根本不在意國民黨能不能拿回政權，但期待能夠過更好的生活。孫大千認為，為什麼國民黨的總統候選人一定要從那幾位「老掉牙的太陽」當中，挑一位來參選？難道，這20年來國民黨沒有其他人才讓人民選擇？誰規定一定要有顯赫的從政資歷，才有資格參選總統。孫大千指出，國民黨內有意參選的諸位太陽們，除了全台走透透、出書爆料明志、成立競選辦公室、等待人民的聲音、以及坐等黃袍加身之外，卻沒有一個人，能夠勇敢提出台灣人民當前最在意的問題，以及解決的方案。孫大千表示，國民黨「老天王」和「舊太陽」們始終不願意改變和轉型，還喊著過時的口號，和擺弄著老派的身段，「那麼，為什麼國民黨不能夠本著天下為公的胸懷，向外去尋找更多適合領導台灣的總統人選呢？」孫大千強調，台灣人民不在意國民黨能不能拿回政權，但強烈期待能夠過更好的生活，2020年任何一個政黨都可以輸，但台灣不能夠再輸了。\n",
      "關鍵字: ['孫大千', '國民黨', '吳敦義', '王金平', '馬英九', '朱立倫']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3757/d3757352.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338674.htm\n",
      "url is not exists!\n",
      "新聞標題: 談高雄「賽馬夢」 韓國瑜：講出去我自己也嚇一跳！\n",
      "發佈時間: 2018/12/24 00:03\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 準高雄市長韓國瑜選後的一舉一動都被放大檢視，日前談到楠梓區高雄煉油廠舊址該如何活化時，拋出在此興建「賽馬場」的點子引發外界熱議。韓國瑜23日出席維多利亞雙語學校校慶時回應，這是他跟工商界人士談的，想法尚未成熟，「結果他們講出去了我自己也嚇了一跳」。韓國瑜20日出席「與韓國瑜準市長有約－工商建研會政策建言」時，提到將已關廠的中油高雄煉油廠轉型為賽馬場，希望以賽馬產業鏈拚經濟，消息一出引發外界熱議。農委會則表示，《動物保護法》第10條規定，不得以直接、間接賭博為目的，利用動物進行競技行為，違者可罰5萬至25萬元，若韓國瑜真的要建賽馬場，以現行法規來說確實違法。法界人士也說，現行《刑法》仍有賭博罪處罰規定，要靠立博弈專法才能設賽馬場。此外，關懷生命協會創會理事長釋昭慧與動保人士不約而同抨擊韓國瑜此舉「違法又殘忍」，釋昭慧更說，若高雄發展賽馬產業，「絕對反對到底」。準新聞局長王淺秋21日澄清，這只是創意發想，還沒有具體政策，必須等團隊上任後深入討論，賽馬場也不一定要蓋在高雄煉油廠，如果有可能希望一月能赴香港考察。韓國瑜23日出席維多利亞雙語學校校慶時回應，「賽馬是我跟工商界人士談的，這是一個我自己的想法，未必完全成熟希望大家保密。不要講出去，拜託他們。結果他們講出去了我自己也嚇了一跳」。\n",
      "關鍵字: ['高雄', '韓國瑜', '賽馬場']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3735/d3735762.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181223/1338704.htm\n",
      "url is not exists!\n",
      "新聞標題: 大學姊悄轉戰台中！柯P「她沒跟我講」…盧秀燕「曝光隱瞞原因」\n",
      "發佈時間: 2018/12/23 23:01\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 準台中市長盧秀燕23日公布第5波人事，觀光局長由台北市長柯文哲競選辦公室發言人、「大學姊」林筱淇出任，成了這波人事的最大亮點；柯文哲得知後坦言，「她（林筱淇）去台中事先沒有跟我講！」對此，盧秀燕也緩頰，表示人事在發布前有要求林筱淇保密，因此她才未事先告知柯P。「用人不分黨派！」盧秀燕表示，先前就曾注意到林筱淇的口條清晰、態度穩健，對她留下很好的印象，因此主動與她聯繫，原本屬意要讓林擔任台中市府發言人，但在知道她的專長是行銷管理後，認為是個拚經濟、拚觀光的好人才，因此改讓她出任觀光局長。面對記者提問，是否有事先告知柯市長；林筱淇表示，先前曾於臉書發文找工作，這一事有告知柯P，「他知道我在找工作，但不知道後續找到怎樣的工作，相信今天記者會後就會知道我的新工作了！」現年40歲的林筱淇已婚，先生是台中市的開業醫師。林筱淇說，台中市是她的第二故鄉，原本新工作就鎖定在台北、台中兩地，當時盧秀燕主動與她聯繫感到相當訝異，「我跟盧市長沒有見過面、也不認識，但2人聊過後感受到她的溫暖、親切，稍加評估後決定接受挑戰！」對於這一人事案，柯文哲23日晚間受訪時表示，先前有詢問林筱淇要不要去選立委，但她意願似乎不高，「後來她說要去找工作，就這樣，她去台中事先沒有跟我講。盧秀燕也幫忙緩頰，表示人事發布前都有先要求當事人保密，避免受到不必要的干擾，維持一個純淨的空間、確保發布人事順利成功，「這也是林筱淇沒有跟柯報告新工作的原因。」\n",
      "關鍵字: ['林筱淇', '盧秀燕', '台中市', '觀光局', '柯文哲', '大學姊']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3783/d3783968.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181223/1338717.htm\n",
      "url is not exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新聞標題: 影／不能說假新聞男大生沒有關係！謝長廷深夜PO文回應了：為幫蘇啓誠解套\n",
      "發佈時間: 2018/12/23 22:58\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 前大阪辦事處處長蘇啓誠遺孀日前公開聲明，強調丈夫是在完成上級交代之檢討報書後，表明「不想受到羞辱」之遺言，以死明志。對此，駐日代表謝長廷23日在臉書表示，看了蘇啓誠「願坦然受處」的檢討報告後，便「鍥而不捨」的查出假消息的散波者，目的就是要幫他解套，而非外界所指的卸責。謝長廷指出，他在9月19日看到9月10日蘇啓誠寫的「願坦然受處」的檢討報告，內容是以巴士是載到泉佐野站及guruguru的電話為真的前提而檢討，他認為，若此事為真，那麼蘇啓誠願受處分的檢討報告會變成可以處分的根據，問題只是處分輕重及是否過當的問題。謝長廷表示，關西機場事件受困旅客輸運完畢的第一天（9/6），他就在臉書指出所謂「中國領事館派車優先救出中國人」是假新聞，後來對照網路資料，更發現guruguru講說他打電話給大阪辦事處遭到冷漠嘲諷的事情，也應該是假消息。謝長廷說，自己鍥而不捨追出假新聞率先散播者guruguru的身分，以及他打電話是假的，還有坐到泉佐野車站也是假的真相，「正是為蘇處長自我檢討報告的前提解套」，因為，蘇啓誠在檢討報告中說，自己沒有前往車站關心難辭其咎，謝長廷認為，外界追究檢討報告是否被迫？上級是否受新聞誤導而處分？處分是否過當？這些問題，既然雙方都通聯記錄可查，應該很簡單可以查明。謝長廷強調，如查出蘇啓誠輕生之前有接獲上級壓力電話而不甘受辱輕生，那應該是直接促使他尋短最後一根稻草，而不能說假新聞沒有關係，現在guruguru尋求國民黨立委保護，在政黨刻意庇護下，當時利用假新聞率先攻擊阪辦事處的委員們一下子説要為蘇啓誠申寃，但一下子又要保護假新聞的造謠者，竟然說guruguru講的也都是事實。謝長廷質疑，如果這樣，那為什麼不勸他堂堂正正出來解釋清楚，他為什麼一再逃避說謊，扯什麼帳號被盜用呢？「一切都是事實」這種說法是要陷蘇前處長於不義？還是真的要為他申寃？令人費解。針對連日來外界對蘇啓誠輕生的各種揣測，謝長廷表示，媒體引用許多「知情人士、關鍵人士、消息靈通人士、可靠人士的話，講了不少猜測，情節也許引人，但多數也是不可靠的假消息」，他強調，究竟他們是誰？有多關鍵？多可靠？多靈通？講話內容的根據什麼？有什麼不能讓人知道真名的理由？「希望社會大眾冷靜思考一下，就不會隨機起舞。」\n",
      "關鍵字: ['謝長廷', '蘇啓誠']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3727/d3727499.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181223/1338585.htm\n",
      "url is not exists!\n",
      "新聞標題: 影／9台人受困印尼海嘯找到了！ 外交部：僅擦傷、已協助就醫 \n",
      "發佈時間: 2018/12/23 22:08\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 印尼巽他海峽（Sunda Strait）22日晚間發生海嘯侵襲，外交部原先已協助救援6名受困的台人，後來又因為新一波海嘯的襲擊，受困人數又多了3名，共導致9人受困，我駐印尼代表持續努力聯繫後，印尼救援隊已接到上述9人，正將我國人送往當地醫院的途，外交部表示，確認相關國人均屬擦傷，並無大礙，根據印尼國家災害應變總署印尼雅加達時間23日下午5時33分發布新聞稿表示，截至本日下午4時止，計有222人死亡、843受傷、28人失蹤，傷亡慘重。外交部新聞稿指出，受困的吳女士夫婿等印尼籍親屬受限於救援隊人數限制雖未同車，但已於隨後到達醫院，駐處人員並即提供手機供家屬聯繫國內親屬報平安，驚醫生評估住院國人身體狀況許可，吳女士及親友等9人將於今晚轉院至鄰近雅加達地區的醫院。外交部表示，有關我國人因海嘯受困事，印尼中央政府各相關機關及災區萬丹省政府均表達協助意願，與我駐處密切聯繫。駐處已派員協同三輪基金會等僑社團體前往營救，雖山上大雨不斷，我駐處同仁仍將突破困難，與我國人會合，以提供更直接有效的協助。由於印尼海嘯災情重大，總統蔡英文及外交部均在推特對本案受害者表達慰問，也已整備四架運輸機待命，一旦我國人或印尼方運輸、救援有需求，可隨時準備出發；另我駐處也將持續與印尼國家災害應變總署(BNPB)等相關單位保持聯繫，以持續瞭解是否有其他國人遭受海嘯波及，同步密切掌握最新發展。外交部提到，我國駐印尼代表處啟動因大阪風災之後檢討建立的急難救助系統，結合印尼各救災資源部門及台商，以提供國人有效協助。\n",
      "關鍵字: ['印尼海嘯', '外交部']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3784/d3784381.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "第 3 頁   Processing:  https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1&idx=1&page=3\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181223/1338646.htm\n",
      "url is not exists!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-085761ab5e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mettoday_crawler2kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-728b99e7b14a>\u001b[0m in \u001b[0;36mettoday_crawler2kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url is not exists!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mbf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_url_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-c495d598a07a>\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, str_input)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetbit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36msetbit\u001b[0;34m(self, name, offset, value)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \"\"\"\n\u001b[1;32m   1404\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SETBIT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36mrelease\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;34m\"Releases the connection back to the pool\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36m_checkpid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_checkpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ettoday_crawler2kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將json檔每則新聞作者取出用方法清理\n",
    "for i in range(0, len(json_data)):  \n",
    "    if json_data[i][\"author\"] == []:\n",
    "        author = []\n",
    "        print(author)\n",
    "    else:\n",
    "        tvbs_author_etl(json_data[i][\"author\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming從每單一json檔取出新聞作者用方法清理\n",
    "if j[\"author\"] == []:\n",
    "    author = []\n",
    "    print(author)\n",
    "else:\n",
    "    tvbs_author_etl(j[\"author\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/2.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/3.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/4.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/5.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/6.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/7.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/8.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/9.html \n",
      "Done, Time cost: 11.269868850708008 \n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import threading, queue, time, urllib\n",
    "from  urllib import request\n",
    "baseUrl = 'http://www.pythontab.com/html/pythonjichu/'\n",
    "urlQueue = queue.Queue()\n",
    "for i in range(2, 10):\n",
    "    url = baseUrl + str(i) + '.html'\n",
    "    urlQueue.put(url)\n",
    "    #print(url)\n",
    "def fetchUrl(urlQueue):\n",
    "    while True:\n",
    "        try:\n",
    "            #不阻塞的读取队列数据\n",
    "            url = urlQueue.get_nowait()\n",
    "            i = urlQueue.qsize()\n",
    "        except Exception as e:\n",
    "            break\n",
    "        print ('Current Thread Name %s, Url: %s ' % (threading.currentThread().name, url))\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            responseCode = response.getcode()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if responseCode == 200:\n",
    "            #抓取内容的数据处理可以放到这里\n",
    "            #为了突出效果， 设置延时\n",
    "            time.sleep(1)\n",
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    threads = []\n",
    "    # 可以调节线程数， 进而控制抓取速度\n",
    "    threadNum = 2\n",
    "    for i in range(0, threadNum):\n",
    "        t = threading.Thread(target=fetchUrl, args=(urlQueue,))\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        #多线程多join的情况下，依次执行各线程的join方法, 这样可以确保主线程最后退出， 且各个线程间没有阻塞\n",
    "        t.join()\n",
    "    endTime = time.time()\n",
    "    print ('Done, Time cost: %s ' %  (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
