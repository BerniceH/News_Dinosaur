{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬蟲 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4\n",
    "#!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib import request\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install redis\n",
    "import redis\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHash(object):\n",
    "    def __init__(self, cap, seed):\n",
    "        self.cap = cap\n",
    "        self.seed = seed\n",
    "\n",
    "    def hash(self, value):\n",
    "        ret = 0\n",
    "        for i in range(len(value)):\n",
    "            ret += self.seed * ret + ord(value[i])\n",
    "        return (self.cap - 1) & ret\n",
    "\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, host='redis', port=6379, db=0, blockNum=1, key='bloomfilter'):\n",
    "        \"\"\"\n",
    "        :param host: the host of Redis\n",
    "        :param port: the port of Redis\n",
    "        :param db: witch db in Redis\n",
    "        :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it.\n",
    "        :param key: the key's name in Redis\n",
    "        \"\"\"\n",
    "        self.server = redis.Redis(host=host, port=port, db=db)\n",
    "        self.bit_size = 1 << 31  # Redis的String类型最大容量为512M，现使用256M\n",
    "        self.seeds = [5, 7, 11, 13, 31, 37, 61]\n",
    "        self.key = key\n",
    "        self.blockNum = blockNum\n",
    "        self.hashfunc = []\n",
    "        for seed in self.seeds:\n",
    "            self.hashfunc.append(SimpleHash(self.bit_size, seed))\n",
    "\n",
    "    def isContains(self, str_input):\n",
    "        if not str_input:\n",
    "            return False\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        ret = True\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            ret = ret & self.server.getbit(name, loc)\n",
    "        return ret\n",
    "\n",
    "    def insert(self, str_input):\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            self.server.setbit(name, loc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BloomFilter方法:處理資料去重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = BloomFilter()\n",
    "# url = \"https://tw.news.appledaily.com/politics/realtime/20181125/1473059/\"\n",
    "# url = url.encode('utf-8')\n",
    "# if bf.isContains(url):   # 判断字符串是否存在\n",
    "#     print('url is exists!')\n",
    "# else:\n",
    "#     print('url is not exists!')\n",
    "#     bf.insert(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloomfilter'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BloomFilter的key\n",
    "bf.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = redis.ConnectionPool(host='redis', port=6379, db=0)\n",
    "r = redis.StrictRedis(connection_pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'bloomfilter0']\n"
     ]
    }
   ],
   "source": [
    "#搜尋redis裡面有哪些keys\n",
    "keys = r.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#刪除redis裡key為XXX\n",
    "r.delete(\"bloomfilter0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "傳送log 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-logstash\n",
    "import logging\n",
    "import logstash\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"logstash\"\n",
    "crawler_logger = logging.getLogger('crawler_logger')\n",
    "crawler_logger.setLevel(logging.INFO)\n",
    "\n",
    "# TCP\n",
    "crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Producer 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kafka\n",
    "from kafka import KafkaProducer\n",
    "import sys\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "連線Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    # Kafka集群在那裡?\n",
    "    bootstrap_servers = [\"kafka1:29092\"],\n",
    "    # 指定msgKey的序列化器, 若Key為None, 無法序列化, 透過producer直接給值\n",
    "    key_serializer = str.encode,\n",
    "    # 指定msgValue的序列化器\n",
    "    value_serializer = lambda m: json.dumps(m).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVBS 爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvbs_crawler2kafka():\n",
    "    try:\n",
    "        ###TVBS爬蟲開始###\n",
    "        url = \"https://news.tvbs.com.tw/politics/\"\n",
    "        response = urlopen(url)\n",
    "        soup = BeautifulSoup(response)\n",
    "        news_ul = soup.find_all(\"ul\", id = \"block_pc\")[0]\n",
    "        news_a = news_ul.find_all(\"a\")\n",
    "\n",
    "        news_no_string_list = []\n",
    "        for a in news_a:\n",
    "            news_no_string = a[\"data-news-id\"]\n",
    "            news_no_string_list.append(news_no_string)\n",
    "\n",
    "        news_no_list = []\n",
    "        for n in news_no_string_list:\n",
    "            news_no_int = n.strip('\\'')\n",
    "            news_no_list.append(news_no_int)\n",
    "\n",
    "        for news_no in news_no_list:\n",
    "\n",
    "            print(\"新聞來源:\", \"TVBS新聞台\")\n",
    "\n",
    "            news_url = \"https://news.tvbs.com.tw/politics/\" + news_no\n",
    "            print(\"新聞網址:\", news_url)\n",
    "\n",
    "\n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "\n",
    "\n",
    "            req = requests.get(news_url)\n",
    "            # requests如果找不到指定編碼，會猜測網頁編碼，有時會形成亂碼，故給指定編碼utf-8\n",
    "            req.encoding = (\"utf-8\")\n",
    "            soup = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "            title = soup.find(\"h1\", class_ = \"margin_b20\").text\n",
    "            title = re.sub(r\"\\u3000\", \" \", title)\n",
    "            title = re.sub(r\"\\xa0\", \" \", title)\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = soup.find(\"div\", class_ = \"icon_time time leftBox2\").text\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "            author = []\n",
    "            author_text = soup.find(\"h4\", class_ = \"font_color5 leftBox1\").text\n",
    "            author_text = re.sub(r\"攝影.*報導\", \"\", author_text)\n",
    "            author.append(author_text)\n",
    "            print(\"作者:\", author)\n",
    "\n",
    "            content = soup.find(\"div\", class_ = \"h7 margin_b20\").text\n",
    "            content = content.replace(\"\\t\", \"\").replace(\"   \",\"\").replace(\"\\n\", \"\").replace(\"（中央社）\",\"\")\n",
    "            content = content.replace(\"最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "            content = content.replace(\"最HOT話題在這！想跟上時事，\",\"\").replace(\"快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "            content = content.replace(\"TVBS最新大數據分析\",\"\").replace(\"看完整內容快點我加入TVBS新聞LINE好友！\",\"\")\n",
    "            print(\"內文:\", content)\n",
    "\n",
    "            keyword = soup.find(\"div\", class_ = \"adWords\").text\n",
    "            keyword = keyword.replace(\"\\t\", \"\")\n",
    "            keyword = keyword.replace(\",\",\"\")\n",
    "            keyword = keyword.replace(\"\\n\", \"\").replace(\"編輯  \",\"\").replace(\" 報導\",\"\")\n",
    "            keyword = keyword.replace(\"記者\",\"\").replace(\"      \",\",\")\n",
    "            keyword = keyword.split(\",\")\n",
    "            print(\"關鍵字\", keyword)\n",
    "            \n",
    "            \n",
    "            img = soup.find(\"div\", class_ = \"margin_b20\")\n",
    "            if not img == None:\n",
    "                img_url = img.find('img')['src']\n",
    "            else:\n",
    "                img_url = \"\"\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            j = {\"source\": \"TVBS新聞台\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(1)\n",
    "\n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash: TVBS crawler success!')\n",
    "            crawler_logger.handlers.clear()\n",
    "        ###TVBS爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"politics_news\"\n",
    "\n",
    "            print(\"Now sending messages ...\")\n",
    "            # 步驟3.產生要發佈到Kafka的訊息\n",
    "            # - 參數  # 1: topicName\n",
    "            # - 參數  # 2: msgKey\n",
    "            # - 參數  # 3: msgValue\n",
    "\n",
    "            producer.send(topic = topic_name, key = \"tvbs\", value = j)\n",
    "            print(\"Message has finished sending!\", \"\\n\")\n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: TVBS crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETN爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setn_crawler2kafka():\n",
    "    try:\n",
    "        ###SETN爬蟲開始###\n",
    "        for page in range(1, 5):\n",
    "            page_url = \"https://www.setn.com/ViewAll.aspx?PageGroupID=6&p=\" + str (page)\n",
    "            print (\"Processing:\", page_url)\n",
    "\n",
    "            response = urlopen (page_url)\n",
    "            bs = BeautifulSoup (response)\n",
    "            titles = bs.find_all (\"h3\", \"view-li-title\")\n",
    "\n",
    "            if len (titles) == 0:\n",
    "                break\n",
    "\n",
    "            for t in titles:\n",
    "\n",
    "                print(\"新聞來源:\", \"SETN三立新聞網\")\n",
    "\n",
    "                news_url = \"https://www.setn.com\" + t.find (\"a\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "\n",
    "\n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "\n",
    "                req = requests.get (news_url)\n",
    "                bs = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "                title = bs.find('h1', class_ = \"news-title-3\").text\n",
    "                title = re.sub(r\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = bs.find(\"time\", class_ = \"page-date\").text.strip()\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                author = []\n",
    "                a_divs = bs.find(\"div\", id=\"Content1\")\n",
    "                first_p = a_divs.find(\"p\")\n",
    "                author_text = first_p.text\n",
    "                #找第一段有沒有含\"／\"，如果沒有給予空字串\n",
    "                if author_text.find(\"／\") == -1:\n",
    "                    author_text = \"\"\n",
    "                #如果作者是空字串，給予空list\n",
    "                if author_text == \"\":\n",
    "                    author = []\n",
    "                else:\n",
    "                    author.append(author_text)\n",
    "                print(\"作者:\", author)\n",
    "\n",
    "                c_divs = bs.find('div', itemprop='articleBody')\n",
    "                content_all = c_divs.find_all (\"p\")\n",
    "                content = \"\"\n",
    "                for c in content_all:\n",
    "                    if c.attrs == {}:\n",
    "                        content = content + c.text\n",
    "                        #print(c.text)\n",
    "\n",
    "                        c_divs = bs.find('div', itemprop='articleBody')\n",
    "                content_all = c_divs.find_all (\"p\")\n",
    "                content = \"\"\n",
    "                for c in content_all:\n",
    "                    if c.attrs == {}:\n",
    "                        content = content + c.text\n",
    "                        #print(c.text)\n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                keyword = []\n",
    "                k_divs = bs.find(\"div\", class_=\"keyword page-keyword-area\")\n",
    "                k_strong = k_divs.find_all(\"strong\")\n",
    "                for k in k_strong:\n",
    "                    keyword_text = k.text\n",
    "                    keyword.append(keyword_text)\n",
    "                print(\"關鍵字:\", keyword)\n",
    "                \n",
    "                \n",
    "                img_p = bs.find(\"p\", style = \"text-align: center;\")\n",
    "                if not img_p == None:\n",
    "                    img_url = img_p.find(\"img\")[\"src\"]\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "                j = {\"source\": \"SETN三立新聞網\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                     \"kw\": keyword, \"img_url\": img_url}\n",
    "                time.sleep (1)\n",
    "\n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash: SETN crawler success!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        ###SETN爬蟲結束###\n",
    "    \n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"politics_news\"\n",
    "\n",
    "                print(\"Now sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"setn\", value = j)\n",
    "                print(\"Message has finished sending!\", \"\\n\")\n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: SETN crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AppleDaily爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appledaily_crawler2kafka():\n",
    "    try:\n",
    "        ###AppleDaily爬蟲開始###\n",
    "        for page in range(1, 12):\n",
    "            time.sleep(2)\n",
    "            href = \"https://tw.news.appledaily.com/politics/realtime/\" + str(page)\n",
    "            res = requests.get(href)\n",
    "            html = BeautifulSoup(res.text)\n",
    "\n",
    "            all_news_1 =  html.find_all(\"ul\", class_= \"rtddd slvl\")\n",
    "            for all_news in all_news_1:\n",
    "                news = all_news.find_all(\"a\")\n",
    "                print(\"第\", page, \"頁\")\n",
    "                for n in news:\n",
    "\n",
    "                    print(\"新聞來源:\", \"蘋果日報\")\n",
    "\n",
    "                    #my_news = {}\n",
    "                    news_url =\"https://tw.news.appledaily.com/\" + str(n[\"href\"])\n",
    "                    print(\"新聞網址:\", news_url)\n",
    "\n",
    "                    #判斷網址是否爬過\n",
    "                    bf = BloomFilter()\n",
    "                    # Unicode-objects must be encoded before hashing\n",
    "                    news_url_u = news_url.encode('utf-8')\n",
    "                    if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                        print('url is exists!')\n",
    "                        continue\n",
    "                    else:\n",
    "                        print('url is not exists!')\n",
    "                        bf.insert(news_url_u)\n",
    "\n",
    "                    news_per = requests.get(news_url)\n",
    "                    bs = BeautifulSoup(news_per.text)\n",
    "\n",
    "                    title = bs.find(\"h1\").text\n",
    "                    title = re.sub(r\"\\u3000\", \" \", title)\n",
    "                    print(\"新聞標題:\", title)           \n",
    "\n",
    "                    date = bs.find(\"div\", class_=\"ndArticle_creat\").text.replace(\"出版時間：\", \"\")\n",
    "                    print(\"發佈時間:\", date)\n",
    "\n",
    "                    if not bs.find(\"div\", class_=\"ndgKeyword\") == None:\n",
    "                        key_word = bs.find(\"div\", class_=\"ndgKeyword\").find_all(\"a\")\n",
    "                        key_list = []\n",
    "                        for k in key_word:\n",
    "                            key_list.append(k.text)\n",
    "                    else:\n",
    "                        key_list = []       \n",
    "\n",
    "                    content_dir = bs.find(\"div\", class_=\"ndArticle_margin\")\n",
    "                    for a in content_dir.find_all(\"a\"):\n",
    "                        if not a == None:\n",
    "                            a.extract()\n",
    "                    for s in content_dir.find_all(\"span\"):\n",
    "                        if not s == None:\n",
    "                            s.extract()\n",
    "                    for i in content_dir.find_all(\"iframe\"):\n",
    "                        if not i == None:\n",
    "                            i.extract()\n",
    "                    for d in content_dir.find_all(\"div\"):\n",
    "                        if not d == None:\n",
    "                            d.extract()\n",
    "                    for st in content_dir.find_all(\"style\"):\n",
    "                        if not st == None:\n",
    "                            st.extract()\n",
    "                    content = content_dir.text.strip()\n",
    "                    content = content.replace(\"）\", \")\").replace(\"（\", \"(\").replace(\"／\", \"/\").replace(\"╱\", \"/\")       \n",
    "                    pat = re.compile(r\"\\(((.{0,15})/.{0,8}[\\u5831][\\u5c0e])\\)\")\n",
    "                    pat_2 = re.compile(r\"【((.{0,15})/.{0,8}[\\u5831][\\u5c0e])】\")\n",
    "                    ans_news = pat.search(content)\n",
    "                    ans_news_2 = pat_2.search(content)\n",
    "                    if ans_news == None:\n",
    "                        author = []\n",
    "                    elif not ans_news_2 == None:\n",
    "                        author = ans_news[2].split(\"、\")\n",
    "                    else:\n",
    "                        #print(ans_news[2])\n",
    "                        author = ans_news[2].split(\"、\")\n",
    "                        content = content.split(ans_news[0])[0]\n",
    "                    print(\"作者:\", author)   \n",
    "                    print(\"內文:\", content)  \n",
    "                    print(\"關鍵字:\", key_list)\n",
    "\n",
    "                    img_box = bs.find(\"div\", class_=\"ndAritcle_headPic\")\n",
    "                    if not img_box == None:\n",
    "                        img_url = img_box.find(\"img\")['src']\n",
    "                    else:\n",
    "                        img_url = \"\"\n",
    "                    print(\"圖片網址:\", img_url)\n",
    "\n",
    "                    j = {\"source\": \"蘋果日報\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                         \"content\": content, \"kw\": key_list, \"img_url\": img_url}\n",
    "                    time.sleep(random.randint(1, 2))\n",
    "\n",
    "                    # TCP連線到logstash\n",
    "                    crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                    # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                    crawler_logger.info('python-crawler-logstash: AppleDaily crawler success!')\n",
    "                    crawler_logger.handlers.clear()\n",
    "        ###AppleDaily爬蟲結束###\n",
    "\n",
    "                    # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                    topic_name = \"politics_news\"\n",
    "\n",
    "                    print(\"Now sending messages ...\")\n",
    "                    # 步驟3.產生要發佈到Kafka的訊息\n",
    "                    # - 參數  # 1: topicName\n",
    "                    # - 參數  # 2: msgKey\n",
    "                    # - 參數  # 3: msgValue\n",
    "\n",
    "                    producer.send(topic = topic_name, key = \"appledaily\", value = j)\n",
    "                    print(\"Message has finished sending!\", \"\\n\")\n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: AppleDaily crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOWnews爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nownews_crawler2kafka():\n",
    "    try:\n",
    "        ###NOWnews爬蟲開始###\n",
    "        for page in range(1,6):\n",
    "            href = 'https://www.nownews.com/cat/politics/page/'+str(page)\n",
    "\n",
    "            response = requests.get(href)\n",
    "            soup=BeautifulSoup(response.text)\n",
    "            news = soup.find_all(\"h3\",class_=\"entry-title td-module-title\")\n",
    "\n",
    "            for n in news:\n",
    "                \n",
    "                print(\"新聞來源:\", \"NOWnews\")\n",
    "\n",
    "                news_url = n.find(\"a\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "\n",
    "\n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "\n",
    "                response = requests.get(news_url)\n",
    "                html = BeautifulSoup(response.text)\n",
    "                title = html.find(\"h1\",class_=\"entry-title\").text\n",
    "                title = re.sub(\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = html.find(\"time\",class_ = \"entry-date\").text\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                author = []\n",
    "                author_text = html.find(\"div\",class_=\"td-post-author-name\").text\n",
    "                author_text = author_text.split(\"／\")[0]\n",
    "                author_text = author_text.split(\"/\")[0].replace(\"記者\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                author.append(author_text)\n",
    "                print(\"作者:\", author)\n",
    "\n",
    "                te = html.find(\"div\", class_=\"td-post-content\").contents\n",
    "                #print(te)\n",
    "                #print(te[0].name == None)\n",
    "                content = \"\"\n",
    "                for a in te:\n",
    "                    if a.name == \"p\":\n",
    "                        content = content + a.text\n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                #處理keyword\n",
    "                if not html.find(\"ul\",class_=\"td-tags\") == None:\n",
    "                    kw = html.find(\"ul\",class_=\"td-tags\").find_all(\"a\")\n",
    "                    keyword = []\n",
    "                    for k in kw:\n",
    "                        keyword.append(k.text)\n",
    "                else:\n",
    "                    keyword = []\n",
    "                print(\"關鍵字:\", keyword)\n",
    "\n",
    "                img_url = html.find(\"div\",class_=\"td-post-featured-image\")\n",
    "                if not img_url == None:\n",
    "                    img_url = img_url.find(\"img\")[\"src\"]\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "                j = {\"source\": \"NOWnews\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                     \"kw\": keyword, \"img_url\": img_url}\n",
    "                time.sleep(random.randint(0,2))\n",
    "\n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash: NOWnews crawler success!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        ###NOWnews爬蟲結束###\n",
    "    \n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"politics_news\"\n",
    "\n",
    "                print(\"Now sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"nownews\", value = j)\n",
    "                print(\"Message has finished sending!\", \"\\n\")\n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: NOWnews crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORM爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storm_crawler2kafka():\n",
    "    try:\n",
    "        ###STORM爬蟲開始###    \n",
    "        for page in range(1,5):\n",
    "            href = 'https://www.storm.mg/category/118/'+str(page)\n",
    "\n",
    "            response = urlopen(href)\n",
    "            html = BeautifulSoup(response)\n",
    "            #找到特定範圍在選取網址列\n",
    "            newf = html.find(\"div\",class_ = \"middle_category_cards\")\n",
    "            #選取所有單篇網址列\n",
    "            news = newf.find_all(\"div\",class_ = \"category_card\")\n",
    "            #迴圈抓取整頁新聞連結\n",
    "            for r in news:\n",
    "\n",
    "                print(\"新聞來源:\", \"風傳媒 THE STORM MEDIA\")\n",
    "\n",
    "                news_url = r.find(\"a\",class_ = \"card_link\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "\n",
    "\n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "\n",
    "                response = urlopen(news_url)\n",
    "                html = BeautifulSoup(response)\n",
    "\n",
    "                title = html.find(\"h1\", id = \"article_title\").text\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = html.find(\"span\", class_=\"info_time\").text\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                author = []\n",
    "                author_text = html.find(\"span\", class_=\"info_author\").text.replace(\"新新聞\",\"\")\n",
    "                author.append(author_text)\n",
    "                print(\"作者:\", author)\n",
    "\n",
    "                content = html.find(\"div\", id=\"CMS_wrapper\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                keyword = html.find(\"div\", id=\"tags_list_wrapepr\").text.split()\n",
    "                print(\"關鍵字:\", keyword)\n",
    "\n",
    "                img_url = html.find(\"img\", id=\"feature_img\")\n",
    "                if not img_url == None:\n",
    "                    img_url = img_url[\"src\"]\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "                \n",
    "                j = {\"source\": \"風傳媒 THE STORM MEDIA\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                     \"content\": content, \"kw\": keyword, \"img_url\": img_url}\n",
    "                time.sleep(random.randint(0,2))\n",
    "\n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash: STORM crawler success!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        ###STORM爬蟲結束###\n",
    "    \n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"politics_news\"\n",
    "\n",
    "                print(\"Now sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"storm\", value = j)\n",
    "                print(\"Message has finished sending!\", \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: STORM crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETtoday爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ettoday_crawler2kafka():\n",
    "    try:\n",
    "        ###ETtoday爬蟲開始###\n",
    "        for page in range(1, 1007):\n",
    "            url = \"https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1\" + \"&idx=1&page=\" + str(\n",
    "                page)\n",
    "            print(\"第\", page, \"頁\",\" \",\"Processing: \", url)\n",
    "            try:\n",
    "                response = urlopen(url)\n",
    "            except HTTPError:\n",
    "                print(\"大概是結束了\")\n",
    "                break\n",
    "            html = BeautifulSoup(response)\n",
    "\n",
    "            all_ar = html.find(\"div\", class_=\"result_archive\")\n",
    "\n",
    "            val = all_ar.find_all(\"div\", class_=\"archive clearfix\")\n",
    "\n",
    "            for u in val:\n",
    "\n",
    "                print(\"新聞來源:\", \"ETtoday\")\n",
    "\n",
    "                box2 = u.find(\"div\", class_=\"box_2\")\n",
    "                news_url = box2.find(\"a\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "\n",
    "\n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "\n",
    "                a_response = urlopen(news_url)\n",
    "                a_html = BeautifulSoup(a_response, \"html.parser\")\n",
    "\n",
    "                title = a_html.find(\"h1\", class_=\"title\").text\n",
    "                title = re.sub(\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = a_html.find(\"time\", class_=\"date\").text.strip()\n",
    "                date = re.sub(\"年\", \"/\", date)\n",
    "                date = re.sub(\"月\", \"/\", date)\n",
    "                date = re.sub(\"日\", \"\", date)\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "\n",
    "                a_story = a_html.find(\"div\", class_=\"story\")                \n",
    "                content_p = a_story.find_all(\"p\")\n",
    "                author_c = \"\"\n",
    "                for c_p in content_p:\n",
    "                    #如果c_p裡面的第一個格子是None\n",
    "                    if c_p.contents[0].name == None:\n",
    "                        author_c = author_c + c_p.text\n",
    "                    elif c_p.contents[0].name == \"span\":\n",
    "                        author_c = author_c + c_p.text\n",
    "                author = []\n",
    "                if re.search(r\".*／.?.?報導\", author_c) == None:\n",
    "                    author = []\n",
    "                else:\n",
    "                    #因為作者與內文會放在同一個格子，所以將內文中會抓到的作者部分在內文處理時刪除\n",
    "                    content_del = re.search(r\".*／.?.?報導\", author_c).group()\n",
    "                    author_text = re.search(r\".*／.?.?報導\", author_c).group() \n",
    "                author.append(author_text)\n",
    "                print(\"作者:\", author)              \n",
    "\n",
    "                content = \"\"\n",
    "                for c in content_p:\n",
    "                    if c.contents[0].name == None:\n",
    "                        content = content + c.text\n",
    "                        content = content.replace(\"\\u3000\", \" \")      \n",
    "                        content = content.replace(content_del, \"\")            \n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                keyword = []\n",
    "                a_web = a_html.find(\"article\")\n",
    "                a_kw = a_web.find(\"p\", class_=\"tag\")\n",
    "                kw = a_kw.find_all(\"a\", target=\"_blank\")\n",
    "                for k in kw:\n",
    "                    kw_n = k.text\n",
    "                    keyword.append(kw_n)\n",
    "                if keyword == [\"\"]:\n",
    "                    keyword = []\n",
    "                print(\"關鍵字:\", keyword)\n",
    "\n",
    "                if not a_story.find(\"img\")['src'] == None:\n",
    "                    img_url = a_story.find(\"img\")['src']\n",
    "                    img_url = \"https:\" + img_url\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "                j = {\"source\": \"ETtoday\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                     \"kw\": keyword, \"img_url\": img_url}\n",
    "                time.sleep(1)\n",
    "\n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash: ETtoday crawler success!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        ###ETtoday爬蟲結束###\n",
    "\n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"politics_news\"\n",
    "\n",
    "                print(\"Now sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"ettoday\", value = j)\n",
    "                print(\"Message has finished sending!\", \"\\n\")\n",
    "    except Exception as e:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.error('python-crawler-logstash: ETtoday crawler error message!! [' + news_url + ']')\n",
    "        crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有爬蟲執行方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_crawler():\n",
    "    tvbs_crawler2kafka()\n",
    "    setn_crawler2kafka()\n",
    "    appledaily_crawler2kafka()\n",
    "    nownews_crawler2kafka()\n",
    "    storm_crawler2kafka()\n",
    "    ettoday_crawler2kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install schedule\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "schedule.every(10).minutes.do(all_crawler)\n",
    "#def job():\n",
    "#    print(\"I'm working...\")\n",
    "#schedule.every(1).minutes.do(job)\n",
    "#schedule.every().day.at(\"16:42\").do(tvbs_crawler2kafka)\n",
    "#schedule.every().day.at(\"17:51\").do(setn_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(appledaily_crawler2kafka) \n",
    "#schedule.every().day.at(\"22:27\").do(nownews_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(storm_crawler2kafka())\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import threading, queue, time, urllib\n",
    "from  urllib import request\n",
    "baseUrl = 'http://www.pythontab.com/html/pythonjichu/'\n",
    "urlQueue = queue.Queue()\n",
    "for i in range(2, 10):\n",
    "    url = baseUrl + str(i) + '.html'\n",
    "    urlQueue.put(url)\n",
    "    #print(url)\n",
    "def fetchUrl(urlQueue):\n",
    "    while True:\n",
    "        try:\n",
    "            #不阻塞的读取队列数据\n",
    "            url = urlQueue.get_nowait()\n",
    "            i = urlQueue.qsize()\n",
    "        except Exception as e:\n",
    "            break\n",
    "        print ('Current Thread Name %s, Url: %s ' % (threading.currentThread().name, url))\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            responseCode = response.getcode()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if responseCode == 200:\n",
    "            #抓取内容的数据处理可以放到这里\n",
    "            #为了突出效果， 设置延时\n",
    "            time.sleep(1)\n",
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    threads = []\n",
    "    # 可以调节线程数， 进而控制抓取速度\n",
    "    threadNum = 2\n",
    "    for i in range(0, threadNum):\n",
    "        t = threading.Thread(target=fetchUrl, args=(urlQueue,))\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        #多线程多join的情况下，依次执行各线程的join方法, 这样可以确保主线程最后退出， 且各个线程间没有阻塞\n",
    "        t.join()\n",
    "    endTime = time.time()\n",
    "    print ('Done, Time cost: %s ' %  (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
