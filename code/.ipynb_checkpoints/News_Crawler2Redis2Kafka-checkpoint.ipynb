{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬蟲 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.6/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from bs4) (4.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib import request\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/conda/lib/python3.6/site-packages (3.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install redis\n",
    "import redis\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHash(object):\n",
    "    def __init__(self, cap, seed):\n",
    "        self.cap = cap\n",
    "        self.seed = seed\n",
    "\n",
    "    def hash(self, value):\n",
    "        ret = 0\n",
    "        for i in range(len(value)):\n",
    "            ret += self.seed * ret + ord(value[i])\n",
    "        return (self.cap - 1) & ret\n",
    "\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, host='redis', port=6379, db=0, blockNum=1, key='bloomfilter'):\n",
    "        \"\"\"\n",
    "        :param host: the host of Redis\n",
    "        :param port: the port of Redis\n",
    "        :param db: witch db in Redis\n",
    "        :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it.\n",
    "        :param key: the key's name in Redis\n",
    "        \"\"\"\n",
    "        self.server = redis.Redis(host=host, port=port, db=db)\n",
    "        self.bit_size = 1 << 31  # Redis的String类型最大容量为512M，现使用256M\n",
    "        self.seeds = [5, 7, 11, 13, 31, 37, 61]\n",
    "        self.key = key\n",
    "        self.blockNum = blockNum\n",
    "        self.hashfunc = []\n",
    "        for seed in self.seeds:\n",
    "            self.hashfunc.append(SimpleHash(self.bit_size, seed))\n",
    "\n",
    "    def isContains(self, str_input):\n",
    "        if not str_input:\n",
    "            return False\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        ret = True\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            ret = ret & self.server.getbit(name, loc)\n",
    "        return ret\n",
    "\n",
    "    def insert(self, str_input):\n",
    "        m5 = md5()\n",
    "        m5.update(str_input)\n",
    "        str_input = m5.hexdigest()\n",
    "        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)\n",
    "        for f in self.hashfunc:\n",
    "            loc = f.hash(str_input)\n",
    "            self.server.setbit(name, loc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BloomFilter方法:處理資料去重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url is not exists!\n"
     ]
    }
   ],
   "source": [
    "bf = BloomFilter()\n",
    "# url = \"https://tw.news.appledaily.com/politics/realtime/20181125/1473059/\"\n",
    "# url = url.encode('utf-8')\n",
    "# if bf.isContains(url):   # 判断字符串是否存在\n",
    "#     print('url is exists!')\n",
    "# else:\n",
    "#     print('url is not exists!')\n",
    "#     bf.insert(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloomfilter'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BloomFilter的key\n",
    "bf.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = redis.ConnectionPool(host='redis', port=6379, db=0)\n",
    "r = redis.StrictRedis(connection_pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'bloomfilter0']\n"
     ]
    }
   ],
   "source": [
    "#搜尋redis裡面有哪些keys\n",
    "keys = r.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#刪除redis裡key為XXX\n",
    "r.delete(\"bloomfilter0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "傳送log 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logstash\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"logstash\"\n",
    "crawler_logger = logging.getLogger('crawler_logger')\n",
    "crawler_logger.setLevel(logging.INFO)\n",
    "\n",
    "# TCP\n",
    "crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Producer 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka in /opt/conda/lib/python3.6/site-packages (1.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka\n",
    "from kafka import KafkaProducer\n",
    "import sys\n",
    "from kafka.errors import KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "連線Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    # Kafka集群在那裡?\n",
    "    bootstrap_servers = [\"kafka1:29092\"],\n",
    "    # 指定msgKey的序列化器, 若Key為None, 無法序列化, 透過producer直接給值\n",
    "    key_serializer = str.encode,\n",
    "    # 指定msgValue的序列化器\n",
    "    value_serializer = lambda m: json.dumps(m).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVBS 爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvbs_crawler2kafka():\n",
    "    ###TVBS爬蟲開始###\n",
    "    url = \"https://news.tvbs.com.tw/politics/\"\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    news_ul = soup.find_all(\"ul\", id = \"block_pc\")[0]\n",
    "    news_a = news_ul.find_all(\"a\")\n",
    "\n",
    "    news_no_string_list = []\n",
    "    for a in news_a:\n",
    "        news_no_string = a[\"data-news-id\"]\n",
    "        news_no_string_list.append(news_no_string)\n",
    "\n",
    "    news_no_list = []\n",
    "    for n in news_no_string_list:\n",
    "        news_no_int = n.strip('\\'')\n",
    "        news_no_list.append(news_no_int)\n",
    "\n",
    "    #json_data = []\n",
    "    for news_no in news_no_list:\n",
    "\n",
    "        print(\"新聞來源:\", \"TVBS新聞台\")\n",
    "\n",
    "        news_url = \"https://news.tvbs.com.tw/politics/\" + news_no\n",
    "        print(\"新聞網址:\", news_url)\n",
    "\n",
    "        \n",
    "        #判斷網址是否爬過\n",
    "        bf = BloomFilter()\n",
    "        # Unicode-objects must be encoded before hashing\n",
    "        news_url_u = news_url.encode('utf-8')\n",
    "        if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "            print('url is exists!')\n",
    "            continue\n",
    "        else:\n",
    "            print('url is not exists!')\n",
    "            bf.insert(news_url_u)\n",
    "        \n",
    "\n",
    "        req = requests.get(news_url)\n",
    "        # requests如果找不到指定編碼，會猜測網頁編碼，有時會形成亂碼，故給指定編碼utf-8\n",
    "        req.encoding = (\"utf-8\")\n",
    "        soup = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"h1\", class_ = \"margin_b20\").text\n",
    "        title = re.sub(r\"\\u3000\", \" \", title)\n",
    "        title = re.sub(r\"\\xa0\", \" \", title)\n",
    "        print(\"新聞標題:\", title)\n",
    "\n",
    "        date = soup.find(\"div\", class_ = \"icon_time time leftBox2\").text\n",
    "        print(\"發佈時間:\", date)\n",
    "\n",
    "        author = []\n",
    "        author_text = soup.find(\"h4\", class_ = \"font_color5 leftBox1\").text\n",
    "        author_text = re.sub(r\"攝影.*報導\", \"\", author_text)\n",
    "        author.append(author_text)\n",
    "        print(\"作者:\", author)\n",
    "\n",
    "        content = soup.find(\"div\", class_ = \"h7 margin_b20\").text\n",
    "        content = content.replace(\"\\t\", \"\").replace(\"   \",\"\").replace(\"\\n\", \"\").replace(\"（中央社）\",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，\",\"\").replace(\"快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"TVBS最新大數據分析\",\"\").replace(\"看完整內容快點我加入TVBS新聞LINE好友！\",\"\")\n",
    "        print(\"內文:\", content)\n",
    "\n",
    "        keyword = soup.find(\"div\", class_ = \"adWords\").text\n",
    "        keyword = keyword.replace(\"\\t\", \"\")\n",
    "        keyword = keyword.replace(\",\",\"\")\n",
    "        keyword = keyword.replace(\"\\n\", \"\").replace(\"編輯  \",\"\").replace(\" 報導\",\"\")\n",
    "        keyword = keyword.replace(\"記者\",\"\").replace(\"      \",\",\")\n",
    "        keyword = keyword.split(\",\")\n",
    "        print(\"關鍵字\", keyword)\n",
    "\n",
    "        img = soup.find(\"div\", class_ = \"margin_b20\")\n",
    "        img_url = img.find('img')['src']\n",
    "        print(\"圖片網址:\", img_url)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        j = {\"source\": \"TVBS新聞台\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "             \"kw\": keyword, \"img_url\": img_url}\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # TCP連線到logstash\n",
    "        crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "        # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "        crawler_logger.info('python-crawler-logstash:TVBS crawler success!!')\n",
    "        crawler_logger.handlers.clear()\n",
    "        #json_data.append(j)\n",
    "    ###TVBS爬蟲結束###\n",
    "    \n",
    "\n",
    "   \n",
    "        # 步驟2.指定想要發佈訊息的topic名稱\n",
    "        topic_name = \"test\"\n",
    "\n",
    "        try:\n",
    "            print(\"Start sending messages ...\")\n",
    "            # 步驟3.產生要發佈到Kafka的訊息\n",
    "            # - 參數  # 1: topicName\n",
    "            # - 參數  # 2: msgKey\n",
    "            # - 參數  # 3: msgValue\n",
    "\n",
    "            producer.send(topic = topic_name, key = \"tvbs\", value = j)\n",
    "            print(\"Message sending completed!\")\n",
    "        except Exception as e:\n",
    "            # 錯誤處理\n",
    "            e_type, e_value, e_traceback = sys.exc_info()\n",
    "            print(\"type ==> %s\" % (e_type))\n",
    "            print(\"value ==> %s\" % (e_value))\n",
    "            print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "            print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "            print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "            # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.error('python-crawler-logstash :TVBS crawler error message!!')\n",
    "            crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETN爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setn_crawler2kafka():\n",
    "    ###SETN爬蟲開始###\n",
    "    page = 1\n",
    "    #json_data = []\n",
    "    while True:\n",
    "        page_url = \"https://www.setn.com/ViewAll.aspx?PageGroupID=6&p=\" + str (page)\n",
    "        print (\"Processing:\", page_url)\n",
    "\n",
    "        response = urlopen (page_url)\n",
    "        bs = BeautifulSoup (response)\n",
    "        titles = bs.find_all (\"h3\", \"view-li-title\")\n",
    "\n",
    "        if len (titles) == 0:\n",
    "            break\n",
    "\n",
    "        for t in titles:\n",
    "\n",
    "            print(\"新聞來源:\", \"SETN三立新聞網\")\n",
    "\n",
    "            news_url = \"https://www.setn.com\" + t.find (\"a\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            req = requests.get (news_url)\n",
    "            bs = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "            title = bs.find('h1', class_ = \"news-title-3\").text\n",
    "            title = re.sub(r\"\\u3000\", \" \", title)\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = bs.find(\"time\", class_ = \"page-date\").text.strip()\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "            author = []\n",
    "            a_divs = bs.find(\"div\", id=\"Content1\")\n",
    "            first_p = a_divs.find(\"p\")\n",
    "            author_text = first_p.text\n",
    "            #找第一段有沒有含\"／\"，如果沒有給予空字串\n",
    "            if author_text.find(\"／\") == -1:\n",
    "                author_text = \"\"\n",
    "            #如果作者是空字串，給予空list\n",
    "            if author_text == \"\":\n",
    "                author = []\n",
    "            else:\n",
    "                author.append(author_text)\n",
    "            print(\"作者:\", author)\n",
    "\n",
    "            c_divs = bs.find('div', itemprop='articleBody')\n",
    "            content_all = c_divs.find_all (\"p\")\n",
    "            content = \"\"\n",
    "            for c in content_all:\n",
    "                if c.attrs == {}:\n",
    "                    content = content + c.text\n",
    "                    #print(c.text)\n",
    "\n",
    "                    c_divs = bs.find('div', itemprop='articleBody')\n",
    "            content_all = c_divs.find_all (\"p\")\n",
    "            content = \"\"\n",
    "            for c in content_all:\n",
    "                if c.attrs == {}:\n",
    "                    content = content + c.text\n",
    "                    #print(c.text)\n",
    "            print(\"內文:\", content)\n",
    "\n",
    "            keyword = []\n",
    "            k_divs = bs.find(\"div\", class_=\"keyword page-keyword-area\")\n",
    "            k_strong = k_divs.find_all(\"strong\")\n",
    "            for k in k_strong:\n",
    "                keyword_text = k.text\n",
    "                keyword.append(keyword_text)\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "            img_p = bs.find(\"p\", style = \"text-align: center;\")\n",
    "            img_url = img_p.find(\"img\")[\"src\"]\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"SETN三立新聞網\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep (1)\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:SETN crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###SETN爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"setn\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :SETN crawler error message!!')\n",
    "                crawler_logger.handlers.clear()\n",
    "        time.sleep (2)\n",
    "        page = page + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AppleDaily爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appledaily_crawler2kafka():\n",
    "    ###AppleDaily爬蟲開始###\n",
    "    for page in range(1, 12):\n",
    "        time.sleep(2)\n",
    "        href = \"https://tw.news.appledaily.com/politics/realtime/\" + str(page)\n",
    "        res = requests.get(href)\n",
    "        html = BeautifulSoup(res.text)\n",
    "\n",
    "        all_news_1 =  html.find_all(\"ul\", class_= \"rtddd slvl\")\n",
    "        for all_news in all_news_1:\n",
    "            news = all_news.find_all(\"a\")\n",
    "            print(\"第\", page, \"頁\")\n",
    "            for n in news:\n",
    "\n",
    "                print(\"新聞來源:\", \"蘋果日報\")\n",
    "\n",
    "                #my_news = {}\n",
    "                news_url =\"https://tw.news.appledaily.com/\" + str(n[\"href\"])\n",
    "                print(\"新聞網址:\", news_url)\n",
    "                \n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "\n",
    "                news_per = requests.get(news_url)\n",
    "                bs = BeautifulSoup(news_per.text)\n",
    "\n",
    "                title = bs.find(\"h1\").text\n",
    "                title = re.sub(r\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)           \n",
    "\n",
    "                date = bs.find(\"div\", class_=\"ndArticle_creat\").text.replace(\"出版時間：\", \"\")\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                if not bs.find(\"div\", class_=\"ndgKeyword\") == None:\n",
    "                    key_word = bs.find(\"div\", class_=\"ndgKeyword\").find_all(\"a\")\n",
    "                    key_list = []\n",
    "                    for k in key_word:\n",
    "                        key_list.append(k.text)\n",
    "                else:\n",
    "                    key_list = []       \n",
    "\n",
    "                content_dir = bs.find(\"div\", class_=\"ndArticle_margin\")\n",
    "                for a in content_dir.find_all(\"a\"):\n",
    "                    if not a == None:\n",
    "                        a.extract()\n",
    "                for s in content_dir.find_all(\"span\"):\n",
    "                    if not s == None:\n",
    "                        s.extract()\n",
    "                for i in content_dir.find_all(\"iframe\"):\n",
    "                    if not i == None:\n",
    "                        i.extract()\n",
    "                for d in content_dir.find_all(\"div\"):\n",
    "                    if not d == None:\n",
    "                        d.extract()\n",
    "                for st in content_dir.find_all(\"style\"):\n",
    "                    if not st == None:\n",
    "                        st.extract()\n",
    "                content = content_dir.text.strip()\n",
    "                content = content.replace(\"）\", \")\").replace(\"（\", \"(\").replace(\"／\", \"/\").replace(\"╱\", \"/\")       \n",
    "                pat = re.compile(r\"\\(((.{0,15})/.{0,8}[\\u5831][\\u5c0e])\\)\")\n",
    "                pat_2 = re.compile(r\"【((.{0,15})/.{0,8}[\\u5831][\\u5c0e])】\")\n",
    "                ans_news = pat.search(content)\n",
    "                ans_news_2 = pat_2.search(content)\n",
    "                if ans_news == None:\n",
    "                    author = []\n",
    "                elif not ans_news_2 == None:\n",
    "                    author = ans_news[2].split(\"、\")\n",
    "                else:\n",
    "                    #print(ans_news[2])\n",
    "                    author = ans_news[2].split(\"、\")\n",
    "                    content = content.split(ans_news[0])[0]\n",
    "                print(\"作者:\", author)   \n",
    "                print(\"內文:\", content)  \n",
    "                print(\"關鍵字:\", key_list)\n",
    "\n",
    "                img_box = bs.find(\"div\", class_=\"ndAritcle_headPic\")\n",
    "                if not img_box == None:\n",
    "                    img_url = img_box.find(\"img\")['src']\n",
    "                else:\n",
    "                    img_url = \"\"\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "                j = {\"source\": \"蘋果日報\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                     \"content\": content, \"kw\": key_list, \"img_url\": img_url}\n",
    "                time.sleep(random.randint(1, 2))\n",
    "                \n",
    "                # TCP連線到logstash\n",
    "                crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "                # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.info('python-crawler-logstash:AppleDaily crawler success!!')\n",
    "                crawler_logger.handlers.clear()\n",
    "                #json_data.append(j)\n",
    "    ###AppleDaily爬蟲結束###\n",
    "\n",
    "                # 步驟2.指定想要發佈訊息的topic名稱\n",
    "                topic_name = \"test\"\n",
    "\n",
    "                try:\n",
    "                    print(\"Start sending messages ...\")\n",
    "                    # 步驟3.產生要發佈到Kafka的訊息\n",
    "                    # - 參數  # 1: topicName\n",
    "                    # - 參數  # 2: msgKey\n",
    "                    # - 參數  # 3: msgValue\n",
    "\n",
    "                    producer.send(topic = topic_name, key = \"appledaily\", value = j)\n",
    "                    print(\"Message sending completed!\")\n",
    "                except Exception as e:\n",
    "                    # 錯誤處理\n",
    "                    e_type, e_value, e_traceback = sys.exc_info()\n",
    "                    print(\"type ==> %s\" % (e_type))\n",
    "                    print(\"value ==> %s\" % (e_value))\n",
    "                    print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                    print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                    print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                    # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                    crawler_logger.error('python-crawler-logstash :AppleDaily crawler error message!!')\n",
    "                    crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOWnews爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nownews_crawler2kafka():\n",
    "    ###NOWnews爬蟲開始###\n",
    "    for page in range(1,6):\n",
    "        href = 'https://www.nownews.com/cat/politics/page/'+str(page)\n",
    "\n",
    "        response = requests.get(href)\n",
    "        soup=BeautifulSoup(response.text)\n",
    "        all_news = soup.find_all(\"div\",class_=\"td_block_inner tdb-block-inner td-fix-index\")\n",
    "\n",
    "        news = all_news[1].find_all(\"h3\",class_=\"entry-title\")\n",
    "        for n in news:\n",
    "            try:\n",
    "                print(\"新聞來源:\", \"NOWnews\")\n",
    "\n",
    "                news_url = n.find(\"a\")[\"href\"]\n",
    "                print(\"新聞網址:\", news_url)\n",
    "                \n",
    "                \n",
    "                #判斷網址是否爬過\n",
    "                bf = BloomFilter()\n",
    "                # Unicode-objects must be encoded before hashing\n",
    "                news_url_u = news_url.encode('utf-8')\n",
    "                if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                    print('url is exists!')\n",
    "                    continue\n",
    "                else:\n",
    "                    print('url is not exists!')\n",
    "                    bf.insert(news_url_u)\n",
    "                    \n",
    "\n",
    "                response = requests.get(news_url)\n",
    "                html = BeautifulSoup(response.text)\n",
    "                title = html.find(\"h1\",class_=\"entry-title\").text\n",
    "                title = re.sub(\"\\u3000\", \" \", title)\n",
    "                print(\"新聞標題:\", title)\n",
    "\n",
    "                date = html.find(\"time\",class_ = \"entry-date\").text\n",
    "                print(\"發佈時間:\", date)\n",
    "\n",
    "                author = []\n",
    "                author_text = html.find(\"div\",class_=\"td-post-author-name\").text\n",
    "                author_text = author_text.split(\"／\")[0]\n",
    "                author_text = author_text.split(\"/\")[0].replace(\"記者\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                author.append(author_text)\n",
    "                print(\"作者:\", author)\n",
    "\n",
    "                te = html.find(\"div\", class_=\"td-post-content\").contents\n",
    "                #print(te)\n",
    "                #print(te[0].name == None)\n",
    "                content = \"\"\n",
    "                for a in te:\n",
    "                    if a.name == \"p\":\n",
    "                        content = content + a.text\n",
    "                print(\"內文:\", content)\n",
    "\n",
    "                #處理keyword\n",
    "                if not html.find(\"ul\",class_=\"td-tags\") == None:\n",
    "                    kw = html.find(\"ul\",class_=\"td-tags\").find_all(\"a\")\n",
    "                    keyword = []\n",
    "                    for k in kw:\n",
    "                        keyword.append(k.text)\n",
    "                else:\n",
    "                    keyword = []\n",
    "                print(\"關鍵字:\", keyword)\n",
    "\n",
    "                img_url = html.find(\"div\",class_=\"td-post-featured-image\")\n",
    "                img_url = img_url.find(\"img\")[\"src\"]\n",
    "                print(\"圖片網址:\", img_url)\n",
    "\n",
    "            except:\n",
    "                print(\"錯誤:\", title, news_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"NOWnews\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(random.randint(0,2))\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:NOWnews crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###NOWnews爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"nownews\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :NOWnews crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORM爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storm_crawler2kafka():\n",
    "    ###STORM爬蟲開始###\n",
    "    json_data = []    \n",
    "    for page in range(1,5):\n",
    "        href = 'https://www.storm.mg/category/118/'+str(page)\n",
    "\n",
    "        response = urlopen(href)\n",
    "        html = BeautifulSoup(response)\n",
    "        #找到特定範圍在選取網址列\n",
    "        newf = html.find(\"div\",class_ = \"middle_category_cards\")\n",
    "        #選取所有單篇網址列\n",
    "        news = newf.find_all(\"div\",class_ = \"category_card\")\n",
    "        #迴圈抓取整頁新聞連結\n",
    "        for r in news:\n",
    "\n",
    "            print(\"新聞來源:\", \"風傳媒 THE STORM MEDIA\")\n",
    "\n",
    "            news_url = r.find(\"a\",class_ = \"card_link\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            response = urlopen(news_url)\n",
    "            html = BeautifulSoup(response)\n",
    "\n",
    "            title = html.find(\"h1\", id = \"article_title\").text\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = html.find(\"span\", class_=\"info_time\").text\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "            author = []\n",
    "            author_text = html.find(\"span\", class_=\"info_author\").text.replace(\"新新聞\",\"\")\n",
    "            author.append(author_text)\n",
    "            print(\"作者:\", author)\n",
    "\n",
    "            content = html.find(\"div\", id=\"CMS_wrapper\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "            print(\"內文:\", content)\n",
    "\n",
    "            keyword = html.find(\"div\", id=\"tags_list_wrapepr\").text.split()\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "            img_url = html.find(\"img\", id=\"feature_img\")[\"src\"]\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            time.sleep (1)\n",
    "\n",
    "            j = {\"source\": \"風傳媒 THE STORM MEDIA\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \n",
    "                 \"content\": content, \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(random.randint(0,2))\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:STORM crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###STORM爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"storm\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :STORM crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETtoday爬蟲方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ettoday_crawler2kafka():\n",
    "    ###ETtoday爬蟲開始###\n",
    "    #json_data = []\n",
    "    for page in range(1, 1007):\n",
    "        url = \"https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1\" + \"&idx=1&page=\" + str(\n",
    "            page)\n",
    "        print(\"第\", page, \"頁\",\" \",\"Processing: \", url)\n",
    "        try:\n",
    "            response = urlopen(url)\n",
    "        except HTTPError:\n",
    "            print(\"大概是結束了\")\n",
    "            break\n",
    "        html = BeautifulSoup(response)\n",
    "\n",
    "        all_ar = html.find(\"div\", class_=\"result_archive\")\n",
    "\n",
    "        val = all_ar.find_all(\"div\", class_=\"archive clearfix\")\n",
    "\n",
    "        for u in val:\n",
    "\n",
    "            print(\"新聞來源:\", \"ETtoday\")\n",
    "\n",
    "            box2 = u.find(\"div\", class_=\"box_2\")\n",
    "            news_url = box2.find(\"a\")[\"href\"]\n",
    "            print(\"新聞網址:\", news_url)\n",
    "            \n",
    "            \n",
    "            #判斷網址是否爬過\n",
    "            bf = BloomFilter()\n",
    "            # Unicode-objects must be encoded before hashing\n",
    "            news_url_u = news_url.encode('utf-8')\n",
    "            if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "                print('url is exists!')\n",
    "                continue\n",
    "            else:\n",
    "                print('url is not exists!')\n",
    "                bf.insert(news_url_u)\n",
    "                \n",
    "\n",
    "            a_response = urlopen(news_url)\n",
    "            a_html = BeautifulSoup(a_response, \"html.parser\")\n",
    "\n",
    "            title = a_html.find(\"h1\", class_=\"title\").text\n",
    "            title = re.sub(\"\\u3000\", \" \", title)\n",
    "            print(\"新聞標題:\", title)\n",
    "\n",
    "            date = a_html.find(\"time\", class_=\"date\").text.strip()\n",
    "            date = re.sub(\"年\", \"/\", date)\n",
    "            date = re.sub(\"月\", \"/\", date)\n",
    "            date = re.sub(\"日\", \"\", date)\n",
    "            print(\"發佈時間:\", date)\n",
    "\n",
    "\n",
    "            a_story = a_html.find(\"div\", class_=\"story\")                \n",
    "            content_p = a_story.find_all(\"p\")\n",
    "            author_c = \"\"\n",
    "            for c_p in content_p:\n",
    "                #如果c_p裡面的第一個格子是None\n",
    "                if c_p.contents[0].name == None:\n",
    "                    author_c = author_c + c_p.text\n",
    "                elif c_p.contents[0].name == \"span\":\n",
    "                    author_c = author_c + c_p.text\n",
    "            author = []\n",
    "            if re.search(r\".*／.?.?報導\", author_c) == None:\n",
    "                author = []\n",
    "            else:\n",
    "                #因為作者與內文會放在同一個格子，所以將內文中會抓到的作者部分在內文處理時刪除\n",
    "                content_del = re.search(r\".*／.?.?報導\", author_c).group()\n",
    "                author_text = re.search(r\".*／.?.?報導\", author_c).group() \n",
    "            author.append(author_text)\n",
    "            print(\"作者:\", author)              \n",
    "\n",
    "            content = \"\"\n",
    "            for c in content_p:\n",
    "                if c.contents[0].name == None:\n",
    "                    content = content + c.text\n",
    "                    content = content.replace(\"\\u3000\", \" \")      \n",
    "                    content = content.replace(content_del, \"\")            \n",
    "            print(\"內文:\", content)\n",
    "\n",
    "\n",
    "            keyword = []\n",
    "            a_web = a_html.find(\"article\")\n",
    "            a_kw = a_web.find(\"p\", class_=\"tag\")\n",
    "            kw = a_kw.find_all(\"a\", target=\"_blank\")\n",
    "            for k in kw:\n",
    "                kw_n = k.text\n",
    "                keyword.append(kw_n)\n",
    "            if keyword == [\"\"]:\n",
    "                keyword = []\n",
    "            print(\"關鍵字:\", keyword)\n",
    "\n",
    "\n",
    "            if not a_story.find(\"img\")['src'] == None:\n",
    "                img_url = a_story.find(\"img\")['src']\n",
    "                img_url = \"https:\" + img_url\n",
    "            else:\n",
    "                img_url = None\n",
    "            print(\"圖片網址:\", img_url)\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "            j = {\"source\": \"ETtoday\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "                 \"kw\": keyword, \"img_url\": img_url}\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # TCP連線到logstash\n",
    "            crawler_logger.addHandler(logstash.TCPLogstashHandler(host, 5000, version=1))\n",
    "            # 爬蟲成功傳送至Elasticsearch的log訊息\n",
    "            crawler_logger.info('python-crawler-logstash:ETtoday crawler success!!')\n",
    "            crawler_logger.handlers.clear()\n",
    "            #json_data.append(j)\n",
    "    ###ETtoday爬蟲結束###\n",
    "    \n",
    "            # 步驟2.指定想要發佈訊息的topic名稱\n",
    "            topic_name = \"test\"\n",
    "\n",
    "            try:\n",
    "                print(\"Start sending messages ...\")\n",
    "                # 步驟3.產生要發佈到Kafka的訊息\n",
    "                # - 參數  # 1: topicName\n",
    "                # - 參數  # 2: msgKey\n",
    "                # - 參數  # 3: msgValue\n",
    "\n",
    "                producer.send(topic = topic_name, key = \"ettoday\", value = j)\n",
    "                print(\"Message sending completed!\")\n",
    "            except Exception as e:\n",
    "                # 錯誤處理\n",
    "                e_type, e_value, e_traceback = sys.exc_info()\n",
    "                print(\"type ==> %s\" % (e_type))\n",
    "                print(\"value ==> %s\" % (e_value))\n",
    "                print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "                print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "                print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "                # 爬蟲失敗傳送至Elasticsearch的log訊息\n",
    "                crawler_logger.error('python-crawler-logstash :ETtoday crawler error message!!')\n",
    "                crawler_logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 頁\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487411/\n",
      "新聞標題: 韓國瑜賽馬場創意惹議 觀光局：希望明年1月赴港考察\n",
      "發佈時間: 2018/12/21 21:17\n",
      "作者: ['周昭平']\n",
      "內文: (新增：準觀光局長潘恆旭說明)準高雄市長韓國瑜昨與中華民國工商建研會座談時，提到未來想把已停工的中油五輕廠，引進賽馬、發展相關產業鏈。此驚人發想立即引發爭議。準高雄市府新聞局長王淺秋今上午出面說明強調，昨天韓國瑜提賽馬只是創意發想，場地也不一定會在中油廠區，需進一步透過專業評估與合乎中央法規，才可能成為確定執行計畫。王淺秋說，昨韓國瑜會在會中提及賽馬，主要是因高雄目前財務現實狀況，要想辦法創造稅收、在價值供應鏈上創造最多效益，賽馬場只是評估考量之一，不過因實際上世界多國都有賽馬場，創造經濟供應鏈驚人，且有機會做公益，所以才會成為考量發展產業之一，雖說目前為止都只是創意發想階段，但觀光局已表示，希望明年一月可以安排到香港考察，同時針對法規問題了解，若評估可行就會來推動。此外針對韓國瑜昨在會中提到有台灣人曾任香港馬會會長，遭網友質疑經查香港馬會根本沒有台灣人，酸韓國瑜遭詐騙。對此，準高市府觀光局局長潘恆旭說，韓所提台灣人會長應是前香港馬會行政總裁黃至剛，他曾到台灣成大念書，後獲美國博士學位後回到台灣工作多年，1996年獲聘香港馬會行政總裁，2007年退休。但黃至剛是否為台灣人，這部分他並不清楚。潘恆旭說，黃至剛與韓國瑜之前有透過相熟的朋友聊到賽馬的經驗，但至於是否實際碰面他並不知情，此外黃至剛的職稱應是行政總監，韓國瑜講會長是口誤。潘恆旭說，因團隊還未就任，韓國瑜對此僅在構想階段，曾提過，但內部也還未有縝密的沙盤推演與輿情的配套，拋出想法是想要振興高雄經濟，畢竟賽馬可創造兩萬工作機會與很高產值，強調其原始概念是如此，不是說非推賽馬不可。至於明年一月去香港實地考察是否成行，會等到就任後，團隊評估覺得可試試看，確實可當成一個可評估的案子，就會成行。另外界質疑，中油五輕廠區多為汙染整治控制場址，該處若興建馬場時光是整治就要花17年。王淺秋回應，賽馬場並「不一定要在高雄煉油廠，很多其他場地可以考量！」強調引進不只是為賽馬這件事，而是運動型態供應鏈可以帶起的產業發展與經濟效益。《蘋果》追問昨行政院會已宣示中油五輕要推動轉型為「循環經濟園區」，韓國瑜昨提廠區要蓋賽馬場是否與中央政策牴觸？王淺秋回應，「是否場地在那裏(中油煉油廠)，其實都還沒有具體評估考量過。」她反問媒體，「您(昨天)在現場嗎？您在會場上嗎？」認為報導所指韓看中中油五輕蓋賽馬場說法，是經過與會者轉述，並非完全是韓國瑜所講的話。王淺秋最後強調，引進賽馬不是一個已經具體確定要執行的政策，須等就職後相關首長會商跟專業評估，同時考量相關中央地方法規，確實可行才會成為確定要執行的計畫。\n",
      "關鍵字: ['韓國瑜', '中油五輕', '賽馬', '王淺秋']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487802/\n",
      "新聞標題: ​金門檢調選後持續查賄 2當選人被提當選無效之訴\n",
      "發佈時間: 2018/12/21 21:11\n",
      "作者: ['蔡宇凌']\n",
      "內文: 九合一選後金門地檢署持續查賄，金城鎮代當選人張含麗、金寧鄉代當選人莊振源2人，涉嫌透過樁腳以每票3000元買票，今日向福建金門地方法院對2人提起當選無效之訴。 檢察官認金城鎮鎮民代表當選人張含麗及金寧鄉鄉民代表當選人莊振源，均有透過樁腳，涉有公職人員選舉罷免法第99條第1項之投票行賄罪嫌，本於公益代表人之身分擔任原告，向法院對2人一併提起當選無效之訴。 縣選委會已在11月30日公布當選人名單，而依據選罷法的規定，若當選人有涉賄情事者，選委會、檢察官或同一選舉區的候選人得以當選人為被告，自公告當選之日起30日內，向法院提起當選無效之訴。 金門地檢署檢察長毛有增表示，選舉雖然已經結束，但相關選舉不法案件並不因此而終結，選舉不法及虛設戶籍妨害投票案件仍由檢察官持續偵辦中，司法單位一定會堅守工作崗位，持續捍衛選舉之純正性與公平性。\n",
      "關鍵字: ['金門', '查賄', '當選人', '當選無效之訴']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_7e4d00a08a608b5395d89ea36dc6c7d3.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487792/\n",
      "新聞標題: ​賴揆視察金門大橋工程進度 指示如質如時完工\n",
      "發佈時間: 2018/12/21 21:05\n",
      "作者: ['蔡宇凌']\n",
      "內文: 金門大橋工程預定進度32.18%，目前進度為31.89%，行政院長賴清德今天到金門大橋工地視察施工情形，指示工作團隊如質如時完工，盼在110年能給小金門鄉親，一條更安全的道路。 賴清德指出金門大橋是台灣極有難度的公共工程，其橋長5.4公里，有4.77公里在海上，最深到23公尺，且氣象、海象條件困難，還有要打到23公尺深的地盤是花崗石岩，是非常艱鉅工程，這類工程不僅在台灣少見，在國際上也少見。做為曾在金門當兵的人，有機會能參與工程，感到很高興及光榮。 賴清德強調要將這條金門大橋建造起來，除了給小金門鄉親一條更安全的道路，也能讓大、小金門串聯，使整體的經濟效益得到更大發揮，不管是醫療、水電或各種功能，都能一併完成。 賴清德表示，臺灣再生能源剛起步，特別是風力發電，行政院期待風力發電產業的發展不僅滿足臺灣電力需求，也希望風力發電產業組成「國家隊」推動至國外，金門大橋也是風力發電產業未來發展的試金石，意義重大。\n",
      "關鍵字: ['賴清德', '金門大橋', '如期完工']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_0248dfc90c18b6012ed74a2a647c4699.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487633/\n",
      "新聞標題: 集郵韓粉最愛就職禮 個人化郵票竟唱起「夜襲」\n",
      "發佈時間: 2018/12/21 20:23\n",
      "作者: ['周昭平']\n",
      "內文: (更新：新增影片)「韓流」也吹進國營事業單位！高雄郵局選前看好國民黨高雄市長候選人長韓國瑜吹起韓流效應，勝選後立即連繫韓國瑜競辦取得他本人Q版商標授權，趕製個人化郵票套組，將在下周12月25日就職當天在典禮會場設置臨時郵局販售，今郵票套組首度亮相，郵摺設計除有Q版韓國瑜圖像，還將他經典競選口號印製在會旋轉的摩天輪車廂上，連造勢場上合唱的「夜襲」，都可在郵摺上按壓聽取，設計極具巧思。發行韓國瑜個人化郵票套組的高雄郵局，今由局長邱鴻恩與企劃行銷科科長許健雄帶著成品，到國民黨高雄市黨部，與準新聞局長王淺秋共同發表郵票套組設計。高雄郵局表示，之前高雄市長陳菊就職時也曾發行個人化郵票，當時發行兩萬套，當時是市府統一購買當作贈品送給貴賓，而韓國瑜的郵票套組則是看好韓流效應，設計風格活潑，一套要賣350元，但受限時間趕印不及，只印製1萬1千套，若時間允許，印量至少3萬起跳。仔細看這套韓國瑜就職紀念郵票，郵摺封面用了韓國瑜選前唱過的「愛你1萬年」，還寫上「一個城市有了夢想，才會偉大」，裡面有高雄愛河和85大樓等景點，一戰成名的愛情摩天輪的車廂上則有他「人進的來，東西賣得出去、高雄發大財」等施政理念口號及造勢晚會的香腸瀑布、韓冰及韓粉如貪吃蛇騎機車掃街拜票等造勢現場等，藉插畫以詼諧幽默方式表現。高雄郵局表示，因應民眾與集郵人士需求，預計就職典禮當天在會場設臨時郵局並發行這套個人化郵票套組，預計發行1萬1千套，原則上每人每次限購三套，一套350元，其中1千套將作為義賣，所得會依照韓國瑜希望捐給財團法人中華基督教衛理公會，做為守護偏鄉學童與弱勢家庭愛心善款。\n",
      "關鍵字: ['韓國瑜', '就職紀念郵票', '愛情摩天輪', '夜襲']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487745/\n",
      "新聞標題: 中華民國護照享德國自動通關 法蘭克福、慕尼黑兩機場適用\n",
      "發佈時間: 2018/12/21 20:18\n",
      "作者: ['陳培煌']\n",
      "內文: 外交部今天表示，台灣今年夏天起納入德國法蘭克福及慕尼黑機場自動通關試辦計畫，持中華民國有效護照者可享有自動化通關出境的待遇，持中華民國有效護照者在機場指定的查驗機，可享有自動化通關出境的待遇，對德方將台灣納入試辦對象表示歡迎。外交部指出，提升民眾旅外尊嚴與便利，一向是外交部重點工作，日後仍將持續推動包括德國在內等國家重要國際機場給予通關便捷待遇。\n",
      "關鍵字: ['德國', '法蘭克福', '慕尼黑', '自動通關', '中華民國', '護照']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_d5b2a296cb0b22e6054be116848296d1.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487775/\n",
      "新聞標題: 爭取民進黨主席 卓榮泰：扭轉資進黨印象、拚經濟絕非兩難\n",
      "發佈時間: 2018/12/21 20:14\n",
      "作者: ['鄭鴻達']\n",
      "內文: 九合一選舉後，民進黨大敗，總統蔡英文也請辭黨主席，行政院秘書長卓榮泰登記參選黨主席。卓榮泰在臉書張貼「勞資正向循環，創造財富要同步分配」文章表示，促進世代對話是黨主席的責任，接下來的難題，是拚經濟果實如何讓人民有感？他強調，扭轉人民的「資進黨」印象、給人民「拚經濟」的感覺絕非選擇上的兩難，而執政黨能做的，就是促進勞資雙方的正向、有機的循環。卓榮泰說，大家經常在媒體上，看見勞工指責慣老闆壓榨，老闆抱怨員工計較，這皆非正向互動跟循環，當經常接受這樣觀點跟資訊，就很難促進勞資關係，沒有互信、就沒有合作，沒有合作、就很難有感受，而一個執政黨能做的，就是促進勞資雙方的正向、有機的循環。他表示，行政院長賴清德已針對企業投資障礙「五缺」進行處理，因此獲得六大工商團體強烈建議留任的肯定，而打開經濟的活水，行政院已經跨出第一步，這點也反映在數據上，但只有這樣，顯然不夠。卓榮泰強調，政府努力為企業打開投資的大門，企業有競爭機會賺錢，也要同步給予勞工好處。他提醒，黨要扮演提醒與溝通的角色，提醒政府兼顧「創造」與「分配」，讓業主跟勞工不會成為互相搶錢的仇人，而是一起賺錢的夥伴。卓榮泰表示，「好員工是公司的資產」，員工有收穫就會更努力，工作有效率，企業才會有收益，這是下一個時代的企業家，都應該抱持的信念；照顧員工也不只反映在薪資上，也該從休息時間、進修、升遷、職災保障等方面，給予有彈性的規劃，來因應不同產業的需求。\n",
      "關鍵字: ['卓榮泰', '民進黨', '拚經濟', '資進黨']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_4b1816cfa50d9316285df719b5b6ee55.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487700/\n",
      "新聞標題: 打擊假訊息草案已送立法院 立法院下會期才處理\n",
      "發佈時間: 2018/12/21 19:50\n",
      "作者: ['曾盈瑜']\n",
      "內文: 為了打擊假訊息，政府展開相關修法，行政院政務委員羅秉成召集「防制假訊息危害專案小組」，盤點13項修法，上周行政院會已通過修正《災害防救法》、《糧食管理法》、《農產品市場交易法》、《傳染病防治法》、《食品安全衛生管理法》、《核子事故緊急應變法》、《廣播電視法》等7項，主要是對散播相關假訊息加重其刑期或罰鍰，送立法院審查。 此外，專案小組也討論到《社會秩序維護法》修正草案，規定若散布假訊息不實之事使公眾產生畏懼或恐慌者，可處3萬元以上、30萬元以下罰鍰，但後來因考量言論自由，未送行政院會通過，還需進一步院際討論協調。 錯假訊息透過網路傳播快速，因此日前政院版也提出「數位通訊傳播法」草案，該案5月已在立法院交通委員會初審通過，「數位通訊傳播法」草案規定，服務提供者，像是臉書、Google、Yahoo等，在知悉行為或資訊違法後，要立即移除或使他人無法接取，才能免責；權利人也能通知服務提供者，使用者發布的訊息涉有侵權行為，要求下架或移除資料。 不過，因朝野立委對於草案內容的「侵權」定義、要求下架的主張是否影響言論自由等尚有疑慮，因此目前草案還須協商，尚未完成二、三讀。 由於立法院本會期12月底即將結束，民進黨團總召柯建銘說，「13+N」個配套修法，行政院正在陸續送出，但都須留待下會期處理。\n",
      "關鍵字: ['行政院', '柯建銘', '立法院']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_db7a4718a67cd0fb81e7cffad2b0d4b3.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487516/\n",
      "新聞標題: 【最即時專題6】嗆謝長廷「鱷魚的眼淚」！羅智強：逼死蘇啟誠的就是你和民進黨\n",
      "發佈時間: 2018/12/21 19:38\n",
      "作者: ['即時新聞中心']\n",
      "內文: 前大阪辦事處長蘇啟誠之死，隨著遺孀曝光蘇尋短原因，外界現在又將矛頭重新指回駐日代表謝長廷身上。而準市議員羅智強，也在臉書砲轟謝長廷，「就是逼死蘇啟誠」元凶。謝長廷稍晚透過臉書回應，表示將對羅智強提告，而羅智強也再度更新發文，表示「謝長廷要告我，歡迎」、「讓社會再次公評，我説的是不是合理評論，有沒有道理？」 羅智強今日在臉書寫道：「我曾説過，這世界上有二種鱷魚。第一種鱷魚，吃了人不掉淚！另一種，吃了人還假裝掉淚。而大阪代表處蘇啟誠處長的輕生，讓我們看到民進黨政壇，充斥著這第二種鱷魚。」 羅智強說「一群『會掉淚的鱷魚』，吃人不吐骨頭就算了，吃了人，還大喊善哉慈悲、切莫殺生。可鄙可惡，令人憤怒！」更喊話「逼死蘇啟誠的，不是輿論，不是假新聞，就是民進黨政府、就是謝長廷！」\n",
      "關鍵字: None\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_c3c058fc8807f37afc9a3d68c5760c58.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487576/\n",
      "新聞標題: 楊秋興爆料「韓國瑜有壓力」 12/6已婉拒出任副市長\n",
      "發佈時間: 2018/12/21 19:17\n",
      "作者: ['王勇超', '張世瑜']\n",
      "內文: (新增：動新聞)高雄市準市長韓國瑜市政小內閣已拍板公布，與副市長一職擦身而過的前高雄縣長楊秋興，今天從廈門剛回國，又風塵僕僕趕到左營，參加韓粉們為慶祝韓國瑜25日就職，免費放送在地美食的活動。面對媒體追問他與副市長一職的轉折，楊秋興低調回應，「過去不要再談，韓市長有一些壓力」，所以他最後決定不要讓市長困擾。楊秋興表示，昨天他在廈門參加台商協會26周年大會，他去旅遊，應要求去參加大會。針對他傳簡訊給國民黨主席吳敦義，婉拒高捷、高銀董事長職務，楊秋興表示，吳敦義對他是否要入高市府小內閣很關心，當天是韓國瑜市長邀請他當高雄捷運公司董事長或者是高雄銀行董事長，他當場就婉拒市長，並表示做兩岸關係也很重要，所以他主動說：「兩岸小組我可以參與！」楊秋興強調，這是無給職，「決定了，就讓吳副總統了解！」 對於為何要婉拒加入韓國瑜小內閣？楊秋興證實，韓國瑜確實有找他幫忙，但後來自己了解到韓市長有壓力，再想想自己是去幫忙做事情的，不是去當官的，所以12/6就向韓市長旁邊的人轉達不入小內閣，雖自己沒入市府，但願意韓市長施政諮詢對象。 媒體追問，如果是副市長的職務，會不會接受？楊秋興再度強調：「現在我已經功成身退，就這樣就好！」有媒體問：「外傳王金平要擋你的路？」楊秋興說：「這個我就不予置評！」並表示韓市長有很大的壓力，他不要給韓市長困擾。 楊秋興說，他進入韓國瑜團隊的初衷，「就是去幫忙的，不是求官啦！」他跟台商的關係熟，跟大陸也有互信，所以他能義務來幫忙。媒體再追問楊秋興：「壓力是否來自國民黨高層？」楊秋興說：「這個我也不好說！」他可以體會，還是有些壓力，但不要給韓市長困擾。 楊秋興澄清，當出傳說KMT大老硬塞27個名單到韓國瑜小內閣，「後來指向我楊秋興，但其實不是！」當初韓市長11月24日與他聊天，希望他能推薦一些小內閣名單，「我確實找了7、8位左右！」名單是韓市長在用晚餐、大約晚上8、9點，他在旁邊幫忙整理一下，在出去澳門之前，稍微幫他整理一下。 楊秋興說，他拿兩張A4紙，「是擬建議名單，有十幾個，但我大概建議六、七個而已！」另外是韓市長內心已有的名單，再來是現任要留用的名單，「我把它寫下來！」扣掉四個一條鞭的單位，「其他27個的筆跡是我的沒錯，我交給韓市長，我有攝影！」楊秋興說，這兩張A4紙，「被韓市長身邊的人洩漏給媒體」，他覺得很不好，「你旁邊有小紅衛兵，我覺得很不好！」這也是他之後會重做思考的原因。 楊秋興表示，他與韓國瑜的關係沒有生變，聯繫管道仍暢通，但現在未入市政府，將來可以做他諮詢的對象，「市長現在很忙，我也不主動打電話給他！」他將向韓國瑜要邀請卡，參加25日的就職典禮。對於韓國瑜小內閣人事的看法，楊秋興說，局處很多，「要給市長一些時間」，希望他們能有好的表現。至於自己未來規劃？楊秋興則笑說：「就四處遊山玩水。」針對韓國瑜拋出高雄蓋賽馬場的話題，楊秋興似乎有不同看法，他表示，若從動物的角度，還要再斟酌。當然韓市長將想法提出來，有意見本來就可以再討論，但他個人比較接受釋昭慧法師的意見。關懷生命協會創會理事長釋昭慧昨曾表示，堅決反對韓國瑜擬發展的賽馬產業鏈，「這是人類為了利益，不惜讓動物受苦的殘忍行為」，且直指韓國瑜若硬要發展賽馬產業，會成為違法市長。倒是旗津賭場一事，楊秋興與韓國瑜似乎看法相同，楊說，他一直認為，旗津應該興建國際休閒度假村，然後可以小規模開放博奕，這在世界各國都是很普遍的狀況。\n",
      "關鍵字: ['楊秋興', '副市長', '旗津', '韓國瑜']\n",
      "圖片網址: None\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1486688/\n",
      "新聞標題: 慶祝韓國瑜就職 韓粉今起美食接力看這裡\n",
      "發佈時間: 2018/12/21 18:52\n",
      "作者: ['吳慧芬', '王勇超']\n",
      "內文: (新增：過音影片)韓國瑜將於下周二上任，從本周五起到25日就職日，天天都有熱心的「韓粉」免費招待美食，本周五有2千顆肉粽、2千碗廣東煲湯，周六有千杯杏仁茶，23日有5千碗滷肉飯、5千顆客家野薑花粽與888瓶冷泡茶，24日有1千份雞排、700份魚排，25日還有200杯綠豆冰沙可以喝，支持韓國瑜共拼經濟。選戰時大力支持韓國瑜的婦女界人士潘金英，今天下午在左營區自由路黃昏市場，準備了2千份糯米雞、2千分廣東煲湯，要免費發送給民眾，消息傳出吸引許多民眾前來排隊，下午兩點不到，就排了200多人，其中排第一號的劉先生(45歲)在胸口掛著自己用鐵絲製作的韓國瑜吊飾，他表示，他今天早上九點就來排隊，要慶祝韓市長當選「貨出得去，人進得來，高雄發大財！」英文老師謝小姐表示，她是韓國瑜的支持者，之前很多次「貪食蛇」(指韓粉慶祝韓國瑜當選，發放美食的排隊隊伍)都因為沒有時間排隊而沒參加，今天剛好有時間，就來參加，「這種活動帶動高雄的經濟！」在現場發送咖啡的業者表示，今天很高興，藉由慶祝韓國瑜當選市長的機會，特別現場請大家喝咖啡，「就像韓市長說的，東西要品質好，我們就推銷出去！」蔡姓民眾喝了咖啡，直呼：「香醇好喝！」韓國瑜選前喊出拼經濟口號，當選後「韓總美食」均受惠，韓國瑜光顧過的小吃店幾乎業績都提升，還有韓粉特製「韓總美食版清單」，讓外縣市遊客，得以按圖索驥從苓雅區滷肉飯，吃到旗山冰店，遍嚐高雄美食，帶動小吃經濟。 另有大方民眾，慶祝韓國瑜下周二上任，從本周五起天天招待美食。這包括一名住在台北韓粉郭先生，本要在25日韓國瑜當選日，於愛河畔發放1萬碗滷肉飯，受限於場地，改在23日中午於國民黨高市黨部前發送5000碗魯肉飯、5000顆客家野薑花粽，另高雄在地財記居冷泡茶，當日也將一併贈送888瓶茶飲。\n",
      "關鍵字: ['韓國瑜', '韓總美食', '就職典禮', '美食接力']\n",
      "圖片網址: None\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487718/\n",
      "新聞標題: 【最即時專題10】蘇啟誠輕生關鍵電話 台日協秘書長張淑玲：我沒打\n",
      "發佈時間: 2018/12/21 18:50\n",
      "作者: ['陳培煌']\n",
      "內文: 我駐日本大阪辦事處前處長蘇啟誠自殺過世滿百日，蘇啟誠遺孀昨發聲明指出，蘇啟誠輕生原因是「不想受到羞辱」。《蘋果》昨獨家報導，蘇啟誠在生前曾接到外交部一通電話告知懲處內容，而《聯晚》與《中時》報導，打這通電話的人是台灣日本關係協會秘書長張淑玲。張淑玲晚間出面澄清強調：「我沒有打過他(蘇啟誠)生前20分鐘電話，部長(外交部長吳釗燮)既未授權、我也沒有打那通電話，當然不會有繪聲繪影那通電話相關內容。」至於是否有其他人打過電話給蘇啟誠？張淑玲表示：「我目前沒有這樣掌握」，蘇輕生前一天20幾分鐘電話是完全沒有聽說。對於駐日代表謝長廷或代表處有打電話給蘇啟誠？張低調說：「這不方便回應！」蘇啟誠過世前兩三天，她都沒有跟蘇有任何聯絡，因有當時在台灣有公務要處理，因此後面兩天都沒有跟蘇聯絡，包括使用通訊軟體。對於蘇啟誠在風災後的情緒，張說：「我個人沒有感覺到他(蘇)有情緒上不穩定，(跟他溝通)都是一般性把事實說清楚。」對於有媒體提到蘇啟誠遺孀拒絕張淑玲參加追思會，張指出，她在蘇輕生後沒有跟家屬有直接碰過面，當時蘇的追思會都是私人朋友參加，這部分她不便談論。她強調，蘇啟誠這樣離開，同為跟蘇共事20幾年的同事都覺得痛，何況是家人。外界關注蘇啟誠與大阪辦事處原預定被懲處情況，張淑玲說，外交部已經再三澄清，當時日本風災發生後，希望可以趕快釐清相關事實，需要一段時間，當時都有對外說明，在還沒有辦法確認責任歸屬錢，不可能有懲處或調職。有媒體報導稱張淑玲有一個兒子在日本，因此積極運作想派任駐大阪處長，張淑玲鄭重澄清：「我沒有家人或親戚在日本」，她沒有表達想要派駐大阪意願，也未有長官向她提過此事，且蘇啟誠7月才赴任大阪處長，「我們在怎樣的狀況都不可能做這樣的處置！」\n",
      "關鍵字: ['日本', '大阪', '蘇啟誠', '張淑玲']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_ff333c8f2e83ba4244f92c1ae1696b5e.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487704/\n",
      "新聞標題: 談韓國瑜推賽馬產業爭議 陳其邁喊加油要他挑對人\n",
      "發佈時間: 2018/12/21 18:41\n",
      "作者: ['周昭平']\n",
      "內文: 高雄市準市長韓國瑜昨拋出中油五輕興建賽馬場議題引發爭議，競爭對手陳其邁今到台中與市長林佳龍參訪光復新村，會後受訪時被問及此問題時回應，韓國瑜還未上任，要大家多給他一點時間，多了解多看看，並意有所指說，「挑對人、挑有能力專業的人」幫助市長，壓力就不會那麼大，他說自己不會如媒體所稱要他「珍重」，反替他加油。陳其邁今回應賽馬爭議時直言，「當市長壓力非常大，韓市長的辛苦與考驗才剛開始！」但他也說，政策若能貼近民意、務實可行，加上找到好的人，在市政工作能全力幫助市長，「挑對人、挑有能力專業的人，壓力就不會那麼大，就能夠全力推動市政。」面對韓國瑜近日身陷用人及政策推動等爭議，他說自己不會如媒體評論要他「珍重」，他要替韓國瑜加油，並開玩笑說，盧秀燕剛就任，「有一個林佳龍障礙在前，壓力一定很大」，也要盧秀燕要多加油。\n",
      "關鍵字: ['陳其邁', '韓國瑜', '人事', '賽馬產業']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_b4230ec7372599977d801a9c1c1739df.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487727/\n",
      "新聞標題: 拉攏無黨無望 新北綠營高敏慧、陳文治選正副議長\n",
      "發佈時間: 2018/12/21 18:40\n",
      "作者: ['突發中心陳威叡']\n",
      "內文: 新北市議員選舉民進黨表現不如預期，從上屆原32席縮減為25席，爭取正、副議長龍頭機會微乎其微，日前原派出現任副議長陳文治拉攏無黨籍議員抗衡藍營未果，今日下午經黨團會議後，決議正副議長由高敏慧與陳文治搭配競選。                                                            國民黨本屆新北議員選舉一舉拿下33席議員席次，若加上無黨結盟，泛藍實質過半，議會龍頭寶座可謂囊中之物，正副議長由蔣根煌、陳鴻源角逐，反觀民進黨本次僅拿下25席，選舉結果不如預期，僅有拉攏無黨籍議員結盟才有機會爭取議長龍頭寶座。 新北市黨團本周二結束黨團會議後，原先推派陳文治協調拉攏無黨籍議員，結果不如預期，於今日黨團會議後，由今日下午登記參選高敏慧與陳文治搭檔爭取正副議長。\n",
      "關鍵字: ['民進黨', '新北市議長', '高敏慧', '陳文治']\n",
      "圖片網址: https://img.appledaily.com.tw/images/ReNews/20181221/640_2743cd3d3780607049e07106e061348c.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: 蘋果日報\n",
      "新聞網址: https://tw.news.appledaily.com//politics/realtime/20181221/1487422/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cc6eeb1d951c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"22:27\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappledaily_crawler2kafka\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \"\"\"\n\u001b[1;32m    410\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running job %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schedule_next_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e80c0763cd2f>\u001b[0m in \u001b[0;36mappledaily_crawler2kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"新聞網址:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mnews_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_per\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    510\u001b[0m         }\n\u001b[1;32m    511\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 )\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!pip install schedule\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "\n",
    "#def job():\n",
    "#    print(\"I'm working...\")\n",
    "#schedule.every(1).minutes.do(job)\n",
    "\n",
    "#schedule.every().day.at(\"16:42\").do(tvbs_crawler2kafka)\n",
    "#schedule.every().day.at(\"17:51\").do(setn_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(appledaily_crawler2kafka) \n",
    "#schedule.every().day.at(\"22:27\").do(nownews_crawler2kafka)\n",
    "#schedule.every().day.at(\"22:27\").do(storm_crawler2kafka())\n",
    "\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 頁   Processing:  https://www.ettoday.net/news_search/doSearch.php?keywords=%E6%94%BF%E6%B2%BB&kind=1&idx=1&page=1\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339321.htm\n",
      "url is not exists!\n",
      "新聞標題: 立法院前秘書長林錫山涉貪 監察院9比0全數通過彈劾案\n",
      "發佈時間: 2018/12/24 17:07\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 立法院前秘書長林錫山被控收賄案，涉嫌索賄3950萬元，並有2.3億不明財產，二審遭判15年、褫奪公權5年，沒收犯罪所得2.6億餘元。監察院24日以9比0的票數通過林錫山彈劾案。監察院指出，林錫山於任職期間，假借權力，收受賄賂，以圖本身之利益，涉犯不違背職務收受賄賂罪、公務員財產來源不明罪及洗錢罪，且未依法據實申報財產，有違公務員服務法之規定，其違法失職事證明確，情節重大，依法提案彈劾。本案發生於2016年初，林錫山遭檢調查出涉嫌利用辦理採購電腦軟、硬體招標案的機會，收受廠商網遠科技負責人李保承3950萬元的賄賂，甚至在高鐵上收受賄賂金錢，檢方發動搜索立法院，並對林錫山聲請羈押獲准，全案於同年5月偵查終結，林錫山等13人遭提起公訴。一審法院去年5月間宣判，重判林錫山有期徒刑16年、褫奪公權6年，另外貪污所得3950萬元、不明財產2.3億餘元，均要追繳沒收抵償。案經上訴，高等法院審理期間，林錫山坦承犯下職務上行為收賄罪，但稱自己遭廠商蠱惑，「我把持不住」。至於不明財產罪部分，他辯稱父親留有遺產，再加上自己的投資，「不能因為我有錢，就說是財產來源不明，將財產全都沒收。」不過林在高院審理期間，繳交貪污所得2800萬元；至於財產來源不明的2.4億餘元，他除了繳回部分現金以外，還陸續提出名下不動產擔保，總計提出2.18億餘元的財物。林錫山並以照顧老母親為由，在過年前請求交保。高院農曆年前作出裁定，准許林以5000萬元交保，並限制住居於臺北市大安區住所，且限制出境、出海，另要求林應於每星期一、三、五，至臺北市政府警察局大安分局安和路派出所報到。至於本案部分，高院29日做出二審宣判，改認定林錫山犯下8個《貪污治罪條例》中的「職務上行為收賄罪」、1個「財產來源不明罪」、2個「洗錢罪」，合併執行有期徒刑15年，褫奪公權5年。另外貪汙所得、不明財產，均宣告沒收抵償。可上訴。本案監察委員，江明蒼、陳慶財、劉德勳、章仁香、高鳳仙、方萬富、尹祚芊、蔡培村、包宗和共9人全數通過林錫山彈劾案，全案將移送公務員懲戒委員會。\n",
      "關鍵字: ['林錫山', '監察院']\n",
      "圖片網址: https://cdn2.ettoday.net/images/1559/1559166.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1338839.htm\n",
      "url is not exists!\n",
      "新聞標題: 把謝長廷派到日本「蔡英文高興好久」 吳子嘉：瘟神…\n",
      "發佈時間: 2018/12/24 17:06\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 前大阪辦事處處長蘇啓誠遺孀日前公開聲明，強調丈夫是在完成上級交代之檢討報書後，表明「不想受到羞辱」之遺言，以死明志。儘管駐日代表謝長廷立刻出面澄清，美麗島電子報董事長吳子嘉仍批評，總統蔡英文送瘟神，讓她高興了很久，卻倒楣碰上一個颱風，讓謝長廷再捲去是非裡面。蘇啓誠遺孀近日透過《東森新聞》聲明，表示丈夫絕無絕無憂鬱症，且並非死於假新聞的壓力，而是在完成上級交代之檢討報書後，開會之前一天，表明「不想受到羞辱」之遺言，以死明志，另也說明，「家屬手中有保留其手機之通聯記錄及手寫遺書，外界諸多說法，是刻意誤導視聽，且有卸責之嫌。」謝長廷先於21日發文澄清，蘇啓誠在檢討報告中説「輿論趁勢大肆撻伐，嚴重損及鈞部形象...深感有愧職守」，覺得他對假新聞的攻擊痛苦不堪，應該是事實；又在23日深夜表示，自己鍥而不捨追出假新聞率先散播者guruguru的身分，以及他打電話是假的，還有坐到泉佐野車站也是假的真相，「正是為蘇處長自我檢討報告的前提解套」。針對此事，吳子嘉21日在《新聞深喉嚨》表示，一個弱女子為什麼要在這個時候站出來？她對抗的是一批什麼樣的對象？對抗的是一些外交部的大官，就是個政府機器，這很可怕的事情。一個弱女子還有小孩，還有未來的日子要過下去，她這時候願意接受電視台的專訪，來捍衛什麼東西？她要捍衛她的先生的尊嚴，蘇啓誠輕生不是為了假新聞，如果大家讓這些官員繼續騙下去，就對不起這些為了國家犧牲的官員了！吳子嘉說，蔡英文怎麼可能把台日外交那麼重大的事情交給謝長廷去辦？謝長廷是一個讓蔡英文頭痛的人物，「你手下有兵馬，你有派系，你把這個恐怖人物送到日本去，把謝長廷送瘟神…送到日本去，蔡英文高興了好久！謝長廷也高興！因為他領高薪，薪水一毛錢不要花，全是公款養他！然後每天遊山玩水對不對？這個是多快樂的生活？那就表示謝長廷準備退休了」，他把他的兵馬…兵權放下來，就在民進黨的派系裡面，他等於是認輸了，這謝長廷人生的一個新的安排，問題是倒楣碰到一個颱風，謝長廷又捲到這個是非裡面，他的毛病就來了！喜歡狡辯。\n",
      "關鍵字: ['吳子嘉', '謝長廷', '蔡英文', '蘇啓誠']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3727/d3727499.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339228.htm\n",
      "url is not exists!\n",
      "新聞標題: 「未入境2年就停健保」 林靜儀建議修法：返台半年才可復保\n",
      "發佈時間: 2018/12/24 17:05\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 台灣擁有優良的醫療品質，加上使用健保之後，掛號費低廉，使許多長年居住在國外的國人，若有醫療需求時，就會特別回台灣使用，但也因此引發爭議。針對此事，民進黨立委林靜儀建議健保署「修健保法施行細則」，若2年都沒入境，就先停保，要等到返國超過半年後才能再復保。林靜儀表示，因為國外醫療費用昂貴又需等待，台灣醫療品質好又快速，用健保又價廉，因此過去有許多長居海外的公民，一旦有醫療需求，就會飛回來台灣進行治療，畢竟「來回飛一趟都划算」。第一線醫療人員都遇過這樣的民眾，對於這種只把台灣當成「可以消費低價醫療」的地方，實在讓人很氣。林靜儀說，健保讓人民不需因為費用而不敢就醫，也讓人不用因病而貧，又為了照顧弱勢取消鎖卡，繳不出健保費也先治療再說，這是照顧國人的行為，卻「被當成凱子利用，讓人眼見人性醜陋。」若是肯定台灣的醫療，相信大家都歡迎，但是如果真的「心繫祖國」，不相信是飛回來看病，甚至還賴帳！林靜儀強調，制度有道德風險，就要修正。停復保規定是在健保法施行細則，不需修法提案，權責在健保署，由署來修訂、公告。久未返國，在返國半年後再復保，避免「飛回來弄牙齒、換關節、開刀完畢就飛回去」，如果返國後就長居，自然在半年後，健保身份與其他國人無異，任何重症急症難症，都保障。這半年期間，沒有健保身份，在台灣如果需要的時候依然有醫療服務啊，自費而已；如果不放心，就保私人醫療保險，一樣是醫療保險，只是不是「全國一起幫你分擔這段期間的醫療費用」而已。最後，林靜儀說，健保署在會期開始前就和她討論過相關議題，但眼見會期快結束了，「我實在不想再等你們」，所以先提出自己的想法，希望快點修改健保法施行細則。此外，林靜儀還貼出兩張照片，其中一張為修改《全門健康保險法》實行細則第37條條文，內容是針對「失蹤未滿6個月者」、「預定出國6個月以上者。但曾辦理出國停保，於返國復保後應屆滿3個月，使得再次辦理停保」，以及「出國連續達2年以上，未有入境紀錄者，自第2年起停保」。林靜儀在旁邊寫上「2年都沒入境過，就先停保吧。不要用一點點健保費養著之後用大條的。」另一張則寫著「二年以上未入境，返國後半年才能復保。避免只把台灣拿來當便宜醫療用。外交人員及隨行配偶子女不在此限」。對於林靜儀的言論，網友們則表示，「支持修法～」、「委員加油，幫我們看緊荷包」、「我之前看到你就想罵，但這件我願100%支持你，就事論事」、「有不少欠費還是爽爽出國玩。其他欠費不出國不鎖也是個錢坑啊！」\n",
      "關鍵字: ['健保', '林靜儀']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3346/d3346793.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339294.htm\n",
      "url is not exists!\n",
      "新聞標題: 宅神睡韓國瑜的床直呼「好硬喔」 網友：翻床一般是要童男\n",
      "發佈時間: 2018/12/24 16:41\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 準高雄市長當選人韓國瑜25日就職當晚，將履行承諾，夜宿高雄市民族果菜市場，傾聽基層聲音。對此，宅神朱學恒24日南下高雄先幫韓國瑜探路，還試躺韓國瑜當晚要睡的床，並直呼「好硬喔，果然只有老兵會睡的著」朱學恒在臉書粉絲專頁「朱學恒的阿宅萬事通事務所」表示，「認真的工作中，試吃當地的滷菜橘子蘿蔔乾豆腐乳和海尼根，順便試躺明天韓國瑜要睡的床，好硬喔，果然只有老兵會睡的著」貼文一出，引來大批網友紛紛留言，「根據民間習俗這叫「翻床」，一般是要童男」、「試吃？明明還有啤酒嘛！有菜有酒，生活似神仙」、「阿宅你是民主特派記者吧！」、「莫非宅神在高雄創造了一萬個工作機會」\n",
      "關鍵字: ['宅神', '朱學恒', '韓國瑜', '果菜市場']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3786/d3786197.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339271.htm\n",
      "url is not exists!\n",
      "新聞標題: 教育部勉予同意管中閔上任 柯志恩：希望民進黨記取拔管傷害\n",
      "發佈時間: 2018/12/24 15:51\n",
      "作者: ['記者賴于榛／台北報導']\n",
      "內文: 教育部長葉俊榮24日下午三點召開記者會，他嚴詞批評台大遴選會達42分鐘，但最終對於準台大校長管中閔的上任說出「勉予同意」；國民黨立委柯志恩隨即在臉書指出，歷時將近一年的「卡管」今天畫下句點，被壓制已久的「大學自主」終於可以掙脫政治枷鎖，抬頭挺胸、昂首闊步、迎向光明，「這不僅是台大校園自主的勝利，更是台灣民主法治與正義公理的彰顯！」教育部24日下午三點召開記者會，「勉予同意」聘任管中閔校長，柯志恩表示，她肯定葉部長在沉重壓力下，秉持法律學者的良知與對公共事務正當的程序，勇敢做出一個關鍵性的正確決定！然而，身為在野黨立委，仍須嚴正指出，「拔管」歹戲皆因政府高層的愚昧、蠻橫，不惜對法治與遴選程序的踐踏扭曲，重重斲傷了大學自主的精神，也造成校園和社會紛擾不安，更重創政府的形象與公信力！希望民進黨記取\"拔管\"翻攪的傷害與教訓，還給大學自主的空間與尊嚴。柯志恩還說，不信公理喚不回！並盛讚葉部長有溫度的溝通，在冷風中格外溫馨！峰迴路轉，柳暗花明，並對願給核聘的葉俊榮一句肯定：謝謝承擔、辛苦了！歷史會記住這一哩路！台大校長遴選委員會從1月5日選出教授管中閔為新校長，教育部認定遴選程序有瑕疵，發函請台大重啟遴選，但台大拒絕並提訴願。僵局持續至今超過11個月，經歷3任教育部部長，包括潘文忠、吳茂昆以及現在的葉俊榮。葉俊榮在剛上任時多次強調用「有溫度的溝通」方法尋找「第三條路」，最後選在跨年前夕勉予同意。\n",
      "關鍵字: []\n",
      "圖片網址: https://cdn2.ettoday.net/images/3754/d3754622.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339214.htm\n",
      "url is not exists!\n",
      "新聞標題: 綠白合作釋善意 卓榮泰嘴發抖：和柯文哲「最好都是朋友」\n",
      "發佈時間: 2018/12/24 15:44\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 行政院秘書長、民進黨主席參選人卓榮泰日前對「兩岸一家親」的說法和台北市長柯文哲隔空互嗆。對此，卓榮泰24日受訪時被問到和柯文哲是否還是朋友，他一時語塞答不出來，嘴唇發抖停頓3秒後說「最好都是朋友」。民進黨遭逢大選潰敗，未來將朝綠白合作的可能，回應民眾期待，卓榮泰表示，跟任何人都應該合作「只要彼此有誠意」，只要是台灣是內部正向的力量，都應該合作，他還說，因為我們外界壓力、威脅很大，內部如果再彼此抵銷，那我們外在敵人會加倍。另外，卓榮泰被問到柯文哲算不算是討厭民進黨的人，他連說2次「應該不至於」，並解釋努力讓柯不討厭；記者追問，和柯文哲線在是朋友嗎？卓榮泰愣了一下說不出話，接著嘴唇發抖欲言又止的說，「最好都是朋友」。\n",
      "關鍵字: ['卓榮泰', '柯文哲', '綠白合作']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3786/d3786053.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339177.htm\n",
      "url is not exists!\n",
      "新聞標題: 館長笑不敢同台直播！卓榮泰不怕：時間很快會敲定\n",
      "發佈時間: 2018/12/24 14:58\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 網路紅人「館長」陳之漢日前聽聞行政院祕書長卓榮泰要約他直播，大嗆「「我笑你不敢來」，並表示沒有收到任何的邀請。對此，卓榮泰24日受訪時回應，已經開始邀約了，時堅很快就會敲定。據《鏡週刊》報導，卓榮泰認為，想要聆聽支持者的聲音，「我可以去跟陳其邁喝咖啡，我也可以去跟館長聊一聊啊！我可以唱詹雅雯的〈深情海岸〉，我也可以找鄭進一來唱〈家後〉吧！」館長表示，沒收到卓榮泰的聯絡，自己很想跟一個要做黨主席的人一起直播，「我第一個就問促轉會」、「我很想問你，你當黨主席的時候，你的立委能不能通過一些有意義的審查案」、「我笑你不敢來！你要來不用新聞叫囂、直接打電話來嘛！」針對館長在直播中的嗆聲，卓榮泰說，可以和所有對民進黨有理性對話的人談話，而和館長同台直播已經在敲定時間了，時程表很快就會出來；而是否擔心館長問題太過心辣，卓榮泰說，當然會，不過會努力讓大家了解未來民進黨的轉變。卓榮泰參選黨主席後，將和館長一起直播，外界也好奇，未來是否有可能和其他網紅對話？他表示，不排斥任何可能性，只要願意理性對話者，他都可以趁機把民進黨未來改變的方式、方向和各界交換意見，自己都非常非常樂意。\n",
      "關鍵字: ['卓榮泰', '館長', '直播', '民進黨']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3785/d3785867.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339057.htm\n",
      "url is not exists!\n",
      "新聞標題: 德機場「25秒通關」歸功於他？  駐德代表：我們只是1顆小螺絲\n",
      "發佈時間: 2018/12/24 14:09\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 世界第9大、德國規模最大的法蘭克福國際機場（Frankfurt Airport）開放10國快速通關，其中也包含台灣，使時代力量高雄黨部副執行長林子盟笑稱，自己花不到30秒就到登機閘口，原先一起排隊的陸客想尾隨，反被要求排隊，讓不少網友直呼太爽，將功勞歸給台灣駐德代表謝志偉。謝志偉則在台灣時間24日謙虛地說，「我們只是其中的一顆小螺絲，兢兢業業盡力作好我們該作的事」。謝志偉在臉書粉絲專頁上提到，關於「TAIWAN列名德國法蘭克䙐、慕尼黑機場出境自動通關測試國家」一事，有人將功勞歸給自己，「我老不回應，恐怕會造成『同意』的印象」，因此在此鄭重指出，「這樣的結果雖然十分符合我在德國前後兩任至今随時隨地所宣揚『Taiwan是個自由民主的國家，值得珍惜與支持，Taiwan有自己的護照，不是China的一部份』之內容，但卻不是我的功勞」，而是總統、外交部長、外交部同仁和國人的努力。謝志偉稱，如果說駐德館處在「出境自動通關測試」也沾上了邊，「我們只是其中的一顆小螺絲，兢兢業業盡力作好我們該作的事」，也就是讓德國人從中央到地方知道、感受到「TAIWAN」是民主陣營的一份子。他代表總統蔡英文的意思與意志來到德國，自知還有很多事情要做，也知道有些事情不做會來不急，有些事情要做也急不來，「台灣人，氣要長，但是不要氣太久。」日前人在德國的林子盟表示，他拿著護照在法蘭克福機場排隊準備離開時，海關人員上前用英文詢問是否來自台，隨即被帶離排了約數十人的隊伍，引導他到自動通關口，花了大概20秒做數位臉孔確認及5秒海關蓋章，前後不到30秒便迅速通關，輕鬆前往登機閘口。林子盟說，後面同是黃種人臉孔的旅客一陣錯愕。海關人員接著用英文攔住想尾隨的人「你這是中國護照，必須排隊」，他大讚，不愧是嚴謹的德國政府，「台灣中國分的一清二楚」。目前可在德國機場快速通關的僅有10個國家護照，包含「台灣、澳洲、紐西蘭、加拿大、日本、南韓、新加坡、美國、智利、阿拉伯聯合大公國」。林子盟表示，很謝謝台灣的外交人員，謝謝在地經營的台僑，「謝謝祖國的民主自由」。\n",
      "關鍵字: ['德國', '快速通關', '駐德代表', '林子盟', '謝志偉', '法蘭克福國際機場', 'Frankfurt Airport']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3785/3785732.jpg\n",
      "\n",
      "\n",
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339086.htm\n",
      "url is not exists!\n",
      "新聞標題: 快訊／淡海輕軌通車首日就「訊號異常」 已停駛逾半小時\n",
      "發佈時間: 2018/12/24 12:37\n",
      "作者: ['政治中心／新北報導']\n",
      "內文: 新北市長朱立倫23日傍晚才親自率賓客搭乘淡海輕軌，24日早上6點開始試營運，卻在通車5小時24分後發生列車異常事故。輕軌1032列車在上午11時54分發出異常訊號，淡金鄧公站、淡金北新站各卡一台車。新北捷運總經理吳國濟對此表示，目前異常已排除，淡海輕軌恢復正常運作。有網友在臉書社團「細說淡水」上表示，「捷運通車即卡車，11時46分車到鄧公站就無法啟動，電力全無，當然無空調，4分鐘後有少少風。11時56分有人發聲問『可以告訴我們為何等10分鐘沒開車嗎？』但卻沒有人回應。12時03分前門開啓，大家自動下車，沒有答案。之後才廣播因車輛調度所以暫停，到現在12時12分還停在鄧公站。」針對通車首日就異常，鄉民們在MRT版上提到，「現在在淡金北新站，卡住ing」、「往紅樹林方向的應該全停了，因為我們這輛車卡在淡金鄧公」、「為了搶卸任前通車，品質堪憂」、「是要考驗台灣製造的品質嗎？」、「淡海輕軌對台灣來說很重要，因為是第一個真正的國車國造案，如果台車這批品質不好，之後的輕軌應該還是回到直接跟外國買車，那好不容易建立的本土工業又浪費了」。新北捷運總經理吳國濟指出，淡海輕軌在快接近12時，在淡金鄧公站發生列車異常狀態，初步判斷是因為濕氣太重，所以按鈕異常；為了安全起見，司機員啟動了所謂「清車作業」，讓民眾離開車輛、下到月台，經過技術人員現場維修，在20分鐘內就排除異常，現在列車已正常行駛，淡海輕軌也恢復正常運作。淡海輕軌分為綠山線和海山線，以兩線連接淡海新市鎮和漁人碼頭，其中綠山線率先於24日進行第一階段通車，起始站從紅樹林出發，經由淡水主要道路淡金公路前往淡海新市鎮，終點站為崁頂站，全線共11個停靠站，總長7.3公里，此外，淡海輕軌由藝術家幾米做全線公共藝術設計，量身打造多個藝術作品，在多個車站都可看到可愛的幾米公仔。淡海輕軌綠山線從24日起試營運，開放民眾免費搭乘一個月，試營運期間班距為15分鐘一班，每天營業時間為早上6點30分至晚上22點。淡海輕軌票價20元起跳，單程票價最高25元。朱立倫23日親自搭乘淡海輕軌時稱，試乘期間一定是聽從大家的意見，如果需要改進的，捷運局跟捷運公司都會來改進，淡海輕軌經過初勘、履勘再三的檢驗，它的安全、舒適、穩定度都沒有問題。\n",
      "關鍵字: ['輕軌', '淡海輕軌']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3723/d3723781.jpg\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sending messages ...\n",
      "Message sending completed!\n",
      "新聞來源: ETtoday\n",
      "新聞網址: https://www.ettoday.net/news/20181224/1339035.htm\n",
      "url is not exists!\n",
      "新聞標題: 台灣民意基金會：6成7不支持蔡英文連任 60.8％挺賴清德出馬\n",
      "發佈時間: 2018/12/24 12:09\n",
      "作者: ['政治中心／綜合報導']\n",
      "內文: 民進黨在九合一大選慘敗，外界將焦點放在2020年總統大選，但民進黨的民調也跌低谷。台灣民意基金會24日發布民意調查結果，顯示出高達6成7的人不支持總統蔡英文連任；且若在蔡英文和賴清德兩人中選一個，有60.8％的人支持賴清德出馬，獲得壓倒性勝利。台灣民意基金會24日上午進行「2018年終台灣重大民意走向」發表會，由董事長游盈隆負責主持。根據民調顯示，受調者被問到「現任民進黨籍總統蔡英文如果尋求連任，您會支持或不支持？」時，有7%非常支持，18%還算支持，3.9%沒意見，26.6%不太支持，40.8%一點也不支持。換句話說，在20歲以上的台灣人中，有2成5基本上支持蔡英文尋求連任，但有高達6成7的人不支持，且值得注意的是，有4成1的人強烈不支持蔡英文尋求連任。至於「民進黨內另一位可能參選總統的人是行政院長賴清德。民進黨如果舉行總統初選，在蔡英文與賴清德兩人當中，您會支持誰？」，結果顯示，有60.8%支持賴清德，19.9%支持蔡英文，19.3%沒意見、不知道、拒答。換言之，若民進黨舉辦總統初選，且參選者只有蔡英文和賴清德，依照這份民調顯示，賴清德將獲得壓倒性勝利。\n",
      "關鍵字: ['蔡英文', '賴清德', '台灣民意基金會']\n",
      "圖片網址: https://cdn2.ettoday.net/images/3771/d3771560.jpg\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-085761ab5e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mettoday_crawler2kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-728b99e7b14a>\u001b[0m in \u001b[0;36mettoday_crawler2kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m             j = {\"source\": \"ETtoday\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n\u001b[1;32m    104\u001b[0m                  \"kw\": keyword, \"img_url\": img_url}\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# TCP連線到logstash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ettoday_crawler2kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將json檔每則新聞作者取出用方法清理\n",
    "for i in range(0, len(json_data)):  \n",
    "    if json_data[i][\"author\"] == []:\n",
    "        author = []\n",
    "        print(author)\n",
    "    else:\n",
    "        tvbs_author_etl(json_data[i][\"author\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming從每單一json檔取出新聞作者用方法清理\n",
    "if j[\"author\"] == []:\n",
    "    author = []\n",
    "    print(author)\n",
    "else:\n",
    "    tvbs_author_etl(j[\"author\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/2.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/3.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/4.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/5.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/6.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/7.html \n",
      "Current Thread Name Thread-9, Url: http://www.pythontab.com/html/pythonjichu/8.html \n",
      "Current Thread Name Thread-10, Url: http://www.pythontab.com/html/pythonjichu/9.html \n",
      "Done, Time cost: 11.269868850708008 \n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import threading, queue, time, urllib\n",
    "from  urllib import request\n",
    "baseUrl = 'http://www.pythontab.com/html/pythonjichu/'\n",
    "urlQueue = queue.Queue()\n",
    "for i in range(2, 10):\n",
    "    url = baseUrl + str(i) + '.html'\n",
    "    urlQueue.put(url)\n",
    "    #print(url)\n",
    "def fetchUrl(urlQueue):\n",
    "    while True:\n",
    "        try:\n",
    "            #不阻塞的读取队列数据\n",
    "            url = urlQueue.get_nowait()\n",
    "            i = urlQueue.qsize()\n",
    "        except Exception as e:\n",
    "            break\n",
    "        print ('Current Thread Name %s, Url: %s ' % (threading.currentThread().name, url))\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            responseCode = response.getcode()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if responseCode == 200:\n",
    "            #抓取内容的数据处理可以放到这里\n",
    "            #为了突出效果， 设置延时\n",
    "            time.sleep(1)\n",
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    threads = []\n",
    "    # 可以调节线程数， 进而控制抓取速度\n",
    "    threadNum = 2\n",
    "    for i in range(0, threadNum):\n",
    "        t = threading.Thread(target=fetchUrl, args=(urlQueue,))\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        #多线程多join的情况下，依次执行各线程的join方法, 这样可以确保主线程最后退出， 且各个线程间没有阻塞\n",
    "        t.join()\n",
    "    endTime = time.time()\n",
    "    print ('Done, Time cost: %s ' %  (endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 步驟1.設定要連線到Kafka集群的相關設定, 並產生一個Kafka的Producer的實例\n",
    "    producer = KafkaProducer(\n",
    "        # Kafka集群在那裡?\n",
    "        bootstrap_servers=[\"kafka1:29092\"],\n",
    "        # 指定msgKey的序列化器, 若Key為None, 無法序列化, 透過producer直接給值\n",
    "        key_serializer = str.encode,\n",
    "        # 指定msgValue的序列化器\n",
    "        value_serializer = lambda m: json.dumps(m).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "        \n",
    "    ###TVBS爬蟲開始###\n",
    "    url = \"https://news.tvbs.com.tw/politics/\"\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    news_ul = soup.find_all(\"ul\", id = \"block_pc\")[0]\n",
    "    news_a = news_ul.find_all(\"a\")\n",
    "\n",
    "    news_no_string_list = []\n",
    "    for a in news_a:\n",
    "        news_no_string = a[\"data-news-id\"]\n",
    "        news_no_string_list.append(news_no_string)\n",
    "\n",
    "    news_no_list = []\n",
    "    for n in news_no_string_list:\n",
    "        news_no_int = n.strip('\\'')\n",
    "        news_no_list.append(news_no_int)\n",
    "\n",
    "    #json_data = []\n",
    "    for news_no in news_no_list:\n",
    "\n",
    "        print(\"新聞來源:\", \"TVBS\")\n",
    "\n",
    "        news_url = \"https://news.tvbs.com.tw/politics/\" + news_no\n",
    "        print(\"新聞網址:\", news_url)\n",
    "\n",
    "        \n",
    "        #判斷網址是否爬過\n",
    "        bf = BloomFilter()\n",
    "        # Unicode-objects must be encoded before hashing\n",
    "        news_url_u = news_url.encode('utf-8') \n",
    "        if bf.isContains(news_url_u):   # 判断字符串是否存在\n",
    "            print('exists!')\n",
    "            continue\n",
    "        else:\n",
    "            print('not exists!')\n",
    "            bf.insert(news_url_u)\n",
    "        \n",
    "\n",
    "        req = requests.get(news_url)\n",
    "        # requests如果找不到指定編碼，會猜測網頁編碼，有時會形成亂碼，故給指定編碼utf-8\n",
    "        req.encoding = (\"utf-8\")\n",
    "        soup = BeautifulSoup (req.text, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"h1\", class_ = \"margin_b20\").text\n",
    "        title = re.sub(r\"\\u3000\", \" \", title)\n",
    "        title = re.sub(r\"\\xa0\", \" \", title)\n",
    "        print(\"新聞標題:\", title)\n",
    "\n",
    "        date = soup.find(\"div\", class_ = \"icon_time time leftBox2\").text\n",
    "        print(\"發佈時間:\", date)\n",
    "\n",
    "        author = []\n",
    "        author_text = soup.find(\"h4\", class_ = \"font_color5 leftBox1\").text\n",
    "        author_text = re.sub(r\"攝影.*報導\", \"\", author_text)\n",
    "        author.append(author_text)\n",
    "        print(\"作者:\", author)\n",
    "\n",
    "        content = soup.find(\"div\", class_ = \"h7 margin_b20\").text\n",
    "        content = content.replace(\"\\t\", \"\").replace(\"   \",\"\").replace(\"\\n\", \"\").replace(\"（中央社）\",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"最HOT話題在這！想跟上時事，\",\"\").replace(\"快點我加入TVBS新聞LINE好友！ \",\"\")\n",
    "        content = content.replace(\"TVBS最新大數據分析\",\"\").replace(\"看完整內容快點我加入TVBS新聞LINE好友！\",\"\")\n",
    "        print(\"內文:\", content)\n",
    "\n",
    "        keyword = soup.find(\"div\", class_ = \"adWords\").text\n",
    "        keyword = keyword.replace(\"\\t\", \"\")\n",
    "        keyword = keyword.replace(\",\",\"\")\n",
    "        keyword = keyword.replace(\"\\n\", \"\").replace(\"編輯  \",\"\").replace(\" 報導\",\"\")\n",
    "        keyword = keyword.replace(\"記者\",\"\").replace(\"      \",\",\")\n",
    "        keyword = keyword.split(\",\")\n",
    "        print(\"關鍵字\", keyword)\n",
    "\n",
    "        img = soup.find(\"div\", class_ = \"margin_b20\")\n",
    "        img_url = img.find('img')['src']\n",
    "        print(\"圖片網址:\", img_url)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        j = {\"source\": \"TVBS\" ,\"url\": news_url, \"title\": title, \"date_\": date, \"author\": author, \"content\": content, \n",
    "             \"kw\": keyword, \"img_url\": img_url}\n",
    "        #json_data.append(j)\n",
    "    ###TVBS爬蟲結束###\n",
    "    \n",
    "\n",
    "   \n",
    "        # 步驟2.指定想要發佈訊息的topic名稱\n",
    "        topic_name = \"test\"\n",
    "\n",
    "        try:\n",
    "            print(\"Start sending messages ...\")\n",
    "            # 步驟3.產生要發佈到Kafka的訊息\n",
    "            # - 參數  # 1: topicName\n",
    "            # - 參數  # 2: msgKey\n",
    "            # - 參數  # 3: msgValue\n",
    "\n",
    "            producer.send(topic = topic_name, key = \"tvbs\", value = j)\n",
    "            print(\"Message sending completed!\")\n",
    "        except Exception as e:\n",
    "            # 錯誤處理\n",
    "            e_type, e_value, e_traceback = sys.exc_info()\n",
    "            print(\"type ==> %s\" % (e_type))\n",
    "            print(\"value ==> %s\" % (e_value))\n",
    "            print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "            print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "            print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "\n",
    "\n",
    "        #finally:\n",
    "            # 步驟4.關掉Producer實例的連線\n",
    "            #producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
