{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正則表達式:TVBS清理作者格式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#作者欄位清理方法，要放author[0]\n",
    "def tvbs_author_etl(author_text):\n",
    "    author = []\n",
    "    author_text = re.sub(r\"記者  \", \"\", author_text)\n",
    "    author_text = re.sub(r\" / 攝影.*\", \"\", author_text)\n",
    "    author_text = re.sub(r\" / \", \"\", author_text)\n",
    "    author_text = re.sub(r\"編輯  \", \"\", author_text)\n",
    "    author_text = re.sub(r\" 報導\", \"\", author_text)\n",
    "    author_text = re.sub(r\" \", \",\", author_text)\n",
    "    #如果有多位作者，使用、斷開，並一一取出放進list\n",
    "    if author_text.find(\",\") != -1:\n",
    "        author_text = author_text.split(\",\")\n",
    "        for a in author_text:\n",
    "            author.append(a)\n",
    "    elif author_text == \"\":\n",
    "        author = []\n",
    "    else:\n",
    "        author.append(author_text)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正則表達式:SETN清理作者格式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETN作者欄位清理方法，要放author[0]\n",
    "def setn_author_etl(author_text):\n",
    "    author = []\n",
    "    #將／XX報導XX字串清除\n",
    "    author_text = re.sub(r\"／.*報導.*\", \"\", author_text)\n",
    "    #將／XX特稿XX字串清除\n",
    "    author_text = re.sub(r\"／.*特稿.*\", \"\", author_text)\n",
    "    #將記者字串清除\n",
    "    author_text = re.sub(r\"記者\", \"\", author_text)\n",
    "    #將文／字串清除\n",
    "    author_text = re.sub(r\"文／\", \"\", author_text)\n",
    "    #如果有多位作者，使用、斷開，並一一取出放進list\n",
    "    if author_text.find(\"、\") != -1:\n",
    "        author_text = author_text.split(\"、\")\n",
    "        for a in author_text:\n",
    "            author.append(a)\n",
    "    else:\n",
    "        author.append(author_text)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章正反面情緒分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在dockerfile的指令\n",
    "#RUN !pip install udicOpenData\n",
    "#RUN !pip install Swinger\n",
    "#ADD ./dict_all.txt /opt/conda/lib/python3.6/site-packages/udicOpenData/dictionary/\n",
    "from udicOpenData.dictionary import *\n",
    "jieba.load_userdict(os.path.join(DIR_NAME, 'dict_all.txt'))\n",
    "#要等一下子\n",
    "from Swinger import Swinger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章正反面情緒分析方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emo_swinger(content, model='LogisticRegression'):\n",
    "    s = Swinger()\n",
    "    s.load(model) \n",
    "    # default model= \"LogisticRegression\", 可替換為\"MultinomialNB\"\n",
    "    emo_result = s.swing(content)\n",
    "    if emo_result == \"pos\":\n",
    "        result = \"positive\"\n",
    "    elif emo_result == \"neg\":\n",
    "        result = \"negative\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章藍綠貼標分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將jieba斷詞寫成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutflow(p):\n",
    "    cutresult = \" \".join(jieba.cut(p))\n",
    "    return cutresult.replace(\"\\r\", \"\").replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入詞向量模型與貝氏模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clg = joblib.load('clg') \n",
    "vec = joblib.load('vec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章藍綠貼標分析方法:貝氏定理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_predict(text):\n",
    "    docs_news = cutflow(text)\n",
    "    c=[{\"content\" : docs_news,\n",
    "       \"tag\" : 0}]\n",
    "    d = pd.DataFrame(c)\n",
    "    test_counts = vec.transform(d[\"content\"])\n",
    "    pre = clg.predict(test_counts)\n",
    "    label_result = list(pre)[0]\n",
    "    if label_result == '1':\n",
    "        result = \"blue\"\n",
    "    elif label_result == '0':\n",
    "        result = \"green\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章摘要分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "!pip install textrank4zh\n",
    "from  __future__  import print_function\n",
    "import jieba\n",
    "import numpy\n",
    "import networkx\n",
    "import sys\n",
    "import codecs \n",
    "from textrank4zh import TextRank4Keyword ,TextRank4Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章摘要方法:TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_summary(text):\n",
    "    tr4s = TextRank4Sentence()\n",
    "    tr4s.analyze( text = text, lower = True , source = ' all_filters ' )\n",
    "    for item in tr4s.get_key_sentences( num = 1 ):\n",
    "        return item.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Elasticsearch連線\n",
    "es = Elasticsearch(['elasticsearch:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在Elasticsearch建立index，亦可加入body參數先定義mapping格式\n",
    "es.indices.create(index = 'news', ignore = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將Elasticsearch定義所放入的資料格式:mapping\n",
    "es.indices.put_mapping(index = \"news\",\n",
    "                       doc_type = \"politics\",                \n",
    "                       body = {\n",
    "                        \"properties\": {\n",
    "                            \"source\": {\"type\": \"text\"},\n",
    "                            \"url\": {\"type\": \"text\"},\n",
    "                            \"title\": {\"type\": \"text\"},\n",
    "                            \"date_\": {\"type\": \"text\"},\n",
    "                            \"author\": {\"type\": \"text\"},\n",
    "                            \"content\": {\"type\": \"text\"},\n",
    "                            \"kw\": {\"type\": \"text\"},\n",
    "                            \"img_url\": {\"type\": \"text\"},\n",
    "                            \"@timestamp\": {\"type\": \"date\"}\n",
    "                        }\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了將匯入資料的時間加入時區\n",
    "from datetime import datetime, timedelta, timezone\n",
    "tz_utc_8 = timezone(timedelta(hours=8)) # 創建時區UTC+8:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Consumer需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka in /opt/conda/lib/python3.6/site-packages (1.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "import sys\n",
    "import json, time, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start listen incoming messages ... \n",
      "\n",
      "load default bestMainFeatures\n",
      "load default bestMainFeatures success!!\n",
      "load model from LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator DictVectorizer from version 0.18.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.18.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.18.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic=test, partition=0, offset=36 : (key=storm, value={'source': '風傳媒 THE STORM MEDIA', 'url': 'https://www.storm.mg/article/736060', 'title': '台中立委補選》綠營打「還林佳龍一個公道」\\u3000王義川盼能以「奇兵」之姿致勝', 'date_': '2018-12-22 09:10', 'author': ['周思宇'], 'content': '下月台中立委補選（北北屯區），被視為市長選舉延長賽。面對沙場老將、前立委沈智慧，民進黨推出前交通局長王義川應戰，據指出，該區屬於深藍票倉，極度不利綠營，民進黨才以「奇兵」應戰，盼出奇制勝。再者，因補選投票率只有三成，關鍵在動員力，地方已有「還林佳龍公道」的氛圍，可強化綠營投票意願。「不敗女王」優勢壓制\\u3000王義川選戰近乎「不可能的任務」台中市北區、北屯區是盧秀燕立委選區，因其優勢結構，一共連任六屆立委，甚至被譽為「不敗女王」。如以近三屆立委選戰觀察，盧秀燕在2008年、2012年都以近六成的得票率，力壓民進黨；在2016年大環境有利民進黨的情況下，綠營也禮讓台聯的劉國隆，盧秀燕最後囊括五成二選票，穩住這席立委，可見其實力。如進一步以林佳龍、盧秀燕於九合一選舉北區、北屯區的得票分析，盧秀燕在其「大本營」都以六成的得票率，「完勝」林佳龍的四成選票。台中市北區、北屯區是準台中市長盧秀燕原選區，一共連任六屆立委，甚至被譽為「不敗女王」。民進黨此次推出台中市前交通局長王義川應戰，勢必將迎來一場硬戰。（資料照，陳品佑攝）知名度低\\u3000王義川要走「網紅行銷」王義川面對幾乎「不可能的任務」，民進黨人士直指，因王義川目前缺乏「全國知名度」，主要策略將以「網紅」行銷突圍，以另類方式炒熱選情，如以網路吸引目光，「好讓民眾覺得有趣」；該人士觀察，王義川投入參選後，因其勇於替政策辯護的形象、善用庶民語言的風格，綠營基層反應是好的。這位輔選人士進一步指出，除了因補選投票率低，必須全力動員外，在選戰的最後階段將主攻盧秀燕的「失誤」，目前已有暫緩山手線的失言、準財政局長羅仙法被爆有家暴史、任用黑派人馬吳皇昇任新聞局長等，「地方上認為，盧秀燕已經贏了，卻又與地方派系合流，應該要還給林佳龍一個公道。」另一名不具名民進黨立委也說，補選的低投票率，就是王義川可著力之處，也是贏的關鍵。他分析，這次九合一選戰，國民黨因「韓流」加速藍軍集結而大勝，但補選非全國性議題，且「韓國瑜效應」稍退，加上國民黨「太陽們」因總統提名而動作頻頻，因內部不團結，藍營不見得有投票意願，這都是機會。接地氣、營造青年從政亮點「突顯與沈智慧差別」該名立委也說，如果以傳統的政治思考、政治判斷，王義川一定不會贏。因此，如何在短時間內衝高知名度、整合泛綠勢力，並且熟稔地方議題，非常重要。選舉策略必須營造王義川與沈智慧的「極大差別」，除了先前主打的「61年次對上61歲」外，必須令選民覺得有青年從政的亮點、新思維。這位立委也表示，王義川雖然擁有台大土木博士的頭銜，但不像一般正經八百的學者，反而像是基層出身的民代，講話頗「接地氣」，更不時穿插台語、俚語，擅於與選民「搏感情」，是民進黨傳統支持者喜愛的風格。台中立委補選（北北屯區），民進黨推出前台中市交通局長王義川應戰。資深黨內市議員也示警，指出王義川有其交通工程上的專業，但知名度不高。（取自王義川臉書）不過，一名民進黨資深市議員也示警，王義川有其交通工程上的專業，但地方知名度不高，端視文宣如何發揮、找到爆發點，否則光靠原有組織，「仍須加把勁」；他也說，林佳龍市長「慘輸」，短時間翻回的可能性「確實有困難。」對於沈智慧的基層實力，綠營輔選人士說，沈智慧上一屆立委選戰耕耘中西東南區，挑戰民進黨黃國書失利，此次補選是「跨區」參戰，雖有高知名度，但在該區（北北屯）力挺的市議員只有自己的胞妹沈佑蓮；反觀綠營這次在北區、北屯區各掉了一席議員，「大家的心態是不能再輸了」，更形團結，甚至期待王義川上去後，下屆市議員才有機會。', 'kw': ['王義川', '台中市立委補選', '沈智慧', '政局動態'], 'img_url': 'https://image.cache.storm.mg/styles/smg-800x533-fp/s3/media/image/2018/12/21/20181221-030041_U2008_M485552_d810.jpg?itok=CEXPLA8f', 'ariticle_emotion': 'positive', 'label': 'green', 'abstract': '台中立委補選（北北屯區），民進黨推出前台中市交通局長王義川應戰'})\n",
      "\n",
      "\n",
      "Start sending data to MySQL !!!\n",
      "<Response [500]>\n",
      "type ==> <class 'simplejson.errors.JSONDecodeError'>\n",
      "value ==> Expecting value: line 1 column 1 (char 0)\n",
      "traceback ==> file name: <ipython-input-23-1cfbf6d7642d>\n",
      "traceback ==> line no: 72\n",
      "traceback ==> function name: <module>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 步驟1.設定要連線到Kafka集群的相關設定, 產生一個Kafka的Consumer的實例\n",
    "    consumer = KafkaConsumer(\n",
    "        # Kafka集群在那裡?\n",
    "        bootstrap_servers=[\"kafka1:29092\"],\n",
    "        # ConsumerGroup的名稱\n",
    "        group_id=\"test01\",\n",
    "        # 指定msgKey的反序列化器, 若Key為None, 無法反序列化\n",
    "        key_deserializer=bytes.decode,\n",
    "        # 指定msgValue的反序列化器\n",
    "        #value_deserializer=bytes.decode,\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        # 是否從這個ConsumerGroup尚未讀取的partition / offset開始讀\n",
    "        auto_offset_reset=\"earliest\",\n",
    "    )\n",
    "    # 步驟2.指定想要訂閱訊息的topic名稱\n",
    "    topic_name = \"test\"\n",
    "    # 步驟3.讓Consumer向Kafka集群訂閱指定的topic\n",
    "    consumer.subscribe(topics = topic_name)\n",
    "\n",
    "\n",
    "    # 步驟4.持續的拉取Kafka有進來的訊息\n",
    "    try:\n",
    "        print(\"Start listen incoming messages ...\",\"\\n\")\n",
    "        # 持續監控是否有新的record進來 - 方法一\n",
    "        for record in consumer:\n",
    "            topic = record.topic\n",
    "            partition = record.partition\n",
    "            offset = record.offset\n",
    "            timestamp = record.timestamp\n",
    "        \n",
    "            # 取出msgKey與msgValue\n",
    "            msgKey = record.key\n",
    "            msgValue = record.value\n",
    "            \n",
    "            # 將新聞作者欄位做處理後再放回\n",
    "            if msgValue[\"source\"] == \"TVBS\":      \n",
    "                msgValue[\"author\"] = tvbs_author_etl(msgValue[\"author\"][0])\n",
    "            elif msgValue[\"source\"] == \"SETN\":\n",
    "                msgValue[\"author\"] = setn_author_etl(msgValue[\"author\"][0])\n",
    "            \n",
    "            \n",
    "            # 將新聞內文做文章情緒分析並加入欄位與結果\n",
    "            msgValue[\"ariticle_emotion\"] = emo_swinger(msgValue[\"content\"])\n",
    "            # 將新聞內文做藍綠分析並加入欄位結果\n",
    "            msgValue[\"label\"] = label_predict(msgValue[\"content\"])\n",
    "            # 將新聞做文章摘要並加入欄位與結果\n",
    "            msgValue[\"abstract\"] = news_summary(msgValue[\"content\"])\n",
    "            \n",
    "            # 秀出metadata與msgKey & msgValue訊息\n",
    "            print(\"topic=%s, partition=%s, offset=%s : (key=%s, value=%s)\" % (record.topic, record.partition,\n",
    "                                                                              record.offset, record.key, msgValue))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            print(\"Start sending data to MySQL !!!\")\n",
    "            # 將資料送入MySQL\n",
    "            ip_location = 'chatbot_api'\n",
    "                         \n",
    "            News = msgValue\n",
    "            \n",
    "            # 將json傳回API Server\n",
    "            Endpoint = 'http://%s:5000/news' % (ip_location)\n",
    "\n",
    "            # header要特別註明是json格式\n",
    "            Header = {'Content-Type':'application/json'}\n",
    "            \n",
    "            # 傳送post對API server新增資料 \n",
    "            Response = requests.post(Endpoint,headers=Header,data=json.dumps(News))\n",
    "\n",
    "            # 印出Response的資料訊息\n",
    "            print(Response)\n",
    "            Response = Response.json()\n",
    "            print(Response)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        \n",
    "            print(\"Start sending data to elasticsearch !!!\")\n",
    "            # 將資料送入Elasticsearch\n",
    "            elasticsearch_data = msgValue\n",
    "            elasticsearch_data[\"@timestamp\"] = datetime.now().replace(tzinfo = tz_utc_8)            \n",
    "            # 將資料放入Elasticsearch，若沒有index，會直接建立index、type \n",
    "            es.index(index = 'news', doc_type = 'politics', body = elasticsearch_data, refresh = True )\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "    finally:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
