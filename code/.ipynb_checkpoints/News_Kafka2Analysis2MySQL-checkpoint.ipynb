{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正則表達式:TVBS清理作者格式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#作者欄位清理方法，要放author[0]\n",
    "def tvbs_author_etl(author_text):\n",
    "    author = []\n",
    "    author_text = re.sub(r\"記者  \", \"\", author_text)\n",
    "    author_text = re.sub(r\" / 攝影.*\", \"\", author_text)\n",
    "    author_text = re.sub(r\" / \", \"\", author_text)\n",
    "    author_text = re.sub(r\"編輯  \", \"\", author_text)\n",
    "    author_text = re.sub(r\" 報導\", \"\", author_text)\n",
    "    author_text = re.sub(r\" \", \",\", author_text)\n",
    "    #如果有多位作者，使用、斷開，並一一取出放進list\n",
    "    if author_text.find(\",\") != -1:\n",
    "        author_text = author_text.split(\",\")\n",
    "        for a in author_text:\n",
    "            author.append(a)\n",
    "    elif author_text == \"\":\n",
    "        author = []\n",
    "    else:\n",
    "        author.append(author_text)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正則表達式:SETN清理作者格式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETN作者欄位清理方法，要放author[0]\n",
    "def setn_author_etl(author_text):\n",
    "    author = []\n",
    "    #將／XX報導XX字串清除\n",
    "    author_text = re.sub(r\"／.*報導.*\", \"\", author_text)\n",
    "    #將／XX特稿XX字串清除\n",
    "    author_text = re.sub(r\"／.*特稿.*\", \"\", author_text)\n",
    "    #將記者字串清除\n",
    "    author_text = re.sub(r\"記者\", \"\", author_text)\n",
    "    #將文／字串清除\n",
    "    author_text = re.sub(r\"文／\", \"\", author_text)\n",
    "    #如果有多位作者，使用、斷開，並一一取出放進list\n",
    "    if author_text.find(\"、\") != -1:\n",
    "        author_text = author_text.split(\"、\")\n",
    "        for a in author_text:\n",
    "            author.append(a)\n",
    "    elif author_text == \"\":\n",
    "        author = []\n",
    "    else:\n",
    "        author.append(author_text)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正則表達式:ETtoday清理作者格式方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETtoday作者欄位清理方法，要放author[0]\n",
    "def ettoday_author_etl(author_text):\n",
    "    author = []\n",
    "    author_text = re.sub(\"／.?.?報導.*\", \"\", author_text)\n",
    "    author_text = re.sub(\".*記者\", \"\", author_text)\n",
    "    author_text = re.sub(\"\\xa0\", \"\", author_text)\n",
    "    #如果有多位作者，使用、斷開，並一一取出放進list\n",
    "    if author_text.find(\"、\") != -1:\n",
    "        author_text = author_text.split(\"、\")\n",
    "        for a in author_text:\n",
    "            author.append(a)\n",
    "    elif author_text == \"\":\n",
    "        author = []\n",
    "    else:\n",
    "        author.append(author_text)\n",
    "    return author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章正反面情緒分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install udicOpenData\n",
    "#!pip install Swinger\n",
    "#在dockerfile的指令\n",
    "#ADD ./dict_all.txt /opt/conda/lib/python3.6/site-packages/udicOpenData/dictionary/\n",
    "from udicOpenData.dictionary import *\n",
    "jieba.load_userdict(os.path.join(DIR_NAME, 'dict_all.txt'))\n",
    "#要等一下子\n",
    "from Swinger import Swinger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章正反面情緒分析方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = None\n",
    "def emo_swinger(content, model='LogisticRegression'):\n",
    "    global s\n",
    "    if s == None:\n",
    "        s = Swinger()\n",
    "        s.load(model)\n",
    "    # default model= \"LogisticRegression\", 可替換為\"MultinomialNB\"\n",
    "    emo_result = s.swing(content)\n",
    "    if emo_result == \"pos\":\n",
    "        result = \"positive\"\n",
    "    elif emo_result == \"neg\":\n",
    "        result = \"negative\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章關鍵字 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章關鍵字方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_analysis(content):\n",
    "    #default kewwords = 8\n",
    "    kws =jieba.analyse.extract_tags(content, 5)\n",
    "    return kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章藍綠貼標分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將jieba斷詞寫成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutflow(p):\n",
    "    cutresult = \" \".join(jieba.cut(p))\n",
    "    return cutresult.replace(\"\\r\", \"\").replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入詞向量模型與貝氏模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clg = joblib.load('clg') \n",
    "vec = joblib.load('vec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章藍綠貼標分析方法:貝氏定理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_predict(text):\n",
    "    docs_news = cutflow(text)\n",
    "    c=[{\"content\" : docs_news,\n",
    "       \"tag\" : 0}]\n",
    "    d = pd.DataFrame(c)\n",
    "    test_counts = vec.transform(d[\"content\"])\n",
    "    pre = clg.predict(test_counts)\n",
    "    label_result = list(pre)[0]\n",
    "    if label_result == '1':\n",
    "        result = \"blue\"\n",
    "    elif label_result == '0':\n",
    "        result = \"green\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章摘要分析 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "!pip install textrank4zh\n",
    "from  __future__  import print_function\n",
    "import jieba\n",
    "import numpy\n",
    "import networkx\n",
    "import sys\n",
    "import codecs \n",
    "from textrank4zh import TextRank4Keyword ,TextRank4Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章摘要方法:TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_summary(text):\n",
    "    tr4s = TextRank4Sentence()\n",
    "    tr4s.analyze( text = text, lower = True , source = ' all_filters ' )\n",
    "    for item in tr4s.get_key_sentences( num = 1 ):\n",
    "        return item.sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch 需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Elasticsearch連線\n",
    "es = Elasticsearch(['elasticsearch:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在Elasticsearch建立index，亦可加入body參數先定義mapping格式\n",
    "es.indices.create(index = 'news', ignore = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將Elasticsearch定義所放入的資料格式:mapping\n",
    "es.indices.put_mapping(index = \"news\",\n",
    "                       doc_type = \"politics\",                \n",
    "                       body = {\n",
    "                        \"properties\": {\n",
    "                            \"source\": {\"type\": \"text\"},\n",
    "                            \"url\": {\"type\": \"text\"},\n",
    "                            \"title\": {\"type\": \"text\"},\n",
    "                            \"date_\": {\"type\": \"text\"},\n",
    "                            \"author\": {\"type\": \"text\"},\n",
    "                            \"content\": {\"type\": \"text\"},\n",
    "                            \"kw\": {\"type\": \"text\"},\n",
    "                            \"img_url\": {\"type\": \"text\"},\n",
    "                            \"@timestamp\": {\"type\": \"date\"}\n",
    "                        }\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了將匯入資料的時間加入時區\n",
    "from datetime import datetime, timedelta, timezone\n",
    "tz_utc_8 = timezone(timedelta(hours=8)) # 創建時區UTC+8:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Consumer需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "import sys\n",
    "import json, time, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 步驟1.設定要連線到Kafka集群的相關設定, 產生一個Kafka的Consumer的實例\n",
    "    consumer = KafkaConsumer(\n",
    "        # Kafka集群在那裡?\n",
    "        bootstrap_servers=[\"kafka1:29092\"],\n",
    "        # ConsumerGroup的名稱\n",
    "        group_id=\"test01\",\n",
    "        # 指定msgKey的反序列化器, 若Key為None, 無法反序列化\n",
    "        key_deserializer=bytes.decode,\n",
    "        # 指定msgValue的反序列化器\n",
    "        #value_deserializer=bytes.decode,\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        # 是否從這個ConsumerGroup尚未讀取的partition / offset開始讀\n",
    "        auto_offset_reset=\"earliest\",\n",
    "    )\n",
    "    # 步驟2.指定想要訂閱訊息的topic名稱\n",
    "    topic_name = \"test\"\n",
    "    # 步驟3.讓Consumer向Kafka集群訂閱指定的topic\n",
    "    consumer.subscribe(topics = topic_name)\n",
    "\n",
    "\n",
    "    # 步驟4.持續的拉取Kafka有進來的訊息\n",
    "    try:\n",
    "        print(\"Start listen incoming messages ...\",\"\\n\")\n",
    "        # 持續監控是否有新的record進來 - 方法一\n",
    "        for record in consumer:\n",
    "            topic = record.topic\n",
    "            partition = record.partition\n",
    "            offset = record.offset\n",
    "            timestamp = record.timestamp\n",
    "        \n",
    "            # 取出msgKey與msgValue\n",
    "            msgKey = record.key\n",
    "            msgValue = record.value\n",
    "            \n",
    "            # 將新聞作者欄位做處理後再放回\n",
    "            if msgValue[\"source\"] == \"TVBS\":      \n",
    "                msgValue[\"author\"] = tvbs_author_etl(msgValue[\"author\"][0])\n",
    "            elif msgValue[\"source\"] == \"SETN\":\n",
    "                msgValue[\"author\"] = setn_author_etl(msgValue[\"author\"][0])\n",
    "            elif msgValue[\"source\"] == \"ETtoday\":\n",
    "                msgValue[\"author\"] = ettoday_author_etl(msgValue[\"author\"][0])\n",
    "            \n",
    "            # 將新聞內文做文章情緒分析並加入欄位與結果\n",
    "            msgValue[\"ariticle_emotion\"] = emo_swinger(msgValue[\"content\"])\n",
    "            # 將新聞內文做關鍵字分析再合併至新聞抓到的關鍵字列表，並把重複的篩選掉\n",
    "            content_keyword = keyword_analysis(msgValue[\"content\"])\n",
    "            for k in content_keyword:\n",
    "                msgValue[\"kw\"].append(k)\n",
    "            msgValue[\"kw\"] = list(set(msgValue[\"kw\"]))\n",
    "            # 將新聞內文做藍綠分析並加入欄位結果\n",
    "            msgValue[\"label\"] = label_predict(msgValue[\"content\"])\n",
    "            # 將新聞做文章摘要並加入欄位與結果\n",
    "            msgValue[\"abstract\"] = news_summary(msgValue[\"content\"])\n",
    "            \n",
    "            # 秀出metadata與msgKey & msgValue訊息\n",
    "            print(\"topic=%s, partition=%s, offset=%s : (key=%s, value=%s)\" % (record.topic, record.partition,\n",
    "                                                                              record.offset, record.key, msgValue))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            print(\"Start sending data to MySQL !!!\")\n",
    "            # 將資料送入MySQL\n",
    "            ip_location = 'chatbot_api'\n",
    "                         \n",
    "            News = msgValue\n",
    "            \n",
    "            # 將json傳回API Server\n",
    "            Endpoint = 'http://%s:5000/news' % (ip_location)\n",
    "\n",
    "            # header要特別註明是json格式\n",
    "            Header = {'Content-Type':'application/json'}\n",
    "            \n",
    "            # 傳送post對API server新增資料 \n",
    "            Response = requests.post(Endpoint,headers=Header,data=json.dumps(News))\n",
    "\n",
    "            # 印出Response的資料訊息\n",
    "            print(Response)\n",
    "            Response = Response.json()\n",
    "            print(Response)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        \n",
    "            print(\"Start sending data to elasticsearch !!!\")\n",
    "            # 將資料送入Elasticsearch\n",
    "            elasticsearch_data = msgValue\n",
    "            elasticsearch_data[\"@timestamp\"] = datetime.now().replace(tzinfo = tz_utc_8)            \n",
    "            # 將資料放入Elasticsearch，若沒有index，會直接建立index、type \n",
    "            es.index(index = 'news', doc_type = 'politics', body = elasticsearch_data, refresh = True )\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except:\n",
    "        # 錯誤處理\n",
    "        e_type, e_value, e_traceback = sys.exc_info()\n",
    "        print(\"type ==> %s\" % (e_type))\n",
    "        print(\"value ==> %s\" % (e_value))\n",
    "        print(\"traceback ==> file name: %s\" % (e_traceback.tb_frame.f_code.co_filename))\n",
    "        print(\"traceback ==> line no: %s\" % (e_traceback.tb_lineno))\n",
    "        print(\"traceback ==> function name: %s\" % (e_traceback.tb_frame.f_code.co_name))\n",
    "    finally:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
